import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
from torch.utils.tensorboard import SummaryWriter
import torch.nn.functional as F
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                             f1_score, confusion_matrix, classification_report)
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import shutil
from datetime import datetime

# ===================== 1. é…ç½®å‚æ•° =====================
class Config:
    # è®­ç»ƒå‚æ•°
    BATCH_SIZE = 64        # æ‰¹æ¬¡å¤§å°
    EPOCHS = 10            # è®­ç»ƒè½®æ¬¡
    LEARNING_RATE = 1e-4   # å­¦ä¹ ç‡
    WEIGHT_DECAY = 1e-5    # æƒé‡è¡°å‡ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰
    DROPOUT = 0.3          # Dropoutæ¯”ä¾‹
    DEVICE = torch.device("cuda:5" if torch.cuda.is_available() else "cpu")  # GPUè®¾å¤‡
    
    # æ•°æ®å¤„ç†å‚æ•°
    TEST_SIZE = 0.2        # æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆ8:2åˆ’åˆ†ï¼‰
    RANDOM_STATE = 42      # éšæœºç§å­ï¼ˆä¿è¯ç»“æœå¯å¤ç°ï¼‰
    N_SMOTE_NEIGHBORS = 5  # SMOTEè¿‡é‡‡æ ·çš„è¿‘é‚»æ•°
    
    # è¯„ä¼°å‚æ•°
    EVAL_METRIC = "weighted"  # æŒ‡æ ‡è®¡ç®—æ–¹å¼
    PLOT_CONFUSION_MATRIX = True  # æ˜¯å¦ç»˜åˆ¶æ··æ·†çŸ©é˜µ

    # é‡‡æ ·å‚æ•°
    OVERSAMPLING_STRATEGY = "smote"  # è¿‡é‡‡æ ·ç­–ç•¥
    MAX_SAMPLES_PER_CLASS = 200      # æ¯ç±»è¿‡é‡‡æ ·åçš„æœ€å¤§æ ·æœ¬æ•°
    
    # æ–‡ä»¶è·¯å¾„
    FEATURES_DIR = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/Task2_69_classification/MLP/MLP_features"
    LOG_DIR = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/Task2_69_classification/MLP/MLP_calssification/tensor_log"
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(LOG_DIR, exist_ok=True)


# ===================== 2. æ•°æ®åŠ è½½ =====================
def load_features():
    """åŠ è½½æ‰€æœ‰ç‰¹å¾æ–‡ä»¶å¹¶æ•´ç†æˆç‰¹å¾å’Œæ ‡ç­¾"""
    print(f"ğŸ“‚ ä» {Config.FEATURES_DIR} åŠ è½½ç‰¹å¾æ–‡ä»¶...")
    
    all_feats = []
    all_labels = []
    label_names = []  # ä¿å­˜ç±»åˆ«åç§°ä¸æ•°å­—æ ‡ç­¾çš„æ˜ å°„
    
    # éå†æ‰€æœ‰npyæ–‡ä»¶
    for filename in os.listdir(Config.FEATURES_DIR):
        if filename.endswith(".npy") and "__and__" in filename:
            # è§£ææ–‡ä»¶åè·å–ç±»åˆ«ä¿¡æ¯
            parts = filename.replace(".npy", "").split("__and__")
            if len(parts) == 2:
                dataset_name, class_name = parts
                class_name = dataset_name + '__' + class_name
                
                # è·å–ç±»åˆ«æ ‡ç­¾ï¼ˆä½¿ç”¨ç´¢å¼•ä½œä¸ºæ•°å­—æ ‡ç­¾ï¼‰
                if class_name not in label_names:
                    label_names.append(class_name)
                label = label_names.index(class_name)
                
                # åŠ è½½ç‰¹å¾
                file_path = os.path.join(Config.FEATURES_DIR, filename)
                features = np.load(file_path)
                
                # æ·»åŠ åˆ°æ€»åˆ—è¡¨
                all_feats.append(features)
                all_labels.extend([label] * features.shape[0])
                
                print(f"   - åŠ è½½ {filename}ï¼š{features.shape[0]} æ ·æœ¬ï¼Œç±»åˆ«ï¼š{class_name} (æ ‡ç­¾ï¼š{label})")
    
    if not all_feats:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç‰¹å¾æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ç‰¹å¾ç›®å½•")
    
    # åˆå¹¶æ‰€æœ‰ç‰¹å¾
    all_feats = np.concatenate(all_feats, axis=0)
    all_labels = np.array(all_labels)
    
    print(f"âœ… ç‰¹å¾åŠ è½½å®Œæˆï¼šæ€»æ ·æœ¬æ•° {all_feats.shape[0]}, ç‰¹å¾ç»´åº¦ {all_feats.shape[1]}, ç±»åˆ«æ•° {len(label_names)}")
    return all_feats, all_labels, label_names


# ===================== 3. æ•°æ®é¢„å¤„ç† =====================
def preprocess_data(all_feats, all_labels):
    """æ•°æ®é¢„å¤„ç†æµç¨‹ï¼šåˆ’åˆ†æ•°æ®é›†ã€å½’ä¸€åŒ–ã€è¿‡é‡‡æ ·"""
    print(f"\nğŸ”§ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
    
    # 1. åˆ†å±‚åˆ’åˆ†è®­ç»ƒé›†/æµ‹è¯•é›†
    from sklearn.model_selection import train_test_split
    train_feat, test_feat, train_label, test_label = train_test_split(
        all_feats, all_labels,
        test_size=Config.TEST_SIZE,
        stratify=all_labels,
        random_state=Config.RANDOM_STATE
    )
    print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆï¼š")
    print(f"   - è®­ç»ƒé›†ï¼š{train_feat.shape[0]} æ ·æœ¬")
    print(f"   - æµ‹è¯•é›†ï¼š{test_feat.shape[0]} æ ·æœ¬")
    
    # 2. å½’ä¸€åŒ–ï¼ˆZ-Scoreï¼‰
    train_mean = np.mean(train_feat, axis=0)
    train_std = np.std(train_feat, axis=0)
    train_std = np.where(train_std < 1e-8, 1e-8, train_std)  # é¿å…é™¤é›¶
    
    train_feat_norm = (train_feat - train_mean) / train_std
    test_feat_norm = (test_feat - train_mean) / train_std
    
    print(f"âœ… å½’ä¸€åŒ–å®Œæˆï¼š")
    print(f"   - è®­ç»ƒé›†å½’ä¸€åŒ–åèŒƒå›´ï¼š[{train_feat_norm.min():.4f}, {train_feat_norm.max():.4f}]")
    print(f"   - æµ‹è¯•é›†å½’ä¸€åŒ–åèŒƒå›´ï¼š[{test_feat_norm.min():.4f}, {test_feat_norm.max():.4f}]")
    
    # 3. è¿‡é‡‡æ ·ç­–ç•¥ï¼ˆå¤šæ•°ç±»ä¸‹é‡‡æ ·+å°‘æ•°ç±»SMOTEï¼‰
    print(f"\nâš–ï¸ å¼€å§‹è¿‡é‡‡æ ·ï¼ˆæ¯ç±»é™åˆ¶æœ€å¤§{Config.MAX_SAMPLES_PER_CLASS}æ ·æœ¬ï¼‰...")
    print(f"   - è¿‡é‡‡æ ·å‰åˆ†å¸ƒï¼š")
    unique_labels = np.unique(train_label)
    for label in unique_labels:
        print(f"     * ç±»åˆ«{label}ï¼š{np.sum(train_label == label)} æ ·æœ¬")
    
    # æ­¥éª¤1ï¼šå¯¹æ‰€æœ‰ç±»åˆ«å…ˆæˆªæ–­åˆ°æœ€å¤§æ ·æœ¬æ•°ï¼ˆå¤šæ•°ç±»ä¸‹é‡‡æ ·ï¼‰
    from collections import defaultdict
    np.random.seed(Config.RANDOM_STATE)
    class_data = defaultdict(list)
    
    # æŒ‰ç±»åˆ«æ”¶é›†æ•°æ®
    for feat, label in zip(train_feat_norm, train_label):
        class_data[label].append(feat)
    
    # æˆªæ–­å¤šæ•°ç±»
    truncated_data = []
    truncated_labels = []
    for label in class_data:
        samples = np.array(class_data[label])
        n_samples = len(samples)
        
        if n_samples > Config.MAX_SAMPLES_PER_CLASS:
            # å¤šæ•°ç±»ï¼šéšæœºä¸‹é‡‡æ ·åˆ°ä¸Šé™
            selected_idx = np.random.choice(n_samples, Config.MAX_SAMPLES_PER_CLASS, replace=False)
            truncated_samples = samples[selected_idx]
        else:
            # å°‘æ•°ç±»ï¼šä¿ç•™å…¨éƒ¨æ ·æœ¬
            truncated_samples = samples
        
        truncated_data.append(truncated_samples)
        truncated_labels.append(np.full(len(truncated_samples), label))
    
    # åˆå¹¶æˆªæ–­åçš„æ•°æ®
    truncated_data = np.concatenate(truncated_data, axis=0)
    truncated_labels = np.concatenate(truncated_labels, axis=0)
    
    # æ­¥éª¤2ï¼šå¯¹å°‘æ•°ç±»ä½¿ç”¨SMOTEè¿‡é‡‡æ ·ï¼ˆè¡¥åˆ°æœ€å¤§æ ·æœ¬æ•°ï¼‰
    from imblearn.over_sampling import SMOTE
    # è®¡ç®—æ¯ä¸ªç±»åˆ«éœ€è¦è¾¾åˆ°çš„æ ·æœ¬æ•°
    sampling_strategy = {
        label: Config.MAX_SAMPLES_PER_CLASS 
        for label in unique_labels
    }
    
    # å¤„ç†å•ç±»åˆ«ç‰¹æ®Šæƒ…å†µ
    if len(unique_labels) == 1:
        print(f"âš ï¸ ä»…1ä¸ªç±»åˆ«ï¼Œæ— éœ€SMOTEï¼Œä½¿ç”¨æˆªæ–­åæ•°æ®")
        train_feat_smote = truncated_data
        train_label_smote = truncated_labels
    else:
        # åº”ç”¨SMOTE
        smote = SMOTE(
            sampling_strategy=sampling_strategy,
            k_neighbors=min(Config.N_SMOTE_NEIGHBORS, min(np.bincount(truncated_labels)) - 1),
            random_state=Config.RANDOM_STATE
        )
        train_feat_smote, train_label_smote = smote.fit_resample(truncated_data, truncated_labels)
    
    # è¾“å‡ºè¿‡é‡‡æ ·ç»“æœ
    print(f"   - è¿‡é‡‡æ ·ååˆ†å¸ƒï¼š")
    for label in np.unique(train_label_smote):
        print(f"     * ç±»åˆ«{label}ï¼š{np.sum(train_label_smote == label)} æ ·æœ¬")
    print(f"   - æ€»æ ·æœ¬æ•°ï¼š{train_feat_smote.shape[0]}")
    
    return (
        train_feat_smote, train_label_smote,
        test_feat_norm, test_label,
        train_mean, train_std
    )


# ===================== 4. æ•°æ®é›†ç±» =====================
class FeatureDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels
        
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return torch.tensor(self.features[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)


# ===================== 5. æ¨¡å‹å®šä¹‰ =====================
class MLPClassifier(nn.Module):
    def __init__(self, input_dim, num_classes, dropout=0.3):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        return self.classifier(x)


# ===================== 6. è®­ç»ƒå’Œè¯„ä¼°å‡½æ•° =====================
def train_model(model, train_loader, val_loader, criterion, optimizer, writer, num_classes, label_names):
    """è®­ç»ƒæ¨¡å‹å¹¶è¿”å›æœ€ä½³æ¨¡å‹"""
    best_val_f1 = 0.0
    best_model_path = os.path.join(Config.LOG_DIR, "best_model.pth")
    metrics_history = []
    
    # è®°å½•ç±»åˆ«åç§°
    with open(os.path.join(Config.LOG_DIR, "class_names.txt"), "w") as f:
        for name in label_names:
            f.write(f"{name}\n")
    
    for epoch in range(Config.EPOCHS):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        all_train_preds = []
        all_train_labels = []
        
        for features, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{Config.EPOCHS} - Training"):
            features, labels = features.to(Config.DEVICE), labels.to(Config.DEVICE)
            
            # å‰å‘ä¼ æ’­
            outputs = model(features)
            loss = criterion(outputs, labels)
            
            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # è®°å½•
            train_loss += loss.item() * features.size(0)
            _, preds = torch.max(outputs, 1)
            all_train_preds.extend(preds.cpu().numpy())
            all_train_labels.extend(labels.cpu().numpy())
        
        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        train_loss = train_loss / len(train_loader.dataset)
        train_acc = accuracy_score(all_train_labels, all_train_preds)
        train_precision = precision_score(all_train_labels, all_train_preds, average=Config.EVAL_METRIC, zero_division=0)
        train_recall = recall_score(all_train_labels, all_train_preds, average=Config.EVAL_METRIC, zero_division=0)
        train_f1 = f1_score(all_train_labels, all_train_preds, average=Config.EVAL_METRIC, zero_division=0)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        all_val_preds = []
        all_val_labels = []
        
        with torch.no_grad():
            for features, labels in tqdm(val_loader, desc=f"Epoch {epoch+1}/{Config.EPOCHS} - Validation"):
                features, labels = features.to(Config.DEVICE), labels.to(Config.DEVICE)
                
                outputs = model(features)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item() * features.size(0)
                _, preds = torch.max(outputs, 1)
                all_val_preds.extend(preds.cpu().numpy())
                all_val_labels.extend(labels.cpu().numpy())
        
        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        val_loss = val_loss / len(val_loader.dataset)
        val_acc = accuracy_score(all_val_labels, all_val_preds)
        val_precision = precision_score(all_val_labels, all_val_preds, average=Config.EVAL_METRIC, zero_division=0)
        val_recall = recall_score(all_val_labels, all_val_preds, average=Config.EVAL_METRIC, zero_division=0)
        val_f1 = f1_score(all_val_labels, all_val_preds, average=Config.EVAL_METRIC, zero_division=0)
        
        # æ‰“å° epoch ç»“æœ
        print(f"\nEpoch {epoch+1}/{Config.EPOCHS}")
        print(f"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Precision: {train_precision:.4f} | Recall: {train_recall:.4f} | F1: {train_f1:.4f}")
        print(f"Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Precision: {val_precision:.4f} | Recall: {val_recall:.4f} | F1: {val_f1:.4f}")
        
        # è®°å½•åˆ°TensorBoard
        writer.add_scalars('Loss', {'train': train_loss, 'val': val_loss}, epoch)
        writer.add_scalars('Accuracy', {'train': train_acc, 'val': val_acc}, epoch)
        writer.add_scalars('Precision', {'train': train_precision, 'val': val_precision}, epoch)
        writer.add_scalars('Recall', {'train': train_recall, 'val': val_recall}, epoch)
        writer.add_scalars('F1', {'train': train_f1, 'val': val_f1}, epoch)
        
        # è®°å½•åˆ°å†å² metrics
        metrics_history.append({
            'epoch': epoch + 1,
            'train_loss': train_loss,
            'train_acc': train_acc,
            'train_precision': train_precision,
            'train_recall': train_recall,
            'train_f1': train_f1,
            'val_loss': val_loss,
            'val_acc': val_acc,
            'val_precision': val_precision,
            'val_recall': val_recall,
            'val_f1': val_f1
        })
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            torch.save(model.state_dict(), best_model_path)
            print(f"ğŸ“Œ ä¿å­˜æœ€ä½³æ¨¡å‹ (F1: {best_val_f1:.4f}) åˆ° {best_model_path}")
    
    # ä¿å­˜æŒ‡æ ‡å†å²åˆ°CSV
    metrics_df = pd.DataFrame(metrics_history)
    metrics_df.to_csv(os.path.join(Config.LOG_DIR, "training_metrics.csv"), index=False)
    print(f"\nğŸ“Š è®­ç»ƒæŒ‡æ ‡å·²ä¿å­˜åˆ° training_metrics.csv")
    
    return best_model_path


def evaluate_model(model_path, test_loader, num_classes, label_names):
    """ä½¿ç”¨æµ‹è¯•é›†è¯„ä¼°æœ€ä½³æ¨¡å‹"""
    print(f"\nğŸ“‹ å¼€å§‹æµ‹è¯•é›†è¯„ä¼°...")
    
    # åŠ è½½æ¨¡å‹
    model = MLPClassifier(input_dim=52, num_classes=num_classes, dropout=Config.DROPOUT)
    model.load_state_dict(torch.load(model_path))
    model.to(Config.DEVICE)
    model.eval()
    
    # æµ‹è¯•
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for features, labels in tqdm(test_loader, desc="Evaluating"):
            features, labels = features.to(Config.DEVICE), labels.to(Config.DEVICE)
            
            outputs = model(features)
            _, preds = torch.max(outputs, 1)
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # è®¡ç®—æŒ‡æ ‡
    test_acc = accuracy_score(all_labels, all_preds)
    test_precision = precision_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    test_recall = recall_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    test_f1 = f1_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    
    print(f"\nğŸ“ æµ‹è¯•é›†ç»“æœ:")
    print(f"Accuracy: {test_acc:.4f}")
    print(f"Precision: {test_precision:.4f}")
    print(f"Recall: {test_recall:.4f}")
    print(f"F1 Score: {test_f1:.4f}")
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    class_report = classification_report(
        all_labels, all_preds, 
        target_names=label_names,
        zero_division=0
    )
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(class_report)
    
    # ä¿å­˜åˆ†ç±»æŠ¥å‘Š
    with open(os.path.join(Config.LOG_DIR, "classification_report.txt"), "w") as f:
        f.write(class_report)
    
    # ä¿å­˜æµ‹è¯•é›†æŒ‡æ ‡
    test_metrics = {
        'test_acc': test_acc,
        'test_precision': test_precision,
        'test_recall': test_recall,
        'test_f1': test_f1,
        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    
    # è¿½åŠ åˆ°CSV
    metrics_path = os.path.join(Config.LOG_DIR, "test_metrics.csv")
    if os.path.exists(metrics_path):
        test_df = pd.read_csv(metrics_path)
        test_df = pd.concat([test_df, pd.DataFrame([test_metrics])], ignore_index=True)
    else:
        test_df = pd.DataFrame([test_metrics])
    test_df.to_csv(metrics_path, index=False)
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    if Config.PLOT_CONFUSION_MATRIX:
        cm = confusion_matrix(all_labels, all_preds)
        plt.figure(figsize=(24, 20))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=label_names, 
                   yticklabels=label_names)
        plt.xlabel('Predicted')
        plt.ylabel('True')
        plt.title('Confusion Matrix')
        plt.tight_layout()
        plt.savefig(os.path.join(Config.LOG_DIR, "confusion_matrix.png"))
        plt.close()
        print(f"ğŸ” æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ° confusion_matrix.png")
    
    return test_metrics


# ===================== 7. ä¸»å‡½æ•° =====================
def main():
    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯å¤ç°æ€§
    np.random.seed(Config.RANDOM_STATE)
    torch.manual_seed(Config.RANDOM_STATE)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(Config.RANDOM_STATE)
    
    # 1. åŠ è½½æ•°æ®
    all_feats, all_labels, label_names = load_features()
    num_classes = len(label_names)
    
    # 2. æ•°æ®é¢„å¤„ç†
    (train_feat, train_label, 
     test_feat, test_label, 
     train_mean, train_std) = preprocess_data(all_feats, all_labels)
    
    # ä¿å­˜å½’ä¸€åŒ–å‚æ•°
    np.save(os.path.join(Config.LOG_DIR, "train_mean.npy"), train_mean)
    np.save(os.path.join(Config.LOG_DIR, "train_std.npy"), train_std)
    
    # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = FeatureDataset(train_feat, train_label)
    test_dataset = FeatureDataset(test_feat, test_label)
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=Config.BATCH_SIZE, 
        shuffle=True, 
        num_workers=4
    )
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=Config.BATCH_SIZE, 
        shuffle=False, 
        num_workers=4
    )
    
    # 4. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    model = MLPClassifier(
        input_dim=train_feat.shape[1],  # 52ç»´ç‰¹å¾
        num_classes=num_classes,
        dropout=Config.DROPOUT
    ).to(Config.DEVICE)
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(
        model.parameters(),
        lr=Config.LEARNING_RATE,
        weight_decay=Config.WEIGHT_DECAY
    )
    
    # 5. åˆå§‹åŒ–TensorBoard
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    writer = SummaryWriter(os.path.join(Config.LOG_DIR, f"run_{timestamp}"))
    print(f"ğŸ“‹ TensorBoardæ—¥å¿—å°†ä¿å­˜åˆ° {os.path.join(Config.LOG_DIR, f'run_{timestamp}')}")
    
    # 6. è®­ç»ƒæ¨¡å‹
    print("\nğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
    best_model_path = train_model(
        model, train_loader, test_loader, 
        criterion, optimizer, writer,
        num_classes, label_names
    )
    
    # 7. è¯„ä¼°æœ€ä½³æ¨¡å‹
    evaluate_model(best_model_path, test_loader, num_classes, label_names)
    
    # å…³é—­TensorBoardå†™å…¥å™¨
    writer.close()
    print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")


if __name__ == "__main__":
    main()
    