import os
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import resample
import seaborn as sns
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import shutil
from datetime import datetime
from collections import defaultdict
from imblearn.over_sampling import SMOTE

# é…ç½®å‚æ•°
class Config:
    # è®­ç»ƒå‚æ•°
    BATCH_SIZE = 64
    EPOCHS = 150
    LEARNING_RATE = 1e-4
    WEIGHT_DECAY = 1e-5
    DROPOUT_RATE = 0.3
    DEVICE = torch.device("cuda:5" if torch.cuda.is_available() else "cpu")
    
    # æ•°æ®å¤„ç†å‚æ•°
    DATA_PATH = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/Task2_69_classification/Mantis/mantis_features_120s"
    TEST_SIZE = 0.2
    RANDOM_STATE = 42
    N_SMOTE_NEIGHBORS = 5
    MAX_SAMPLES_PER_CLASS = 200
    OVERSAMPLING_STRATEGY = "SMOTE"
    
    # è¯„ä¼°å‚æ•°
    EVAL_METRIC = "weighted"
    PLOT_CONFUSION_MATRIX = True
    
    # æ—¥å¿—å’Œä¿å­˜å‚æ•°
    LOG_DIR = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/Task2_69_classification/Mantis/mantis_classification_logs"
    TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")
    BEST_MODEL_PATH = os.path.join(LOG_DIR, f"best_model_{TIMESTAMP}.pt")
    METRICS_CSV = os.path.join(LOG_DIR, f"training_metrics_{TIMESTAMP}.csv")
    CONFUSION_MATRIX_PATH = os.path.join(LOG_DIR, f"confusion_matrix_{TIMESTAMP}.png")

# åˆ›å»ºæ—¥å¿—ç›®å½•
os.makedirs(Config.LOG_DIR, exist_ok=True)

# å®šä¹‰MLPæ¨¡å‹
class MLPClassifier(nn.Module):
    def __init__(self, input_dim=256, hidden_dim=128, 
                 num_classes=2, dropout_rate=Config.DROPOUT_RATE):
        super(MLPClassifier, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim // 2, num_classes)
        )
        
    def forward(self, x):
        return self.layers(x)

# è‡ªå®šä¹‰æ•°æ®é›†
class FeatureDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels
        
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return torch.tensor(self.features[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)

# åŠ è½½æ•°æ®
def load_data(data_path):
    print(f"ğŸ“‚ å¼€å§‹åŠ è½½æ•°æ® from {data_path}")
    
    # è·å–æ‰€æœ‰ç±»åˆ«æ–‡ä»¶å¤¹
    class_files = [f for f in os.listdir(data_path) if f.endswith(".pt")]
    num_classes = len(class_files)
    print(f"âœ… å‘ç° {num_classes} ä¸ªç±»åˆ«")
    
    # è§£æç±»åˆ«åç§°ï¼ˆä»æ–‡ä»¶åæå–ï¼‰
    class_names = [os.path.splitext(f)[0] for f in class_files]
    
    # æ ‡ç­¾ç¼–ç 
    label_encoder = LabelEncoder()
    label_encoder.fit(class_names)
    
    # åŠ è½½æ‰€æœ‰ç‰¹å¾æ•°æ®
    all_feats = []
    all_labels = []
    
    # å…ˆåŠ è½½ä¸€ä¸ªæ–‡ä»¶è·å–ç‰¹å¾ç»´åº¦
    sample_file = class_files[0]
    sample_path = os.path.join(data_path, sample_file)
    sample_features = torch.load(sample_path)
    feature_dim = sample_features.shape[1]
    print(f"âœ… ç‰¹å¾ç»´åº¦: {feature_dim}")
    
    # åŠ è½½æ‰€æœ‰ç±»åˆ«ç‰¹å¾
    for file_name in tqdm(class_files, desc="åŠ è½½ç‰¹å¾æ•°æ®"):
        file_path = os.path.join(data_path, file_name)
        class_name = os.path.splitext(file_name)[0]
        
        try:
            # åŠ è½½ç‰¹å¾æ–‡ä»¶ (n_samples, feature_dim)
            features = torch.load(file_path)
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            features_np = features.numpy() if isinstance(features, torch.Tensor) else features
            
            # è·å–æ ‡ç­¾
            label = label_encoder.transform([class_name])[0]
            
            # æ·»åŠ åˆ°åˆ—è¡¨
            all_feats.extend(features_np)
            all_labels.extend([label] * len(features_np))
            
            print(f"åŠ è½½ç±»åˆ« {class_name}: {len(features_np)} ä¸ªæ ·æœ¬")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_feats_np = np.array(all_feats)
    all_labels_np = np.array(all_labels)
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {all_feats_np.shape[0]} ä¸ªæ ·æœ¬, ç‰¹å¾ç»´åº¦: {all_feats_np.shape[1]}")
    return all_feats_np, all_labels_np, label_encoder, num_classes, feature_dim

# æ•°æ®é¢„å¤„ç†ï¼ˆåˆ’åˆ†+å½’ä¸€åŒ–+è¿‡é‡‡æ ·ï¼‰
def preprocess_data(all_feats, all_labels):
    """
    ç»å…¸æ•°æ®é¢„å¤„ç†æµç¨‹ï¼š
    1. åˆ†å±‚åˆ’åˆ†è®­ç»ƒé›†/æµ‹è¯•é›†
    2. å½’ä¸€åŒ–ï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰
    3. è¿‡é‡‡æ ·ï¼šå¤šæ•°ç±»ä¸‹é‡‡æ ·åˆ°ä¸Šé™ï¼Œå°‘æ•°ç±»SMOTEè¿‡é‡‡æ ·åˆ°ä¸Šé™ï¼ˆç»å…¸å¹³è¡¡ç­–ç•¥ï¼‰
    """
    print(f"\nğŸ”§ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
    
    # 1. åˆ†å±‚åˆ’åˆ†è®­ç»ƒé›†/æµ‹è¯•é›†
    from sklearn.model_selection import train_test_split
    train_feat, test_feat, train_label, test_label = train_test_split(
        all_feats, all_labels, 
        test_size=Config.TEST_SIZE, 
        stratify=all_labels, 
        random_state=Config.RANDOM_STATE
    )
    
    print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆï¼š")
    print(f" - è®­ç»ƒé›†ï¼š{train_feat.shape[0]} æ ·æœ¬")
    print(f" - æµ‹è¯•é›†ï¼š{test_feat.shape[0]} æ ·æœ¬")
    
    # 2. å½’ä¸€åŒ–ï¼ˆZ-Scoreï¼‰
    train_mean = np.mean(train_feat, axis=0)
    train_std = np.std(train_feat, axis=0)
    train_std = np.where(train_std < 1e-8, 1e-8, train_std)  # é¿å…é™¤é›¶
    
    train_feat_norm = (train_feat - train_mean) / train_std
    test_feat_norm = (test_feat - train_mean) / train_std
    
    print(f"âœ… å½’ä¸€åŒ–å®Œæˆï¼š")
    print(f" - è®­ç»ƒé›†å½’ä¸€åŒ–åèŒƒå›´ï¼š[{train_feat_norm.min():.4f}, {train_feat_norm.max():.4f}]")
    print(f" - æµ‹è¯•é›†å½’ä¸€åŒ–åèŒƒå›´ï¼š[{test_feat_norm.min():.4f}, {test_feat_norm.max():.4f}]")
    
    # 3. ç»å…¸è¿‡é‡‡æ ·ç­–ç•¥ï¼ˆå¤šæ•°ç±»ä¸‹é‡‡æ ·+å°‘æ•°ç±»SMOTEï¼‰
    print(f"\nâš–ï¸ å¼€å§‹è¿‡é‡‡æ ·ï¼ˆæ¯ç±»é™åˆ¶æœ€å¤§{Config.MAX_SAMPLES_PER_CLASS}æ ·æœ¬ï¼‰...")
    print(f" - è¿‡é‡‡æ ·å‰åˆ†å¸ƒï¼š")
    unique_labels = np.unique(train_label)
    for label in unique_labels:
        print(f" * ç±»åˆ«{label}ï¼š{np.sum(train_label == label)} æ ·æœ¬")
    
    # æ­¥éª¤1ï¼šå¯¹æ‰€æœ‰ç±»åˆ«å…ˆæˆªæ–­åˆ°æœ€å¤§æ ·æœ¬æ•°ï¼ˆå¤šæ•°ç±»ä¸‹é‡‡æ ·ï¼‰
    from collections import defaultdict
    np.random.seed(Config.RANDOM_STATE)  # å…¨å±€è®¾ç½®éšæœºç§å­
    
    class_data = defaultdict(list)  # æŒ‰ç±»åˆ«æ”¶é›†æ•°æ®
    for feat, label in zip(train_feat_norm, train_label):
        class_data[label].append(feat)
    
    # æˆªæ–­å¤šæ•°ç±»
    truncated_data = []
    truncated_labels = []
    for label in class_data:
        samples = np.array(class_data[label])
        n_samples = len(samples)
        
        if n_samples > Config.MAX_SAMPLES_PER_CLASS:
            # å¤šæ•°ç±»ï¼šéšæœºä¸‹é‡‡æ ·åˆ°ä¸Šé™
            selected_idx = np.random.choice(n_samples, Config.MAX_SAMPLES_PER_CLASS, replace=False)
            truncated_samples = samples[selected_idx]
        else:
            # å°‘æ•°ç±»ï¼šä¿ç•™å…¨éƒ¨æ ·æœ¬
            truncated_samples = samples
        
        truncated_data.append(truncated_samples)
        truncated_labels.append(np.full(len(truncated_samples), label))
    
    # åˆå¹¶æˆªæ–­åçš„æ•°æ®
    truncated_data = np.concatenate(truncated_data, axis=0)
    truncated_labels = np.concatenate(truncated_labels, axis=0)
    
    # æ­¥éª¤2ï¼šå¯¹å°‘æ•°ç±»ä½¿ç”¨SMOTEè¿‡é‡‡æ ·ï¼ˆè¡¥åˆ°æœ€å¤§æ ·æœ¬æ•°ï¼‰
    # è®¡ç®—æ¯ä¸ªç±»åˆ«éœ€è¦è¾¾åˆ°çš„æ ·æœ¬æ•°
    sampling_strategy = {
        label: Config.MAX_SAMPLES_PER_CLASS for label in unique_labels
    }
    
    # å¤„ç†å•ç±»åˆ«ç‰¹æ®Šæƒ…å†µ
    if len(unique_labels) == 1:
        print(f"âš ï¸ ä»…1ä¸ªç±»åˆ«ï¼Œæ— éœ€SMOTEï¼Œä½¿ç”¨æˆªæ–­åæ•°æ®")
        train_feat_smote = truncated_data
        train_label_smote = truncated_labels
    else:
        # ç»å…¸SMOTEåº”ç”¨
        smote = SMOTE(
            sampling_strategy=sampling_strategy,
            k_neighbors=min(Config.N_SMOTE_NEIGHBORS, min(np.bincount(truncated_labels)) - 1),
            random_state=Config.RANDOM_STATE
        )
        train_feat_smote, train_label_smote = smote.fit_resample(truncated_data, truncated_labels)
    
    # è¾“å‡ºè¿‡é‡‡æ ·ç»“æœ
    print(f" - è¿‡é‡‡æ ·ååˆ†å¸ƒï¼š")
    for label in np.unique(train_label_smote):
        print(f" * ç±»åˆ«{label}ï¼š{np.sum(train_label_smote == label)} æ ·æœ¬")
    print(f" - æ€»æ ·æœ¬æ•°ï¼š{train_feat_smote.shape[0]}")
    
    return (
        train_feat_smote, train_label_smote,
        test_feat_norm, test_label,
        train_mean, train_std
    )

# è®­ç»ƒå‡½æ•°
def train_model(model, train_loader, val_loader, criterion, optimizer, writer, num_classes):
    best_val_f1 = 0.0
    metrics_history = []
    
    # åˆå§‹åŒ–CSVæ–‡ä»¶
    if not os.path.exists(Config.METRICS_CSV):
        pd.DataFrame(columns=[
            'epoch', 'train_loss', 'train_acc', 'train_precision', 
            'train_recall', 'train_f1', 'val_loss', 'val_acc',
            'val_precision', 'val_recall', 'val_f1'
        ]).to_csv(Config.METRICS_CSV, index=False)
    
    for epoch in range(Config.EPOCHS):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_preds = []
        train_targets = []
        
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{Config.EPOCHS} - Training"):
            inputs, labels = batch
            inputs, labels = inputs.to(Config.DEVICE), labels.to(Config.DEVICE)
            
            optimizer.zero_grad()
            
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item() * inputs.size(0)
            
            _, preds = torch.max(outputs, 1)
            train_preds.extend(preds.cpu().numpy())
            train_targets.extend(labels.cpu().numpy())
        
        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        train_loss = train_loss / len(train_loader.dataset)
        train_acc = accuracy_score(train_targets, train_preds)
        train_precision = precision_score(
            train_targets, train_preds, 
            average=Config.EVAL_METRIC, 
            zero_division=0
        )
        train_recall = recall_score(
            train_targets, train_preds, 
            average=Config.EVAL_METRIC, 
            zero_division=0
        )
        train_f1 = f1_score(
            train_targets, train_preds, 
            average=Config.EVAL_METRIC, 
            zero_division=0
        )
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_preds = []
        val_targets = []
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Epoch {epoch+1}/{Config.EPOCHS} - Validation"):
                inputs, labels = batch
                inputs, labels = inputs.to(Config.DEVICE), labels.to(Config.DEVICE)
                
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item() * inputs.size(0)
                
                _, preds = torch.max(outputs, 1)
                val_preds.extend(preds.cpu().numpy())
                val_targets.extend(labels.cpu().numpy())
        
        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        val_loss = val_loss / len(val_loader.dataset)
        val_acc = accuracy_score(val_targets, val_preds)
        val_precision = precision_score(
            val_targets, val_preds, 
            average=Config.EVAL_METRIC, 
            zero_division=0
        )
        val_recall = recall_score(
            val_targets, val_preds, 
            average=Config.EVAL_METRIC, 
            zero_division=0
        )
        val_f1 = f1_score(
            val_targets, val_preds, 
            average=Config.EVAL_METRIC, 
            zero_division=0
        )
        
        # æ‰“å° epoch ç»“æœ
        print(f"\nEpoch {epoch+1}/{Config.EPOCHS}")
        print(f"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Precision: {train_precision:.4f} | Recall: {train_recall:.4f} | F1: {train_f1:.4f}")
        print(f"Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Precision: {val_precision:.4f} | Recall: {val_recall:.4f} | F1: {val_f1:.4f}")
        
        # è®°å½•åˆ°TensorBoard
        writer.add_scalar('Loss/Train', train_loss, epoch)
        writer.add_scalar('Loss/Validation', val_loss, epoch)
        writer.add_scalar('Accuracy/Train', train_acc, epoch)
        writer.add_scalar('Accuracy/Validation', val_acc, epoch)
        writer.add_scalar('Precision/Train', train_precision, epoch)
        writer.add_scalar('Precision/Validation', val_precision, epoch)
        writer.add_scalar('Recall/Train', train_recall, epoch)
        writer.add_scalar('Recall/Validation', val_recall, epoch)
        writer.add_scalar('F1/Train', train_f1, epoch)
        writer.add_scalar('F1/Validation', val_f1, epoch)
        
        # ä¿å­˜æŒ‡æ ‡åˆ°CSV
        metrics_df = pd.DataFrame([{
            'epoch': epoch+1,
            'train_loss': train_loss,
            'train_acc': train_acc,
            'train_precision': train_precision,
            'train_recall': train_recall,
            'train_f1': train_f1,
            'val_loss': val_loss,
            'val_acc': val_acc,
            'val_precision': val_precision,
            'val_recall': val_recall,
            'val_f1': val_f1
        }])
        metrics_df.to_csv(Config.METRICS_CSV, mode='a', header=False, index=False)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_val_f1': best_val_f1,
                'train_mean': train_mean,
                'train_std': train_std
            }, Config.BEST_MODEL_PATH)
            print(f"ğŸ“Œ ä¿å­˜æœ€ä½³æ¨¡å‹ (F1: {best_val_f1:.4f}) åˆ° {Config.BEST_MODEL_PATH}")
    
    return model, metrics_history

# æµ‹è¯•å‡½æ•°
def test_model(model, test_loader, label_encoder):
    print("\nğŸ“Š å¼€å§‹æµ‹è¯•æœ€ä½³æ¨¡å‹...")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    checkpoint = torch.load(Config.BEST_MODEL_PATH)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(Config.DEVICE)
    model.eval()
    
    test_preds = []
    test_targets = []
    
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Testing"):
            inputs, labels = batch
            inputs, labels = inputs.to(Config.DEVICE), labels.to(Config.DEVICE)
            
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            
            test_preds.extend(preds.cpu().numpy())
            test_targets.extend(labels.cpu().numpy())
    
    # è®¡ç®—æµ‹è¯•æŒ‡æ ‡
    test_acc = accuracy_score(test_targets, test_preds)
    test_precision = precision_score(
        test_targets, test_preds, 
        average=Config.EVAL_METRIC, 
        zero_division=0
    )
    test_recall = recall_score(
        test_targets, test_preds, 
        average=Config.EVAL_METRIC, 
        zero_division=0
    )
    test_f1 = f1_score(
        test_targets, test_preds, 
        average=Config.EVAL_METRIC, 
        zero_division=0
    )
    
    print(f"\næµ‹è¯•ç»“æœ:")
    print(f"Accuracy: {test_acc:.4f}")
    print(f"Precision: {test_precision:.4f}")
    print(f"Recall: {test_recall:.4f}")
    print(f"F1 Score: {test_f1:.4f}")
    
    # è®°å½•æµ‹è¯•æŒ‡æ ‡åˆ°TensorBoard
    writer = SummaryWriter(Config.LOG_DIR)
    writer.add_scalar('Test/Accuracy', test_acc, 0)
    writer.add_scalar('Test/Precision', test_precision, 0)
    writer.add_scalar('Test/Recall', test_recall, 0)
    writer.add_scalar('Test/F1', test_f1, 0)
    writer.close()
    
    # è¿½åŠ æµ‹è¯•æŒ‡æ ‡åˆ°CSV
    test_metrics_df = pd.DataFrame([{
        'epoch': 'test',
        'train_loss': None,
        'train_acc': None,
        'train_precision': None,
        'train_recall': None,
        'train_f1': None,
        'val_loss': None,
        'val_acc': test_acc,
        'val_precision': test_precision,
        'val_recall': test_recall,
        'val_f1': test_f1
    }])
    test_metrics_df.to_csv(Config.METRICS_CSV, mode='a', header=False, index=False)
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    if Config.PLOT_CONFUSION_MATRIX:
        cm = confusion_matrix(test_targets, test_preds)
        plt.figure(figsize=(15, 12))
        sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', 
                   xticklabels=label_encoder.classes_,
                   yticklabels=label_encoder.classes_)
        plt.xlabel('Predicted')
        plt.ylabel('True')
        plt.title('Confusion Matrix')
        plt.tight_layout()
        plt.savefig(Config.CONFUSION_MATRIX_PATH)
        print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ° {Config.CONFUSION_MATRIX_PATH}")
    
    return test_acc, test_precision, test_recall, test_f1

# ä¸»å‡½æ•°
def main():
    print("===== Mantisç‰¹å¾MLPåˆ†ç±» =====")
    print(f"ä½¿ç”¨è®¾å¤‡: {Config.DEVICE}")
    
    # 1. åŠ è½½æ•°æ®
    all_feats, all_labels, label_encoder, num_classes, feature_dim = load_data(Config.DATA_PATH)
    
    # 2. æ•°æ®é¢„å¤„ç†
    global train_mean, train_std  # å£°æ˜ä¸ºå…¨å±€å˜é‡ä»¥ä¾¿åœ¨train_modelä¸­ä½¿ç”¨
    train_feat, train_label, test_feat, test_label, train_mean, train_std = preprocess_data(all_feats, all_labels)
    
    # 3. åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = FeatureDataset(train_feat, train_label)
    test_dataset = FeatureDataset(test_feat, test_label)
    
    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=4)
    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=4)
    
    # 4. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    model = MLPClassifier(
        input_dim=feature_dim,
        hidden_dim=128,
        num_classes=num_classes,
        dropout_rate=Config.DROPOUT_RATE
    ).to(Config.DEVICE)
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(
        model.parameters(),
        lr=Config.LEARNING_RATE,
        weight_decay=Config.WEIGHT_DECAY
    )
    
    # 5. åˆå§‹åŒ–TensorBoard
    writer = SummaryWriter(Config.LOG_DIR)
    print(f"ğŸ“ TensorBoardæ—¥å¿—å°†ä¿å­˜åˆ°: {Config.LOG_DIR}")
    
    # 6. è®­ç»ƒæ¨¡å‹
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    model, _ = train_model(model, train_loader, test_loader, criterion, optimizer, writer, num_classes)
    writer.close()
    
    # 7. æµ‹è¯•æ¨¡å‹
    test_model(model, test_loader, label_encoder)
    
    print("\nğŸ‰ è®­ç»ƒå’Œæµ‹è¯•å®Œæˆ!")
    print(f"æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {Config.LOG_DIR}")

if __name__ == "__main__":
    main()
