# File: train_mantis.py
import os
import torch
import numpy as np
from tqdm import tqdm
import logging
from sklearn.metrics import accuracy_score

# Mantis ç›¸å…³å¯¼å…¥ï¼ˆæ ¹æ®å®˜æ–¹ APIï¼‰
from mantis.architecture import Mantis8M
from mantis.trainer import MantisTrainer

# å¼•å…¥é€šç”¨å·¥å…·ç»„ä»¶
sys.path.append(str(Path(__file__).parent.parent / "tools"))
# from models.moe_classifier import DiseaseClassifier
# from models.moe_classifier_unfreeze_v2 import DiseaseClassifier
# from moe_dataset.speech_disease_dataset import SpeechDiseaseDataset
from moe_dataset.speech_disease_dataset_v2 import SpeechDiseaseDataset

# é…ç½®
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
DATA_ROOT = "/path/to/your/dataset"  # âš ï¸ æ›¿æ¢ä¸ºä½ çš„å®é™…è·¯å¾„
MODEL_SAVE_DIR = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/benchmark/mantis/model"
os.makedirs(MODEL_SAVE_DIR, exist_ok=True)

# è¶…å‚æ•°
SAMPLE_RATE = 8000
N_FFT = 512
HOP_LENGTH = 358
BATCH_SIZE = 16
NUM_EPOCHS = 100
FINE_TUNING_TYPE = 'head'  # æˆ– 'full'
LABEL_MAP = {
    'M_Con': 0,
    'F_Con': 0,
    'M_Dys': 1,
    'F_Dys': 1
}
NUM_CLASSES = 2

# ä¼˜åŒ–å™¨åˆå§‹åŒ–å‡½æ•°ï¼ˆå¿…é¡»æŒ‰ Mantis è¦æ±‚æ ¼å¼ï¼‰
def init_optimizer(params):
    return torch.optim.AdamW(params, lr=2e-4, betas=(0.9, 0.999), weight_decay=0.05)

# æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def prepare_data_for_mantis(dataloader):
    """
    å°† DataLoader è½¬æ¢ä¸º Mantis æ‰€éœ€çš„æ ¼å¼ï¼š
        X: list of tensors, each [T, 512] (T = num_windows)
        y: list of int labels
    """
    X, y = [], []
    for windows, label, length in tqdm(dataloader.dataset.dataset, desc="Formatting data for Mantis"):
        # windows: [T, 512] tensor
        # æˆªæ–­æˆ–ä¿ç•™å‰ N_MAX ä¸ªçª—å£ï¼ˆMantis å¯èƒ½æœ‰é•¿åº¦é™åˆ¶ï¼‰
        T = windows.shape[0]
        max_len = 512  # æ ¹æ® Mantis çš„æœ€å¤§åºåˆ—é•¿åº¦è°ƒæ•´
        if T > max_len:
            windows = windows[:max_len]
        X.append(windows.numpy())  # è½¬ä¸º numpy array of float32
        y.append(label)
    return X, y


def main():
    logger.info("ğŸš€ å¼€å§‹åŠ è½½æ•°æ®é›†...")
    train_loader, val_loader, test_loader, N_MAX = SpeechDiseaseDataset.get_dataloaders(
        data_root=DATA_ROOT,
        sample_rate=SAMPLE_RATE,
        n_fft=N_FFT,
        hop_length=HOP_LENGTH,
        label_map=LABEL_MAP,
        train_ratio=0.8,
        val_ratio=0.1,
        test_ratio=0.1,
        batch_size=BATCH_SIZE,
        n_max=None,
        q_percentile=95,
        seed=42,
        num_workers=4
    )
    logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œæœ€å¤§çª—å£æ•° N_MAX = {N_MAX}")

    # Step 1: è½¬æ¢æ•°æ®æ ¼å¼ä¸º Mantis æ‰€éœ€
    logger.info("ğŸ”„ è½¬æ¢æ•°æ®æ ¼å¼...")
    X_train, y_train = prepare_data_for_mantis(train_loader)
    X_test, y_test = prepare_data_for_mantis(test_loader)
    logger.info(f"âœ… è®­ç»ƒæ ·æœ¬: {len(X_train)}, æµ‹è¯•æ ·æœ¬: {len(X_test)}")

    # Step 2: åŠ è½½ Mantis-8M æ¨¡å‹
    logger.info("ğŸ“¥ åŠ è½½ Mantis-8M é¢„è®­ç»ƒæ¨¡å‹...")
    network = Mantis8M(device=DEVICE)
    network = network.from_pretrained("paris-noah/Mantis-8M")
    logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    # Step 3: åˆå§‹åŒ– Trainer
    model = MantisTrainer(
        device=DEVICE,
        network=network,
        num_classes=NUM_CLASSES  # æ˜¾å¼æŒ‡å®šåˆ†ç±»å¤´è¾“å‡ºç»´åº¦
    )

    # Step 4: å¾®è°ƒ
    logger.info(f"ğŸ”¥ å¼€å§‹ {FINE_TUNING_TYPE} å¾®è°ƒ...")
    model.fit(
        X_train, y_train,
        num_epochs=NUM_EPOCHS,
        fine_tuning_type=FINE_TUNING_TYPE,
        init_optimizer=init_optimizer
    )

    # Step 5: é¢„æµ‹
    logger.info("ğŸ”® å¼€å§‹é¢„æµ‹...")
    y_pred = model.predict(X_test)

    # Step 6: è¯„ä¼°
    test_acc = accuracy_score(y_test, y_pred)
    logger.info(f"âœ… æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}")
    print(f"Accuracy on the test set is {test_acc}")

    # Step 7: ä¿å­˜æ¨¡å‹ï¼ˆå¦‚æœæ”¯æŒï¼‰
    try:
        model.save(os.path.join(MODEL_SAVE_DIR, "mantis_finetuned.pth"))
        logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³ {MODEL_SAVE_DIR}")
    except:
        logger.warning("âš ï¸ æ— æ³•ä¿å­˜æ¨¡å‹ï¼Œå¯èƒ½ Trainer ä¸æ”¯æŒ save æ–¹æ³•")

    # ä¿å­˜é…ç½®
    config = {
        'data_root': DATA_ROOT,
        'sample_rate': SAMPLE_RATE,
        'n_fft': N_FFT,
        'hop_length': HOP_LENGTH,
        'batch_size': BATCH_SIZE,
        'num_epochs': NUM_EPOCHS,
        'fine_tuning_type': FINE_TUNING_TYPE,
        'num_classes': NUM_CLASSES,
        'model_name': 'Mantis-8M',
        'pretrained_checkpoint': 'paris-noah/Mantis-8M'
    }
    import json
    with open(os.path.join(MODEL_SAVE_DIR, "config.json"), 'w') as f:
        json.dump(config, f, indent=2)
    logger.info("ğŸ“„ é…ç½®å·²ä¿å­˜")


if __name__ == "__main__":
    main()