# File: fine_tune_classifier_with_windows.py

import os
import torch
import sys
from pathlib import Path
import torch.nn.functional as F
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

from mantis.architecture import Mantis8M
from mantis.trainer import MantisTrainer
import logging

import time
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, roc_auc_score
from sklearn.preprocessing import label_binarize
from itertools import cycle


# å¼•å…¥é€šç”¨å·¥å…·ç»„ä»¶
sys.path.append(str(Path(__file__).parent.parent.parent / "tools"))
# from models.moe_classifier import DiseaseClassifier
# from models.moe_classifier_unfreeze_v2 import DiseaseClassifier
# from moe_dataset.speech_disease_dataset import SpeechDiseaseDataset
from moe_dataset.speech_disease_dataset_v2 import SpeechDiseaseDataset

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# é…ç½®å‚æ•°
DATA_ROOT = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/dataset/Parkinson_3700"  # ä¿®æ”¹ä¸ºä½ çš„è·¯å¾„
SAMPLE_RATE = 8000
BATCH_SIZE = 32
DEVICE = 'cuda:6' if torch.cuda.is_available() else 'cpu'
MODEL_NAME = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/benchmark/mantis/model"
N_FFT = 512
HOP_LENGTH = 256  # å¯è°ƒæ•´æ­¥é•¿
TARGET_LENGTH = 512  # Mantis è¾“å…¥é•¿åº¦
POOLING_METHOD = 'mean'  # 'mean', 'max'


def extract_window_features(model, dataloader):
    """
    å¯¹æ¯ä¸ªçª—å£æå– Mantis ç‰¹å¾
    è¿”å›ï¼šlist of (features, label, num_windows)
        features: (N, 256) æ¯ä¸ªçª—å£çš„ç‰¹å¾
    """
    model.network.eval()
    all_features = []
    all_labels = []

    with torch.no_grad():
        for windows, labels, lengths in dataloader:
            B = windows.size(0)
            N_max = windows.size(1)
            x = windows.view(-1, 1, N_max * 512 // N_max)  # (B*N, 1, 512)
            x = x[:, :, :TARGET_LENGTH]  # ç¡®ä¿é•¿åº¦ä¸º 512

            # ä½¿ç”¨ Mantis æå–ç‰¹å¾
            z = model.transform(x.numpy())  # (B*N, 256)
            z = torch.tensor(z, device=DEVICE)

            # æ¢å¤æ¯ä¸ªæ ·æœ¬çš„çª—å£ç»“æ„
            z = z.view(B, -1, z.size(-1))  # (B, N, 256)

            # æ± åŒ–ï¼šmean æˆ– max
            if POOLING_METHOD == 'mean':
                pooled = z.mean(dim=1)  # (B, 256)
            elif POOLING_METHOD == 'max':
                pooled, _ = z.max(dim=1)  # (B, 256)
            else:
                raise ValueError(f"Unknown pooling: {POOLING_METHOD}")

            all_features.append(pooled.cpu().numpy())
            all_labels.append(labels.numpy())

    X = np.concatenate(all_features, axis=0)
    y = np.concatenate(all_labels, axis=0)
    return X, y

def main_v1():
    logger.info("ğŸš€ å¼€å§‹åŠ è½½æ•°æ®é›†...")

    # Step 1: è·å–åˆ†çª—åçš„ DataLoaderï¼ˆä½¿ç”¨åŸå§‹ä»£ç ä¸­çš„ get_dataloadersï¼‰
    train_loader, val_loader, test_loader, N_MAX = SpeechDiseaseDataset.get_dataloaders(
        data_root=DATA_ROOT,
        sample_rate=SAMPLE_RATE,
        n_fft=N_FFT,
        hop_length=HOP_LENGTH,
        batch_size=BATCH_SIZE,
        # n_max=N_MAX,  # å¯è®¾ä¸º 100 æˆ–æ ¹æ® get_recommended_N_max()
        num_workers=4
    )

    logger.info("âœ… æ•°æ®åŠ è½½å®Œæˆ")

    # Step 2: åŠ è½½ Mantis æ¨¡å‹ï¼ˆä»…ç”¨äºç‰¹å¾æå–ï¼‰
    logger.info("ğŸ“¥ åŠ è½½ Mantis-8M é¢„è®­ç»ƒæ¨¡å‹...")
    network = Mantis8M(device=DEVICE)
    network = network.from_pretrained(MODEL_NAME)
    model = MantisTrainer(device=DEVICE, network=network)
    logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    # Step 3: æå–è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„èšåˆç‰¹å¾
    logger.info("ğŸ” æå–è®­ç»ƒé›†çª—å£ç‰¹å¾...")
    X_train, y_train = extract_window_features(model, train_loader)

    logger.info("ğŸ” æå–æµ‹è¯•é›†çª—å£ç‰¹å¾...")
    X_test, y_test = extract_window_features(model, test_loader)

    logger.info(f"âœ… ç‰¹å¾æå–å®Œæˆ: X_train={X_train.shape}, X_test={X_test.shape}")

    # Step 4: è®­ç»ƒåˆ†ç±»å™¨ï¼ˆä¸è®­ç»ƒ Mantisï¼Œåªè®­ç»ƒé¡¶å±‚ï¼‰
    logger.info("ğŸ¯ è®­ç»ƒåˆ†ç±»å™¨...")
    classifier = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42)
    classifier.fit(X_train, y_train)

    # Step 5: è¯„ä¼°
    y_pred = classifier.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    logger.info(f"âœ… æµ‹è¯•é›†å‡†ç¡®ç‡: {acc:.4f}")
    print(f"Accuracy on the test set is {acc:.4f}")

def plot_feature_importance(classifier, output_dir, top_k=20, filename="feature_importance.png"):
    """
    ç»˜åˆ¶éšæœºæ£®æ—çš„ç‰¹å¾é‡è¦æ€§å›¾ï¼ˆTop-Kï¼‰
    """
    importances = classifier.feature_importances_
    indices = np.argsort(importances)[::-1][:top_k]  # é™åºå–å‰ top_k

    plt.figure(figsize=(10, 6))
    plt.title(f"Top {top_k} Feature Importances from Random Forest", fontsize=14)
    bars = plt.bar(range(top_k), importances[indices], color='cornflowerblue', edgecolor='black', alpha=0.8)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, bar in enumerate(bars):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2., height + 0.001,
                 f"{importances[indices[i]]:.3f}",
                 ha='center', va='bottom', fontsize=9)

    plt.xticks(range(top_k), [f"Feature {idx}" for idx in indices], rotation=60)
    plt.ylabel("Importance Score", fontsize=12)
    plt.xlabel("Feature Index", fontsize=12)
    plt.ylim(0, max(importances[indices]) * 1.1)
    plt.tight_layout()
    
    save_path = os.path.join(output_dir, filename)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    logger.info(f"âœ… ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜è‡³: {save_path}")
    plt.close()

def main():
    logger.info("ğŸš€ å¼€å§‹åŠ è½½æ•°æ®é›†...")

    # Step 1: è·å–åˆ†çª—åçš„ DataLoader
    train_loader, val_loader, test_loader, N_MAX = SpeechDiseaseDataset.get_dataloaders(
        data_root=DATA_ROOT,
        sample_rate=SAMPLE_RATE,
        n_fft=N_FFT,
        hop_length=HOP_LENGTH,
        batch_size=BATCH_SIZE,
        num_workers=4
    )

    logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
    logger.info(f"ğŸ“Š è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_loader.dataset)} | æ‰¹æ¬¡æ•°: {len(train_loader)}")
    logger.info(f"ğŸ“Š æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_loader.dataset)} | æ‰¹æ¬¡æ•°: {len(test_loader)}")
    logger.info(f"ğŸ“ æ¯ä¸ªæ ·æœ¬æœ€å¤§çª—å£æ•° N_MAX: {N_MAX}")

    # Step 2: åŠ è½½ Mantis æ¨¡å‹ï¼ˆä»…ç”¨äºç‰¹å¾æå–ï¼‰
    logger.info("ğŸ“¥ åŠ è½½ Mantis-8M é¢„è®­ç»ƒæ¨¡å‹...")
    network = Mantis8M(device=DEVICE)
    network = network.from_pretrained(MODEL_NAME)
    model = MantisTrainer(device=DEVICE, network=network)
    logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    # logger.info(f"ğŸ§  æ¨¡å‹ç»“æ„: {network}")

    # Step 3: æå–è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„èšåˆç‰¹å¾ï¼ˆå¸¦è¿›åº¦ï¼‰
    logger.info("ğŸ” å¼€å§‹æå–ã€è®­ç»ƒé›†ã€‘çª—å£ç‰¹å¾...")
    start_time = time.time()
    X_train, y_train = extract_window_features(model, train_loader)
    train_extract_time = time.time() - start_time
    logger.info(f"âœ… è®­ç»ƒé›†ç‰¹å¾æå–å®Œæˆ | è€—æ—¶: {train_extract_time:.2f}s | X_train.shape={X_train.shape}, y_train.shape={y_train.shape}")

    logger.info("ğŸ” å¼€å§‹æå–ã€æµ‹è¯•é›†ã€‘çª—å£ç‰¹å¾...")
    start_time = time.time()
    X_test, y_test = extract_window_features(model, test_loader)
    test_extract_time = time.time() - start_time
    logger.info(f"âœ… æµ‹è¯•é›†ç‰¹å¾æå–å®Œæˆ | è€—æ—¶: {test_extract_time:.2f}s | X_test.shape={X_test.shape}, y_test.shape={y_test.shape}")

    # Step 4: è®­ç»ƒåˆ†ç±»å™¨
    logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒåˆ†ç±»å™¨...")
    logger.info(f"ğŸ§® ä½¿ç”¨åˆ†ç±»å™¨: RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)")

    start_train_time = time.time()
    classifier = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42)
    classifier.fit(X_train, y_train)
    train_classifier_time = time.time() - start_train_time
    logger.info(f"âœ… åˆ†ç±»å™¨è®­ç»ƒå®Œæˆ | è€—æ—¶: {train_classifier_time:.2f}s")

    # Step 5: è¯„ä¼°
    logger.info("ğŸ“Š æ­£åœ¨è¿›è¡Œæ¨¡å‹è¯„ä¼°...")
    y_pred = classifier.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, output_dict=False)
    cm = confusion_matrix(y_test, y_pred)

    # è¾“å‡ºè¯¦ç»†è¯„ä¼°æŠ¥å‘Š
    logger.info(f"âœ… æœ€ç»ˆç»“æœ:")
    logger.info(f"   ğŸ¯ æµ‹è¯•é›†å‡†ç¡®ç‡: {acc:.4f}")
    logger.info(f"   ğŸ“Š åˆ†ç±»æŠ¥å‘Š:\n{report}")
    logger.info(f"   ğŸ”¢ æ··æ·†çŸ©é˜µ:\n{cm}")

    print(f"âœ… Accuracy on the test set is {acc:.4f}")
    print(f"ğŸ“ˆ åˆ†ç±»æŠ¥å‘Š:\n{report}")

    # === ğŸ”½ ç»˜å›¾ä¸ä¿å­˜ç»“æœéƒ¨åˆ† ğŸ”½ ===
    OUTPUT_DIR = os.path.dirname(os.path.abspath(__file__))
    os.makedirs(OUTPUT_DIR, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨

    # æ–‡ä»¶åå®šä¹‰
    CONFUSION_MATRIX_FILENAME = "easycall_confusion_matrix.png"
    METRICS_FILENAME = "easycall_training_metrics_detailed.txt"

    # --- 1. ç»˜åˆ¶å¹¶ä¿å­˜æ··æ·†çŸ©é˜µ ---
    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title("Confusion Matrix (Random Forest on Mantis-8M Features)", fontsize=14)
    plt.colorbar()
    classes = ['Healthy (0)', 'Dysphonia (1)']
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    thresh = cm.max() / 2.
    for i, j in np.ndindex(cm.shape):
        plt.text(j, i, format(cm[i, j], 'd'),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True Label', fontsize=12)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.tight_layout()
    confusion_matrix_path = os.path.join(OUTPUT_DIR, CONFUSION_MATRIX_FILENAME)
    plt.savefig(confusion_matrix_path, dpi=300, bbox_inches='tight')
    logger.info(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: {confusion_matrix_path}")
    plt.close()

    # --- 2. ä¿å­˜è¯¦ç»†æŒ‡æ ‡åˆ°æ–‡æœ¬æ–‡ä»¶ ---
    metrics_content = f"""# Model Evaluation Report
Generated at: {time.strftime('%Y-%m-%d %H:%M:%S')}
Model: Mantis-8M + RandomForestClassifier
Dataset: Parkinson_3700
Test Samples: {len(y_test)}
Class Distribution (Test): Healthy={np.sum(y_test == 0)}, Dysphonia={np.sum(y_test == 1)}

ğŸ¯ Accuracy: {acc:.4f}

ğŸ“Š Classification Report:
{classification_report(y_test, y_pred, target_names=['Healthy', 'Dysphonia'])}

ğŸ”¢ Confusion Matrix:
[[{cm[0, 0]}  {cm[0, 1]}]
 [{cm[1, 0]}  {cm[1, 1]}]]
"""
    metrics_path = os.path.join(OUTPUT_DIR, METRICS_FILENAME)
    with open(metrics_path, 'w', encoding='utf-8') as f:
        f.write(metrics_content)
    logger.info(f"âœ… è¯¦ç»†è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜è‡³: {metrics_path}")

    # --- 3. å¯é€‰ï¼šç»˜åˆ¶åˆ†ç±»æŠ¥å‘Šçš„æŸ±çŠ¶å›¾ï¼ˆF1, Precision, Recallï¼‰---
    try:
        from sklearn.metrics import precision_recall_fscore_support
        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=0)
        x = np.arange(len(classes))
        width = 0.25

        plt.figure(figsize=(10, 6))
        plt.bar(x - width, precision, width, label='Precision', color='skyblue')
        plt.bar(x, recall, width, label='Recall', color='lightgreen')
        plt.bar(x + width, f1, width, label='F1-Score', color='salmon')

        plt.ylabel('Score')
        plt.title('Per-Class Performance Metrics')
        plt.xticks(x, classes)
        plt.ylim(0, 1.0)
        plt.legend()
        plt.grid(axis='y', linestyle='--', alpha=0.7)

        for i, v in enumerate(precision):
            plt.text(i - width, v + 0.01, f"{v:.2f}", ha='center', va='bottom', fontsize=9)
        for i, v in enumerate(recall):
            plt.text(i, v + 0.01, f"{v:.2f}", ha='center', va='bottom', fontsize=9)
        for i, v in enumerate(f1):
            plt.text(i + width, v + 0.01, f"{v:.2f}", ha='center', va='bottom', fontsize=9)

        plt.tight_layout()
        metrics_plot_path = os.path.join(OUTPUT_DIR, "easycall_performance_bars.png")
        plt.savefig(metrics_plot_path, dpi=300, bbox_inches='tight')
        logger.info(f"âœ… æ€§èƒ½æŒ‡æ ‡æŸ±çŠ¶å›¾å·²ä¿å­˜è‡³: {metrics_plot_path}")
        plt.close()
    except Exception as e:
        logger.warning(f"âš ï¸ æ— æ³•ç”Ÿæˆæ€§èƒ½æŸ±çŠ¶å›¾: {e}")
    
     # --- 4. ç»˜åˆ¶å¹¶ä¿å­˜ç‰¹å¾é‡è¦æ€§å›¾ ---
    try:
        plot_feature_importance(classifier, OUTPUT_DIR, top_k=20, filename="easycall_feature_importance.png")
    except Exception as e:
        logger.warning(f"âš ï¸ æ— æ³•ç”Ÿæˆç‰¹å¾é‡è¦æ€§å›¾: {e}")

    logger.info("ğŸ”š æ‰€æœ‰è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜ã€‚")

if __name__ == "__main__":
    main()