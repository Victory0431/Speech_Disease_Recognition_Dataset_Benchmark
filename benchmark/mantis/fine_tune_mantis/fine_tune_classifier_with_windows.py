# File: fine_tune_classifier_with_windows.py

import os
import torch
import sys
from pathlib import Path
import torch.nn.functional as F
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

from mantis.architecture import Mantis8M
from mantis.trainer import MantisTrainer
import logging

import time
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


# å¼•å…¥é€šç”¨å·¥å…·ç»„ä»¶
sys.path.append(str(Path(__file__).parent.parent.parent / "tools"))
# from models.moe_classifier import DiseaseClassifier
# from models.moe_classifier_unfreeze_v2 import DiseaseClassifier
# from moe_dataset.speech_disease_dataset import SpeechDiseaseDataset
from moe_dataset.speech_disease_dataset_v2 import SpeechDiseaseDataset

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# é…ç½®å‚æ•°
DATA_ROOT = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/dataset/Parkinson_3700"  # ä¿®æ”¹ä¸ºä½ çš„è·¯å¾„
SAMPLE_RATE = 8000
BATCH_SIZE = 32
DEVICE = 'cuda:6' if torch.cuda.is_available() else 'cpu'
MODEL_NAME = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/benchmark/mantis/model"
N_FFT = 512
HOP_LENGTH = 256  # å¯è°ƒæ•´æ­¥é•¿
TARGET_LENGTH = 512  # Mantis è¾“å…¥é•¿åº¦
POOLING_METHOD = 'mean'  # 'mean', 'max'

def extract_window_features(model, dataloader):
    """
    å¯¹æ¯ä¸ªçª—å£æå– Mantis ç‰¹å¾
    è¿”å›ï¼šlist of (features, label, num_windows)
        features: (N, 256) æ¯ä¸ªçª—å£çš„ç‰¹å¾
    """
    model.network.eval()
    all_features = []
    all_labels = []

    with torch.no_grad():
        for windows, labels, lengths in dataloader:
            B = windows.size(0)
            N_max = windows.size(1)
            x = windows.view(-1, 1, N_max * 512 // N_max)  # (B*N, 1, 512)
            x = x[:, :, :TARGET_LENGTH]  # ç¡®ä¿é•¿åº¦ä¸º 512

            # ä½¿ç”¨ Mantis æå–ç‰¹å¾
            z = model.transform(x.numpy())  # (B*N, 256)
            z = torch.tensor(z, device=DEVICE)

            # æ¢å¤æ¯ä¸ªæ ·æœ¬çš„çª—å£ç»“æ„
            z = z.view(B, -1, z.size(-1))  # (B, N, 256)

            # æ± åŒ–ï¼šmean æˆ– max
            if POOLING_METHOD == 'mean':
                pooled = z.mean(dim=1)  # (B, 256)
            elif POOLING_METHOD == 'max':
                pooled, _ = z.max(dim=1)  # (B, 256)
            else:
                raise ValueError(f"Unknown pooling: {POOLING_METHOD}")

            all_features.append(pooled.cpu().numpy())
            all_labels.append(labels.numpy())

    X = np.concatenate(all_features, axis=0)
    y = np.concatenate(all_labels, axis=0)
    return X, y

def main_v1():
    logger.info("ğŸš€ å¼€å§‹åŠ è½½æ•°æ®é›†...")

    # Step 1: è·å–åˆ†çª—åçš„ DataLoaderï¼ˆä½¿ç”¨åŸå§‹ä»£ç ä¸­çš„ get_dataloadersï¼‰
    train_loader, val_loader, test_loader, N_MAX = SpeechDiseaseDataset.get_dataloaders(
        data_root=DATA_ROOT,
        sample_rate=SAMPLE_RATE,
        n_fft=N_FFT,
        hop_length=HOP_LENGTH,
        batch_size=BATCH_SIZE,
        # n_max=N_MAX,  # å¯è®¾ä¸º 100 æˆ–æ ¹æ® get_recommended_N_max()
        num_workers=4
    )

    logger.info("âœ… æ•°æ®åŠ è½½å®Œæˆ")

    # Step 2: åŠ è½½ Mantis æ¨¡å‹ï¼ˆä»…ç”¨äºç‰¹å¾æå–ï¼‰
    logger.info("ğŸ“¥ åŠ è½½ Mantis-8M é¢„è®­ç»ƒæ¨¡å‹...")
    network = Mantis8M(device=DEVICE)
    network = network.from_pretrained(MODEL_NAME)
    model = MantisTrainer(device=DEVICE, network=network)
    logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    # Step 3: æå–è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„èšåˆç‰¹å¾
    logger.info("ğŸ” æå–è®­ç»ƒé›†çª—å£ç‰¹å¾...")
    X_train, y_train = extract_window_features(model, train_loader)

    logger.info("ğŸ” æå–æµ‹è¯•é›†çª—å£ç‰¹å¾...")
    X_test, y_test = extract_window_features(model, test_loader)

    logger.info(f"âœ… ç‰¹å¾æå–å®Œæˆ: X_train={X_train.shape}, X_test={X_test.shape}")

    # Step 4: è®­ç»ƒåˆ†ç±»å™¨ï¼ˆä¸è®­ç»ƒ Mantisï¼Œåªè®­ç»ƒé¡¶å±‚ï¼‰
    logger.info("ğŸ¯ è®­ç»ƒåˆ†ç±»å™¨...")
    classifier = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42)
    classifier.fit(X_train, y_train)

    # Step 5: è¯„ä¼°
    y_pred = classifier.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    logger.info(f"âœ… æµ‹è¯•é›†å‡†ç¡®ç‡: {acc:.4f}")
    print(f"Accuracy on the test set is {acc:.4f}")

def main():
    logger.info("ğŸš€ å¼€å§‹åŠ è½½æ•°æ®é›†...")

    # Step 1: è·å–åˆ†çª—åçš„ DataLoader
    train_loader, val_loader, test_loader, N_MAX = SpeechDiseaseDataset.get_dataloaders(
        data_root=DATA_ROOT,
        sample_rate=SAMPLE_RATE,
        n_fft=N_FFT,
        hop_length=HOP_LENGTH,
        batch_size=BATCH_SIZE,
        num_workers=4
    )

    logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
    logger.info(f"ğŸ“Š è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_loader.dataset)} | æ‰¹æ¬¡æ•°: {len(train_loader)}")
    logger.info(f"ğŸ“Š æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_loader.dataset)} | æ‰¹æ¬¡æ•°: {len(test_loader)}")
    logger.info(f"ğŸ“ æ¯ä¸ªæ ·æœ¬æœ€å¤§çª—å£æ•° N_MAX: {N_MAX}")

    # Step 2: åŠ è½½ Mantis æ¨¡å‹ï¼ˆä»…ç”¨äºç‰¹å¾æå–ï¼‰
    logger.info("ğŸ“¥ åŠ è½½ Mantis-8M é¢„è®­ç»ƒæ¨¡å‹...")
    network = Mantis8M(device=DEVICE)
    network = network.from_pretrained(MODEL_NAME)
    model = MantisTrainer(device=DEVICE, network=network)
    logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    logger.info(f"ğŸ§  æ¨¡å‹ç»“æ„: {network}")

    # Step 3: æå–è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„èšåˆç‰¹å¾ï¼ˆå¸¦è¿›åº¦ï¼‰
    logger.info("ğŸ” å¼€å§‹æå–ã€è®­ç»ƒé›†ã€‘çª—å£ç‰¹å¾...")
    start_time = time.time()
    X_train, y_train = extract_window_features(model, train_loader)
    train_extract_time = time.time() - start_time
    logger.info(f"âœ… è®­ç»ƒé›†ç‰¹å¾æå–å®Œæˆ | è€—æ—¶: {train_extract_time:.2f}s | X_train.shape={X_train.shape}, y_train.shape={y_train.shape}")

    logger.info("ğŸ” å¼€å§‹æå–ã€æµ‹è¯•é›†ã€‘çª—å£ç‰¹å¾...")
    start_time = time.time()
    X_test, y_test = extract_window_features(model, test_loader)
    test_extract_time = time.time() - start_time
    logger.info(f"âœ… æµ‹è¯•é›†ç‰¹å¾æå–å®Œæˆ | è€—æ—¶: {test_extract_time:.2f}s | X_test.shape={X_test.shape}, y_test.shape={y_test.shape}")

    # Step 4: è®­ç»ƒåˆ†ç±»å™¨
    logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒåˆ†ç±»å™¨...")
    logger.info(f"ğŸ§® ä½¿ç”¨åˆ†ç±»å™¨: RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)")

    start_train_time = time.time()
    classifier = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42)
    classifier.fit(X_train, y_train)
    train_classifier_time = time.time() - start_train_time
    logger.info(f"âœ… åˆ†ç±»å™¨è®­ç»ƒå®Œæˆ | è€—æ—¶: {train_classifier_time:.2f}s")

    # Step 5: è¯„ä¼°
    logger.info("ğŸ“Š æ­£åœ¨è¿›è¡Œæ¨¡å‹è¯„ä¼°...")
    y_pred = classifier.predict(X_test)
    acc = accuracy_score(y_test, y_pred)

    # è¾“å‡ºè¯¦ç»†è¯„ä¼°æŠ¥å‘Š
    logger.info(f"âœ… æœ€ç»ˆç»“æœ:")
    logger.info(f"   ğŸ¯ æµ‹è¯•é›†å‡†ç¡®ç‡: {acc:.4f}")
    logger.info(f"   ğŸ“Š åˆ†ç±»æŠ¥å‘Š:\n{classification_report(y_test, y_pred)}")
    logger.info(f"   ğŸ”¢ æ··æ·†çŸ©é˜µ:\n{confusion_matrix(y_test, y_pred)}")

    print(f"âœ… Accuracy on the test set is {acc:.4f}")
    print(f"ğŸ“ˆ åˆ†ç±»æŠ¥å‘Š:\n{classification_report(y_test, y_pred)}")


if __name__ == "__main__":
    main()