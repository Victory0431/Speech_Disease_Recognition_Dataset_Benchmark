import os
import sys
import random
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from pathlib import Path
import warnings
warnings.filterwarnings("ignore")

from torch.nn import DataParallel
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
from tqdm import tqdm


# å¼•å…¥é€šç”¨å·¥å…·ç»„ä»¶
sys.path.append(str(Path(__file__).parent.parent / "tools"))

from trainer.train_and_evaluate_time_moe_v2 import train_and_evaluate_time_moe  # è®­ç»ƒè¯„ä¼°æ¥å£
from utils.save_time_moe_results import save_time_moe_results  # ç»“æœä¿å­˜æ¥å£
from utils.metrics import calculate_class_distribution  # ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡
from models.moe_model import TimeMoEClassifier  # Time-MoEåˆ†ç±»æ¨¡å‹
from models.BaseTimeDataset import BaseTimeDataset  # æ—¶åºæ•°æ®åŸºç±»ï¼ˆå·²å®ç°éŸ³é¢‘åŠ è½½/çª—å£é€»è¾‘ï¼‰



# ========================= æ ¸å¿ƒï¼šé…ç½®ç±»ï¼ˆä¿®å¤å¤šå¡è®¾å¤‡é…ç½®ï¼‰=========================
class Config:
    """
    å¸•é‡‘æ£®ç—…æ•°æ®é›†é…ç½®ï¼šæŒ‰æ–‡ä»¶å¤¹åŒºåˆ†ç±»åˆ«ï¼ˆå¥åº·ç±»ï¼šM_Con/F_Conï¼›ç–¾ç—…ç±»ï¼šF_Dys/M_Dysï¼‰
    å¤šå¡é…ç½®ï¼šä»…å¯ç”¨0-3å·GPUï¼Œä¸»å¡è®¾ä¸º0å·
    """
    # -------------------------- 1. æ•°æ®ç›¸å…³ï¼ˆå¸•é‡‘æ£®æ•°æ®é›†æ ¸å¿ƒé…ç½®ï¼‰--------------------------
    DATASET_NAME = "Parkinson_3700"  
    ROOT_DIR = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/dataset/Parkinson_3700"  
    TRAIN_LABEL_PATH = None   
    TEST_LABEL_PATH = None   
    CLASS_NAMES = ["Healthy", "Parkinson"]  # 0=å¥åº·ï¼Œ1=å¸•é‡‘æ£®
    
    # éŸ³é¢‘ä¸æ—¶åºçª—å£é…ç½®
    SAMPLE_RATE = 8000  
    WINDOW_SIZE = 2048   # 8kHzä¸‹0.256ç§’
    WINDOW_STRIDE = 2048 # 0%é‡å ï¼ˆå‡å°‘æ»‘çª—æ•°ï¼Œé™ä½å†…å­˜ï¼‰
    AUDIO_EXT = ".wav"   

    # -------------------------- 2. æ¨¡å‹ç›¸å…³ï¼ˆä¿®å¤éª¨å¹²ç½‘ç»œè®¾å¤‡ï¼‰--------------------------
    BACKBONE_PATH = "/mnt/data/test1/repo/Time-MoE/pretrain_model"  
    NUM_CLASSES = len(CLASS_NAMES)  
    FREEZE_BACKBONE = True  # å†»ç»“éª¨å¹²ï¼Œä»…è®­åˆ†ç±»å¤´
    DROPOUT_RATE = 0.1      

    # -------------------------- 3. è®­ç»ƒç›¸å…³ï¼ˆå¤šå¡æ ¸å¿ƒä¿®å¤ï¼‰--------------------------
    # ä¿®å¤1ï¼šä»…å¯ç”¨0-3å·GPUï¼Œé¿å…å¤šä½™å¡å¹²æ‰°
    os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3"  
    # ä¿®å¤2ï¼šä¸»å¡è®¾ä¸º0å·ï¼ˆä¸visibleåˆ—è¡¨ç¬¬ä¸€ä¸ªä¸€è‡´ï¼‰
    DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"  
    SEED = 42  
    BATCH_SIZE = 8  # å¤šå¡ä¸‹å¯å¢è‡³16ï¼ˆæ¯å¡2ä¸ªæ ·æœ¬ï¼‰ï¼Œæš‚ä¿æŒ8æ–¹ä¾¿è°ƒè¯•
    NUM_EPOCHS = 10  
    LR = 1e-3  
    WEIGHT_DECAY = 1e-4  
    NUM_WORKERS = 16  

    # æ•°æ®é›†åˆ’åˆ†æ¯”ä¾‹
    TRAIN_RATIO = 0.7
    VALID_RATIO = 0.15
    TEST_RATIO = 0.15

    # éŸ³é¢‘è¿‡æ»¤å‚æ•°
    MONO = True  
    NORMALIZE_AUDIO = True  
    MIN_AUDIO_LENGTH = 10000  # 8kHzä¸‹1.25ç§’
    SILENCE_THRESHOLD = 0.05  # ä¿®å¤ï¼šä»0.1é™è‡³0.05ï¼Œå‡å°‘æœ‰æ•ˆçª—å£ä¸¢å¤±
    MAX_SILENCE_RATIO = 0.8  

    # -------------------------- 4. è¾“å‡ºç›¸å…³ --------------------------
    OUTPUT_DIR = os.path.join(Path(__file__).parent, "results", DATASET_NAME)  
    os.makedirs(OUTPUT_DIR, exist_ok=True)  
    METRICS_FILENAME = f"{DATASET_NAME}_time_moe_metrics.txt"  
    CONFUSION_MATRIX_FILENAME = f"{DATASET_NAME}_time_moe_cm.png"  


# ========================= æ—¶åºæ•°æ®é›†ç±»ï¼ˆä¿®å¤æ•°æ®è®¾å¤‡ç§»åŠ¨ï¼‰=========================
class TimeMoEDataset(BaseTimeDataset):
    """
    ä¿®å¤ç‚¹ï¼šæ‰€æœ‰è¾“å‡ºå¼ é‡å¼ºåˆ¶ç§»åˆ°ä¸»å¡ï¼ˆconfig.DEVICEï¼‰ï¼Œé¿å…è®¾å¤‡ä¸åŒ¹é…
    """
    
    def __init__(self, file_list, labels, config, mode="train"):
        super().__init__(
            sample_rate=config.SAMPLE_RATE,
            mono=config.MONO,
            normalize=config.NORMALIZE_AUDIO,
            min_audio_length=config.MIN_AUDIO_LENGTH,
            silence_threshold=config.SILENCE_THRESHOLD,
            max_silence_ratio=config.MAX_SILENCE_RATIO
        )
        self.config = config
        self.mode = mode
        self.file_list = file_list
        self.labels = labels
    
    @classmethod
    def from_config(cls, config, mode="train"):
        all_audio_files = cls._get_all_audio_files(config.ROOT_DIR, config.AUDIO_EXT)
        if not all_audio_files:
            raise ValueError(f"æ ¹ç›®å½• {config.ROOT_DIR} æœªæ‰¾åˆ° {config.AUDIO_EXT} æ–‡ä»¶")
        print(f"\nğŸ” åŠ è½½ {mode} é›†åŸå§‹éŸ³é¢‘ï¼š{len(all_audio_files)} ä¸ª")
        
        valid_files, valid_labels = cls._filter_invalid_files(all_audio_files, config)
        if not valid_files:
            raise ValueError(f"{mode} é›†è¿‡æ»¤åæ— æœ‰æ•ˆæ–‡ä»¶ï¼Œè¯·è°ƒæ•´å‚æ•°")
        print(f"âœ… {mode} é›†è¿‡æ»¤å®Œæˆï¼š{len(all_audio_files)} â†’ {len(valid_files)} ä¸ªæœ‰æ•ˆæ–‡ä»¶")
        
        return cls(file_list=valid_files, labels=valid_labels, config=config, mode=mode)
    
    @staticmethod
    def _get_all_audio_files(root_dir, audio_ext):
        audio_files = []
        for dir_path, _, file_names in os.walk(root_dir):
            for file_name in file_names:
                if file_name.lower().endswith(audio_ext.lower()):
                    audio_files.append(os.path.join(dir_path, file_name))
        return audio_files
    
    @classmethod
    def _filter_invalid_files(cls, all_audio_files, config):
        valid_files = []
        valid_labels = []
        base_dataset = BaseTimeDataset(
            sample_rate=config.SAMPLE_RATE,
            mono=config.MONO,
            normalize=config.NORMALIZE_AUDIO,
            min_audio_length=config.MIN_AUDIO_LENGTH,
            silence_threshold=config.SILENCE_THRESHOLD,
            max_silence_ratio=config.MAX_SILENCE_RATIO
        )
        
        for file_path in all_audio_files:
            wav, _ = base_dataset._load_audio(file_path, check_validity=True)
            if wav is not None:
                valid_files.append(file_path)
                label = cls._parse_single_label(file_path, config.CLASS_NAMES)
                valid_labels.append(label)
        return valid_files, valid_labels
    
    @staticmethod
    def _parse_single_label(file_path, class_names):
        folder_label_map = {"M_Con":0, "F_Con":0, "F_Dys":1, "M_Dys":1}
        parent_folder = os.path.basename(os.path.dirname(file_path))
        if parent_folder not in folder_label_map:
            raise ValueError(f"æœªçŸ¥æ–‡ä»¶å¤¹ {parent_folder}ï¼Œæ–‡ä»¶è·¯å¾„ï¼š{file_path}")
        return folder_label_map[parent_folder]
    
    def __getitem__(self, idx):
        """
        ä¿®å¤3ï¼šæ‰€æœ‰è¾“å‡ºå¼ é‡ï¼ˆwindow/windows/label_tensorï¼‰å¼ºåˆ¶ç§»åˆ°ä¸»å¡
        """
        file_path = self.file_list[idx]
        label = self.labels[idx]
        
        # åŠ è½½éŸ³é¢‘ï¼ˆå·²è¿‡æ»¤ï¼Œæ— éœ€å†æ£€æŸ¥ï¼‰
        wav, _ = self._load_audio(file_path, check_validity=False)
        
        if self.mode == "train":
            # è®­ç»ƒæ¨¡å¼ï¼šéšæœºçª—å£ + ç§»åˆ°ä¸»å¡
            window = self._get_random_window(wav, window_size=self.config.WINDOW_SIZE)
            # ä¿®å¤ç‚¹ï¼šçª—å£ç§»åˆ°ä¸»å¡
            window = window.to(self.config.DEVICE, non_blocking=True)
            # ä¿®å¤ç‚¹ï¼šæ ‡ç­¾ç§»åˆ°ä¸»å¡
            label_tensor = torch.tensor(label, dtype=torch.int64).to(self.config.DEVICE, non_blocking=True)
            return window, label_tensor
        
        else:
            # éªŒè¯/æµ‹è¯•æ¨¡å¼ï¼šæ»‘çª— + ç§»åˆ°ä¸»å¡
            windows = self._get_sliding_windows(
                wav=wav,
                window_size=self.config.WINDOW_SIZE,
                stride=self.config.WINDOW_STRIDE
            )
            # ä¿®å¤ç‚¹ï¼šæ»‘çª—ç§»åˆ°ä¸»å¡
            windows = windows.to(self.config.DEVICE, non_blocking=True)
            # ä¿®å¤ç‚¹ï¼šæ ‡ç­¾ç§»åˆ°ä¸»å¡
            label_tensor = torch.tensor(label, dtype=torch.int64).to(self.config.DEVICE, non_blocking=True)
            return windows, label_tensor, file_path
    
    def __len__(self):
        return len(self.file_list)


# ========================= ä¸»æµç¨‹ï¼ˆä¿®å¤æ¨¡å‹å¤šå¡åŒ…è£…ï¼‰=========================
def main():
    # 1. åˆå§‹åŒ–é…ç½®ä¸éšæœºç§å­
    config = Config()
    torch.manual_seed(config.SEED)
    random.seed(config.SEED)
    np.random.seed(config.SEED)
    print(f"ğŸ“Œ å®éªŒé…ç½®ï¼šæ•°æ®é›†={config.DATASET_NAME} | è®¾å¤‡={config.DEVICE} | å¤šå¡={[0,1,2,3]}")
    print(f"ğŸ“Œ ç±»åˆ«æ˜ å°„ï¼š{dict(enumerate(config.CLASS_NAMES))}")

    # 2. åŠ è½½å®Œæ•´æ•°æ®é›†
    print("\n" + "="*80)
    print("1. åŠ è½½å¸•é‡‘æ£®ç—…æ•°æ®é›†")
    print("="*80)
    full_dataset = TimeMoEDataset.from_config(config, mode="train")
    print(f"âœ… å®Œæ•´æ•°æ®é›†è§„æ¨¡ï¼š{len(full_dataset)} ä¸ªéŸ³é¢‘æ–‡ä»¶")

    # 3. åˆ†å±‚åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
    print("\n" + "="*80)
    print("2. åˆ†å±‚åˆ’åˆ†æ•°æ®é›†")
    print("="*80)
    file_list = full_dataset.file_list
    labels = full_dataset.labels

    # åˆ’åˆ†è®­ç»ƒé›† & ä¸´æ—¶é›†
    train_files, temp_files, train_labels, temp_labels = train_test_split(
        file_list, labels,
        test_size=config.VALID_RATIO + config.TEST_RATIO,
        stratify=labels,
        random_state=config.SEED
    )

    # åˆ’åˆ†éªŒè¯é›† & æµ‹è¯•é›†
    val_files, test_files, val_labels, test_labels = train_test_split(
        temp_files, temp_labels,
        test_size=config.TEST_RATIO / (config.VALID_RATIO + config.TEST_RATIO),
        stratify=temp_labels,
        random_state=config.SEED
    )

    # æ„å»ºå„å­é›†
    train_dataset = TimeMoEDataset(train_files, train_labels, config, mode="train")
    val_dataset = TimeMoEDataset(val_files, val_labels, config, mode="val")
    test_dataset = TimeMoEDataset(test_files, test_labels, config, mode="test")

    # æ‰“å°ç±»åˆ«åˆ†å¸ƒ
    print("ğŸ“Š è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒï¼š")
    train_dist = calculate_class_distribution(train_labels, config.CLASS_NAMES)
    for cls_name, (count, ratio) in train_dist.items():
        print(f"  {cls_name}: {count} ä¸ªï¼ˆ{ratio:.2f}%ï¼‰")
    print("ğŸ“Š éªŒè¯é›†ç±»åˆ«åˆ†å¸ƒï¼š")
    val_dist = calculate_class_distribution(val_labels, config.CLASS_NAMES)
    for cls_name, (count, ratio) in val_dist.items():
        print(f"  {cls_name}: {count} ä¸ªï¼ˆ{ratio:.2f}%ï¼‰")
    print("ğŸ“Š æµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒï¼š")
    test_dist = calculate_class_distribution(test_labels, config.CLASS_NAMES)
    for cls_name, (count, ratio) in test_dist.items():
        print(f"  {cls_name}: {count} ä¸ªï¼ˆ{ratio:.2f}%ï¼‰")

    # 4. æ„å»ºDataLoaderï¼ˆä¿®å¤4ï¼špin_memory=Trueé…åˆnon_blockingï¼‰
    print("\n" + "="*80)
    print("3. æ„å»ºDataLoader")
    print("="*80)
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=True,
        num_workers=config.NUM_WORKERS,
        pin_memory=True,  # åŠ é€ŸGPUæ•°æ®ä¼ è¾“ï¼Œé…åˆ__getitem__çš„non_blocking
        drop_last=True    # é¿å…æœ€åä¸€æ‰¹æ ·æœ¬æ•°ä¸è¶³å¯¼è‡´è®¾å¤‡åˆ†é…é—®é¢˜
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=1,
        shuffle=False,
        num_workers=config.NUM_WORKERS,
        pin_memory=True
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=1,
        shuffle=False,
        num_workers=config.NUM_WORKERS,
        pin_memory=True
    )
    print(f"âœ… è®­ç»ƒLoaderï¼š{len(train_loader)} æ‰¹ï¼ˆæ‰¹å¤§å°{config.BATCH_SIZE}ï¼‰")
    print(f"âœ… éªŒè¯Loaderï¼š{len(val_loader)} ä¸ªæ–‡ä»¶")
    print(f"âœ… æµ‹è¯•Loaderï¼š{len(test_loader)} ä¸ªæ–‡ä»¶")

    # 5. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ï¼ˆä¿®å¤5ï¼šç¡®ä¿æ¨¡å‹å…¨å‚æ•°åœ¨ä¸»å¡ï¼‰
    print("\n" + "="*80)
    print("4. åˆå§‹åŒ–æ¨¡å‹ä¸è®­ç»ƒç»„ä»¶")
    print("="*80)
    # æ­¥éª¤1ï¼šåˆå§‹åŒ–æ¨¡å‹ï¼ˆç¡®ä¿å†…éƒ¨backboneå’Œclassifieréƒ½åœ¨ä¸»å¡ï¼‰
    model = TimeMoEClassifier(config)
    # å¼ºåˆ¶æ¨¡å‹æ‰€æœ‰å‚æ•°ç§»åˆ°ä¸»å¡ï¼ˆåŒ…æ‹¬å†»ç»“çš„éª¨å¹²ç½‘ç»œï¼‰
    model = model.to(config.DEVICE)
    # è°ƒè¯•éªŒè¯ï¼šæ£€æŸ¥éª¨å¹²ç½‘ç»œå‚æ•°è®¾å¤‡
    for name, param in model.backbone.named_parameters():
        if param.device != torch.device(config.DEVICE):
            param.data = param.data.to(config.DEVICE)
            print(f"âš ï¸  å·²å°†éª¨å¹²å‚æ•° {name} ç§»åˆ°ä¸»å¡ {config.DEVICE}")
    
    # æ­¥éª¤2ï¼šå¤šå¡åŒ…è£…ï¼ˆä»…ç”¨0-3å·å¡ï¼Œä¸VISIBLE_DEVICESä¸€è‡´ï¼‰
    model = DataParallel(model, device_ids=[0,1,2,3])
    print(f"âœ… æ¨¡å‹ç»“æ„ï¼šTime-MoEéª¨å¹²ï¼ˆå†»ç»“ï¼‰ + åˆ†ç±»å¤´ï¼ˆ{config.NUM_CLASSES}ï¼‰| å¤šå¡ï¼š0,1,2,3å·")

    # æŸå¤±å‡½æ•°ï¼ˆæƒé‡åœ¨ä¸»å¡ï¼‰
    class_counts = np.bincount(train_labels)
    class_weights = len(train_labels) / (len(config.CLASS_NAMES) * class_counts)
    criterion = nn.CrossEntropyLoss(
        weight=torch.tensor(class_weights, device=config.DEVICE, dtype=torch.float32)
    )
    print(f"âœ… æŸå¤±å‡½æ•°æƒé‡ç±»å‹: {criterion.weight.dtype}ï¼ˆæ­£ç¡®ï¼štorch.float32ï¼‰")

    # ä¼˜åŒ–å™¨ï¼ˆå¤šå¡ä¸‹é€šè¿‡model.moduleè®¿é—®åŸæ¨¡å‹åˆ†ç±»å¤´ï¼‰
    optimizer = torch.optim.AdamW(
        model.module.classifier.parameters(),  # ä¿®å¤ï¼šæ­£ç¡®è®¿é—®åˆ†ç±»å¤´å‚æ•°
        lr=config.LR,
        weight_decay=config.WEIGHT_DECAY
    )
    print(f"âœ… ä¼˜åŒ–å™¨ï¼šAdamWï¼ˆLR={config.LR}ï¼Œæƒé‡è¡°å‡={config.WEIGHT_DECAY}ï¼‰")

    # 6. è®­ç»ƒä¸è¯„ä¼°
    print("\n" + "="*80)
    print("5. å¼€å§‹è®­ç»ƒä¸è¯„ä¼°")
    print("="*80)
    results = train_and_evaluate_time_moe(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        test_loader=test_loader,
        criterion=criterion,
        optimizer=optimizer,
        config=config
    )

    # 7. ä¿å­˜ç»“æœ
    print("\n" + "="*80)
    print("6. ä¿å­˜å®éªŒç»“æœ")
    print("="*80)
    save_time_moe_results(
        results=results,
        config=config,
        aggregation_strategy="mean"
    )
    print(f"âœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³ï¼š{config.OUTPUT_DIR}")

    # 8. æ‰“å°æœ€ç»ˆæµ‹è¯•ç»“æœ
    print("\n" + "="*80)
    print("7. æœ€ç»ˆæµ‹è¯•ç»“æœ")
    print("="*80)
    final_metrics = results["final_test_metrics"]
    key_metrics = ["accuracy", "f1_score", "auc", "sensitivity", "specificity"]
    for metric in key_metrics:
        if metric in final_metrics:
            print(f"{metric.upper()}: {final_metrics[metric]:.4f}")


if __name__ == "__main__":
    main()