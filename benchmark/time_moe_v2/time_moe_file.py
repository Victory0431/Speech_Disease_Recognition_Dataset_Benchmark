# -*- coding: utf-8 -*-
"""
ICASSP Benchmark: Time-MoE + MLP for Speech Disease Classification
Dataset: Parkinson_3700
Model: Frozen Time-MoE + Trainable MLP Head
Features: Windowing (512, 30% overlap), 8kHz, Masked Pooling
"""

import os
import sys
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from torch.amp import autocast, GradScaler
import librosa
import numpy as np
import pandas as pd
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¼•å…¥é€šç”¨å·¥å…·ç»„ä»¶
sys.path.append(str(Path(__file__).parent.parent / "tools"))
# from models.moe_classifier import DiseaseClassifier
from models.moe_classifier_unfreeze_v2 import DiseaseClassifier
# from moe_dataset.speech_disease_dataset import SpeechDiseaseDataset
from moe_dataset.speech_disease_dataset_v2 import SpeechDiseaseDataset

# ===========================================
# 1. é…ç½®å‚æ•°
# ===========================================
DEVICE = torch.device("cuda:7" if torch.cuda.is_available() else "cpu")
SAMPLE_RATE_ORIG = None  # ä¿ç•™åŸå§‹é‡‡æ ·ç‡
SAMPLE_RATE = 8000
WINDOW_LENGTH = 512      # L=512
HOP_LENGTH = int(WINDOW_LENGTH * 0.7)  # 30% overlap â†’ hop=358
BATCH_SIZE = 1
NUM_EPOCHS = 100
NUM_WORKERS = 16
LEARNING_RATE = 1e-3
N_MAX = None  # ç¨åç»Ÿè®¡ 95% åˆ†ä½æ•°
DATA_ROOT = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/dataset/Parkinson_3700"
# ===========================================
# é…ç½®å‚æ•°ï¼ˆæ–°å¢ï¼‰
# ===========================================
BACKBONE_PATH = "/mnt/data/test1/repo/Time-MoE/pretrain_model"  # ä½ çš„é¢„è®­ç»ƒè·¯å¾„
NUM_CLASSES = 2  # å¯æ”¹ä¸º3æˆ–4
FREEZE_BACKBONE = True  # å¿…é¡»ä¸º True


# ===========================================
# 6. Collate Functionï¼ˆè¡¥é›¶ + Maskï¼‰
# ===========================================
# ä¿®æ”¹ collate_fn ä¸­çš„ mask å¤„ç†
def collate_fn(batch):
    global N_MAX
    # print (N_MAX)
    windows_list = [item[0] for item in batch]
    labels = [item[1] for item in batch]
    lengths = [item[2] for item in batch]

    padded_windows = []
    masks = []

    for windows, length in zip(windows_list, lengths):
        if length > N_MAX:
            windows = windows[:N_MAX]
            mask = torch.ones(N_MAX, dtype=torch.bool)  # bool
        else:
            pad_len = N_MAX - length
            padding = torch.zeros(pad_len, windows.shape[1])
            windows = torch.cat([windows, padding], dim=0)
            mask = torch.cat([
                torch.ones(length, dtype=torch.bool),
                torch.zeros(pad_len, dtype=torch.bool)
            ])
        padded_windows.append(windows)
        masks.append(mask)

    x = torch.stack(padded_windows)     # [B, N_MAX, L]
    y = torch.LongTensor(labels)        # [B]
    mask = torch.stack(masks)           # [B, N_MAX], dtype=bool

    return x, y, mask


def train_epoch(model, dataloader, optimizer, criterion, device, accumulation_steps=8):
    model.train()
    total_loss = 0.0
    scaler = GradScaler()

    optimizer.zero_grad()

    for step, (x, y, mask) in enumerate(tqdm(dataloader, desc="Training")):
        x, y, mask = x.to(device), y.to(device), mask.to(device)

        # å‰å‘ä¼ æ’­
        with autocast(device_type='cuda', dtype=torch.float16):
            logits = model(x, mask)
            loss = criterion(logits, y)
            loss = loss / accumulation_steps  # æ¢¯åº¦ç´¯ç§¯

        # åå‘ä¼ æ’­
        scaler.scale(loss).backward()

        total_loss += loss.item() * accumulation_steps

        # å‚æ•°æ›´æ–°
        if (step + 1) % accumulation_steps == 0 or (step + 1) == len(dataloader):
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

    return total_loss / len(dataloader)


def eval_model(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0
    all_preds = []
    all_labels = []
    all_logits = []

    with torch.no_grad():
        for batch_idx, (x, y, mask) in enumerate(tqdm(dataloader, desc="Evaluating")):
            x, y, mask = x.to(device), y.to(device), mask.to(device)
            logits = model(x, mask)
            loss = criterion(logits, y)
            total_loss += loss.item()

            pred = logits.argmax(dim=1)
            correct += (pred == y).sum().item()
            total += y.size(0)

            all_logits.append(logits.cpu())
            all_preds.append(pred.cpu())
            all_labels.append(y.cpu())

            # ğŸ”¥ æ‰“å°ç¬¬ä¸€ä¸ª batch è°ƒè¯•ä¿¡æ¯
            if batch_idx == 0:
                print("\n" + "="*50)
                print("ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šç¬¬ä¸€ä¸ªéªŒè¯ batch")
                print("="*50)
                print(f"è¾“å…¥ x.shape: {x.shape}")
                print(f"æ ‡ç­¾ y: {y.tolist()}")
                print(f"logits: {logits.tolist()}")
                print(f"é¢„æµ‹ pred: {pred.tolist()}")
                print(f"æŸå¤± loss: {loss.item():.4f}")
                print(f"è¯¥ batch å‡†ç¡®ç‡: {(pred == y).float().mean().item():.4f}")
                print(f"logits å·®å€¼ |logits[:,0] - logits[:,1]|: {(logits[:,0] - logits[:,1]).abs().tolist()}")
                print("="*50)

    all_logits = torch.cat(all_logits)
    all_preds = torch.cat(all_preds)
    all_labels = torch.cat(all_labels)
    acc = correct / total

    # ğŸ”¥ å…¨å±€ç»Ÿè®¡
    print("\n" + "="*50)
    print("ğŸ“Š éªŒè¯é›†å…¨å±€ç»Ÿè®¡")
    print("="*50)
    print(f"æ€»æ ·æœ¬æ•°: {total}")
    print(f"çœŸå®æ ‡ç­¾åˆ†å¸ƒ: ç±»åˆ« 0 = {all_labels.eq(0).sum().item()}, ç±»åˆ« 1 = {all_labels.eq(1).sum().item()}")
    print(f"æ¨¡å‹é¢„æµ‹åˆ†å¸ƒ: é¢„æµ‹ 0 = {all_preds.eq(0).sum().item()}, é¢„æµ‹ 1 = {all_preds.eq(1).sum().item()}")

    if all_preds.unique().size(0) == 1:
        print(f"ğŸš¨ è­¦å‘Šï¼šæ¨¡å‹æ‰€æœ‰é¢„æµ‹å‡ä¸ºç±»åˆ« {all_preds[0].item()}ï¼")
    else:
        print("âœ… é¢„æµ‹ç»“æœæœ‰å˜åŒ–")

    print(f"logits å‡å€¼: {all_logits.mean().item():.4f}, æ ‡å‡†å·®: {all_logits.std().item():.4f}")
    print(f"logits ç¬¬0ç±»å‡å€¼: {all_logits[:,0].mean().item():.4f}, ç¬¬1ç±»å‡å€¼: {all_logits[:,1].mean().item():.4f}")
    logit_diff = (all_logits[:,0] - all_logits[:,1]).abs()
    print(f"logits å·®å€¼ |logit0 - logit1| å‡å€¼: {logit_diff.mean().item():.4f}")
    if logit_diff.mean() < 0.1:
        print("âš ï¸ è­¦å‘Šï¼šlogits å·®å€¼æå°ï¼Œæ¨¡å‹å‡ ä¹æ— æ³•åŒºåˆ†ä¸¤ç±»ï¼")

    return total_loss / len(dataloader), acc


def main():
    print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {DEVICE}")
    print("------------------")
    print(f"ğŸ“¦ æ•°æ®é…ç½®: batch_size={BATCH_SIZE}, workers={NUM_WORKERS}")
    print(f"ğŸµ éŸ³é¢‘å‚æ•°: sample_rate={SAMPLE_RATE}, window={WINDOW_LENGTH}, hop={HOP_LENGTH}")
    print("------------------\n")

    # è·å– dataloader
    train_loader, val_loader, test_loader, N_MAX = SpeechDiseaseDataset.get_dataloaders(
        data_root=DATA_ROOT,
        sample_rate=SAMPLE_RATE,
        n_fft=WINDOW_LENGTH,
        hop_length=HOP_LENGTH,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS,
        q_percentile=95,
        seed=42
    )

    # Step 4: åˆå§‹åŒ–æ¨¡å‹ï¼ˆå…³é”®ä¿®æ”¹ï¼‰
    model = DiseaseClassifier(
        backbone_path=BACKBONE_PATH,
        num_classes=NUM_CLASSES,
        device=DEVICE,
        unfreeze_last_n=1,
    )
    model = model.to(DEVICE)

    # ğŸ” æ‰“å°å¯è®­ç»ƒå‚æ•°æ•°é‡
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    backbone_trainable = sum(p.numel() for p in model.backbone.parameters() if p.requires_grad)
    classifier_trainable = sum(p.numel() for p in model.classifier.parameters() if p.requires_grad)

    print(f"ğŸ”§ æ€»å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
    print(f"ğŸ”§ ä¸»å¹²å¯è®­ç»ƒå‚æ•°æ•°é‡: {backbone_trainable:,}")
    print(f"ğŸ”§ åˆ†ç±»å¤´å¯è®­ç»ƒå‚æ•°æ•°é‡: {classifier_trainable:,}\n")

    # Step 5: ä¼˜åŒ–å™¨ & æŸå¤±å‡½æ•°
    optimizer = optim.Adam([
        {'params': model.backbone.parameters(), 'lr': 1e-5},      # ä¸»éª¨å¹²å­¦ä¹ ç‡å°
        {'params': model.classifier.parameters(), 'lr': 2e-4}     # åˆ†ç±»å¤´å­¦ä¹ ç‡å¤§
    ])
    
    # âœ… ä½¿ç”¨æ ‡ç­¾å¹³æ»‘é˜²æ­¢è¿‡æ‹Ÿåˆ
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

    # Step 6: è®­ç»ƒå¾ªç¯
    best_val_acc = 0.0
    for epoch in range(NUM_EPOCHS):
        print(f"\nâ•â•â•â•â•â•â•â• EPOCH {epoch+1}/{NUM_EPOCHS} â•â•â•â•â•â•â•â•")
        
        train_loss = train_epoch(model, train_loader, optimizer, criterion, DEVICE, accumulation_steps=16)
        print(f"ğŸ“ˆ è®­ç»ƒé›†æŸå¤±: {train_loss:.4f}")
        
        val_loss, val_acc = eval_model(model, val_loader, criterion, DEVICE)
        print(f"ğŸ“Š éªŒè¯é›†æŸå¤±: {val_loss:.4f} | å‡†ç¡®ç‡: {val_acc:.4f}")
        
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), "best_model.pth")
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ â†’ éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
    
    # æµ‹è¯•é˜¶æ®µ
    print("\nğŸ”„ åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•...")
    model.load_state_dict(torch.load("best_model.pth"))
    test_loss, test_acc = eval_model(model, test_loader, criterion, DEVICE)
    print("\n========== æœ€ç»ˆæµ‹è¯•ç»“æœ ==========")
    print(f"ğŸ§ª æµ‹è¯•é›†æŸå¤±: {test_loss:.4f}")
    print(f"ğŸ† æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}")
    print("===============================")


if __name__ == "__main__":
    main()