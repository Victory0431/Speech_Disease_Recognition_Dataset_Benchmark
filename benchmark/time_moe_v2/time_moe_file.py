# -*- coding: utf-8 -*-
"""
ICASSP Benchmark: Time-MoE + MLP for Speech Disease Classification
Dataset: Parkinson_3700
Model: Frozen Time-MoE + Trainable MLP Head
Features: Windowing (512, 30% overlap), 8kHz, Masked Pooling
"""

import os
import sys
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from torch.amp import autocast, GradScaler
import librosa
import numpy as np
import pandas as pd
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¼•å…¥é€šç”¨å·¥å…·ç»„ä»¶
sys.path.append(str(Path(__file__).parent.parent / "tools"))
# from models.moe_classifier import DiseaseClassifier
from models.moe_classifier_unfreeze import DiseaseClassifier
# from moe_dataset.speech_disease_dataset import SpeechDiseaseDataset
from moe_dataset.speech_disease_dataset_v2 import SpeechDiseaseDataset

# ===========================================
# 1. é…ç½®å‚æ•°
# ===========================================
DEVICE = torch.device("cuda:7" if torch.cuda.is_available() else "cpu")
SAMPLE_RATE_ORIG = None  # ä¿ç•™åŸå§‹é‡‡æ ·ç‡
SAMPLE_RATE = 8000
WINDOW_LENGTH = 512      # L=512
HOP_LENGTH = int(WINDOW_LENGTH * 0.7)  # 30% overlap â†’ hop=358
BATCH_SIZE = 2
NUM_EPOCHS = 10
NUM_WORKERS = 16
LEARNING_RATE = 1e-3
N_MAX = None  # ç¨åç»Ÿè®¡ 95% åˆ†ä½æ•°
DATA_ROOT = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/dataset/Parkinson_3700"
# ===========================================
# é…ç½®å‚æ•°ï¼ˆæ–°å¢ï¼‰
# ===========================================
BACKBONE_PATH = "/mnt/data/test1/repo/Time-MoE/pretrain_model"  # ä½ çš„é¢„è®­ç»ƒè·¯å¾„
NUM_CLASSES = 2  # å¯æ”¹ä¸º3æˆ–4
FREEZE_BACKBONE = True  # å¿…é¡»ä¸º True


# ===========================================
# 6. Collate Functionï¼ˆè¡¥é›¶ + Maskï¼‰
# ===========================================
# ä¿®æ”¹ collate_fn ä¸­çš„ mask å¤„ç†
def collate_fn(batch):
    global N_MAX
    # print (N_MAX)
    windows_list = [item[0] for item in batch]
    labels = [item[1] for item in batch]
    lengths = [item[2] for item in batch]

    padded_windows = []
    masks = []

    for windows, length in zip(windows_list, lengths):
        if length > N_MAX:
            windows = windows[:N_MAX]
            mask = torch.ones(N_MAX, dtype=torch.bool)  # bool
        else:
            pad_len = N_MAX - length
            padding = torch.zeros(pad_len, windows.shape[1])
            windows = torch.cat([windows, padding], dim=0)
            mask = torch.cat([
                torch.ones(length, dtype=torch.bool),
                torch.zeros(pad_len, dtype=torch.bool)
            ])
        padded_windows.append(windows)
        masks.append(mask)

    x = torch.stack(padded_windows)     # [B, N_MAX, L]
    y = torch.LongTensor(labels)        # [B]
    mask = torch.stack(masks)           # [B, N_MAX], dtype=bool

    return x, y, mask


# ===========================================
# 7. è®­ç»ƒä¸è¯„ä¼°
# ===========================================
def train_epoch_old(model, dataloader, optimizer, criterion, device):
    model.train()
    total_loss = 0.0
    scaler = GradScaler()

    for i, (x, y, mask) in enumerate(tqdm(dataloader, desc="Training")):
        # æ‰“å°è¾“å…¥å°ºå¯¸
        B, N, L = x.shape
        valid_windows = mask.sum(dim=1)  # æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆçª—å£æ•°
        # print(f"\n[Batch {i+1}] è¾“å…¥å°ºå¯¸: x={x.shape}, y={y.shape}")
        # print(f"           æœ‰æ•ˆçª—å£æ•°: min={valid_windows.min().item()}, "
        #       f"max={valid_windows.max().item()}, "
        #       f"mean={valid_windows.float().mean().item()}")
        # print(f"           æ€»çª—å£æ•°: {B * N} (B={B}, N={N})")

        x, y, mask = x.to(device), y.to(device), mask.to(device)
        optimizer.zero_grad()

        # with autocast(device_type='cuda', dtype=torch.float16):
        logits = model(x, mask)
        loss = criterion(logits, y)
        if torch.isnan(loss):
            print(f"âŒ Loss is nan at batch {i}")
            optimizer.zero_grad()
            continue

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        total_loss += loss.item()

    return total_loss / len(dataloader)

def train_epoch(model, dataloader, optimizer, criterion, device,
                accumulation_steps=8):          # K æ¬¡ micro-batch ç´¯ç§¯
    model.train()
    total_loss = 0.0
    scaler = GradScaler()

    # 1. å…ˆæ¸…é›¶ï¼Œå‡†å¤‡ç´¯ç§¯
    optimizer.zero_grad()

    for step, (x, y, mask) in enumerate(tqdm(dataloader, desc="Training")):
        # 2. æŠŠæ•°æ®æ¬åˆ° GPU
        x, y, mask = x.to(device), y.to(device), mask.to(device)

        # 3. å‰å‘ + è®¡ç®— lossï¼ˆmicro-batch = 1ï¼‰
        logits = model(x, mask)
        loss = criterion(logits, y)

        # 4. æŒ‰ç´¯ç§¯æ­¥æ•°ç¼©æ”¾ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        loss = loss / accumulation_steps

        # 5. åå‘ä¼ æ’­ï¼ˆæ¢¯åº¦ç´¯ç§¯ï¼‰
        scaler.scale(loss).backward()

        # 6. ç»Ÿè®¡çœŸå® lossï¼ˆç”¨äºæ‰“å° / æ—¥å¿—ï¼‰
        total_loss += loss.item() * accumulation_steps

        # 7. æ¯ç´¯ç§¯ K æ­¥ï¼Œç»Ÿä¸€æ›´æ–°ä¸€æ¬¡å‚æ•°
        if (step + 1) % accumulation_steps == 0 or (step + 1) == len(dataloader):
            # å¯é€‰ï¼šæ¢¯åº¦è£å‰ª
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            scaler.step(optimizer)   # çœŸæ­£æ›´æ–°
            scaler.update()
            optimizer.zero_grad()    # æ¸…é›¶ï¼Œå‡†å¤‡ä¸‹ä¸€è½®ç´¯ç§¯

    # 8. è¿”å›å¹³å‡ loss
    return total_loss / len(dataloader)

def eval_model(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0

    # ç”¨äºæ”¶é›†æ‰€æœ‰é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
    all_preds = []
    all_labels = []
    all_logits = []

    with torch.no_grad():
        for batch_idx, (x, y, mask) in enumerate(tqdm(dataloader, desc="Evaluating")):
            x, y, mask = x.to(device), y.to(device), mask.to(device)
            logits = model(x, mask)  # [B, 2]
            loss = criterion(logits, y)
            total_loss += loss.item()

            pred = logits.argmax(dim=1)  # [B]
            correct += (pred == y).sum().item()
            total += y.size(0)

            # æ”¶é›†å½“å‰ batch çš„ç»“æœ
            all_logits.append(logits.cpu())
            all_preds.append(pred.cpu())
            all_labels.append(y.cpu())

            # ğŸ”¥ æ‰“å°ç¬¬ä¸€ä¸ª batch çš„è¯¦ç»†ä¿¡æ¯ï¼ˆä»…ä¸€æ¬¡ï¼‰
            if batch_idx == 0:
                print("\n" + "="*50)
                print("ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šç¬¬ä¸€ä¸ªéªŒè¯ batch")
                print("="*50)
                print(f"è¾“å…¥ x.shape: {x.shape}")          # åº”ä¸º [B, N_MAX, 512] æˆ– [B, N_MAX, 1]
                print(f"æ ‡ç­¾ y: {y.tolist()}")
                print(f"logits: {logits.tolist()}")
                print(f"é¢„æµ‹ pred: {pred.tolist()}")
                print(f"æŸå¤± loss: {loss.item():.4f}")
                print(f"è¯¥ batch å‡†ç¡®ç‡: {(pred == y).float().mean().item():.4f}")
                print(f"logits å·®å€¼ |logits[:,0] - logits[:,1]|: {(logits[:,0] - logits[:,1]).abs().tolist()}")
                print("="*50)

    # æ‹¼æ¥æ‰€æœ‰ batch
    all_logits = torch.cat(all_logits)
    all_preds = torch.cat(all_preds)
    all_labels = torch.cat(all_labels)

    acc = correct / total

    # ğŸ”¥ æ‰“å°å…¨å±€ç»Ÿè®¡
    print("\n" + "="*50)
    print("ğŸ“Š éªŒè¯é›†å…¨å±€ç»Ÿè®¡")
    print("="*50)
    print(f"æ€»æ ·æœ¬æ•°: {total}")
    print(f"çœŸå®æ ‡ç­¾åˆ†å¸ƒ: ç±»åˆ« 0 = {all_labels.eq(0).sum().item()}, ç±»åˆ« 1 = {all_labels.eq(1).sum().item()}")
    print(f"æ¨¡å‹é¢„æµ‹åˆ†å¸ƒ: é¢„æµ‹ 0 = {all_preds.eq(0).sum().item()}, é¢„æµ‹ 1 = {all_preds.eq(1).sum().item()}")
    
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰é¢„æµ‹éƒ½ä¸€æ ·
    if all_preds.unique().size(0) == 1:
        print(f"ğŸš¨ è­¦å‘Šï¼šæ¨¡å‹æ‰€æœ‰é¢„æµ‹å‡ä¸ºç±»åˆ« {all_preds[0].item()}ï¼")
    else:
        print("âœ… é¢„æµ‹ç»“æœæœ‰å˜åŒ–")

    # æŸ¥çœ‹ logits åˆ†å¸ƒ
    print(f"logits å‡å€¼: {all_logits.mean().item():.4f}, æ ‡å‡†å·®: {all_logits.std().item():.4f}")
    print(f"logits ç¬¬0ç±»å‡å€¼: {all_logits[:,0].mean().item():.4f}, ç¬¬1ç±»å‡å€¼: {all_logits[:,1].mean().item():.4f}")

    # æŸ¥çœ‹æ˜¯å¦ logits å·®å¼‚æå°ï¼ˆè¯´æ˜æ¨¡å‹æ²¡ä¿¡å¿ƒï¼‰
    logit_diff = (all_logits[:,0] - all_logits[:,1]).abs()
    print(f"logits å·®å€¼ |logit0 - logit1| å‡å€¼: {logit_diff.mean().item():.4f}")
    if logit_diff.mean() < 0.1:
        print("âš ï¸ è­¦å‘Šï¼šlogits å·®å€¼æå°ï¼Œæ¨¡å‹å‡ ä¹æ— æ³•åŒºåˆ†ä¸¤ç±»ï¼")

    return total_loss / len(dataloader), acc



def main():
    print(f"&#128293; ä½¿ç”¨è®¾å¤‡: {DEVICE}")
    print("------------------")
    print(f"&#128230; æ•°æ®é…ç½®: batch_size={BATCH_SIZE}, workers={NUM_WORKERS}")
    print(f"&#127925; éŸ³é¢‘å‚æ•°: sample_rate={SAMPLE_RATE}, window={WINDOW_LENGTH}, hop={HOP_LENGTH}")
    print("------------------\n")

    # ä¸€è¡Œä»£ç è·å–æ‰€æœ‰ dataloader + N_MAX
    train_loader, val_loader, test_loader, N_MAX = SpeechDiseaseDataset.get_dataloaders(
        data_root=DATA_ROOT,
        sample_rate=SAMPLE_RATE,
        n_fft=WINDOW_LENGTH,
        hop_length=HOP_LENGTH,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS,
        q_percentile=95,
        seed=42
    )

    # Step 4: åˆå§‹åŒ–æ¨¡å‹
    model = DiseaseClassifier(
        backbone_path=BACKBONE_PATH,
        num_classes=NUM_CLASSES,
        device=DEVICE,
        freeze_backbone=False,  # ä¸å†»ç»“ä¸»å¹²
        unfreeze_last_n=0  # è§£å†»æœ€å2å±‚ï¼ˆå¯æ ¹æ®æ•ˆæœè°ƒæ•´1~3ï¼‰
    )
    model = model.to(DEVICE)
    # model.backbone.requires_grad_(False)  # å†»ç»“ä¸»å¹²
    # print(f"&#127959;ï¸ æ¨¡å‹æ¶æ„: {model}")
    # print(f"&#9876;ï¸ å¯è®­ç»ƒå‚æ•°æ•°é‡: {sum(p.numel() for p in model.classifier.parameters())}\n")

    # æ‰“å°å¯è®­ç»ƒå‚æ•°ï¼ˆéªŒè¯è§£å†»æ˜¯å¦ç”Ÿæ•ˆï¼‰
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"&#9876;ï¸ æ€»å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params}")
    # å¯å•ç‹¬æ‰“å°ä¸»å¹²å’Œè§£å†»å±‚çš„å‚æ•°æ•°é‡ï¼Œç¡®è®¤è§£å†»æ˜¯å¦æ­£ç¡®
    backbone_trainable = sum(p.numel() for p in model.backbone.parameters() if p.requires_grad)
    print(f"&#9876;ï¸ ä¸»å¹²å¯è®­ç»ƒå‚æ•°æ•°é‡: {backbone_trainable}")
    classifier_trainable = sum(p.numel() for p in model.classifier.parameters() if p.requires_grad)
    print(f"&#9876;ï¸ åˆ†ç±»å¤´å¯è®­ç»ƒå‚æ•°æ•°é‡: {classifier_trainable}\n")

    # Step 5: ä¼˜åŒ–å™¨ & æŸå¤±
    # optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)
    optimizer = optim.Adam([
        {'params': model.backbone.parameters(), 'lr': 1e-5},  # ä¸»å¹²è§£å†»å±‚
        {'params': model.classifier.parameters(), 'lr': 2e-4}  # åˆ†ç±»å¤´
    ])
    criterion = nn.CrossEntropyLoss()

    # Step 6: è®­ç»ƒå¾ªç¯
    best_val_acc = 0.0
    for epoch in range(NUM_EPOCHS):
        print(f"\nâ•â•â•â•â•â•â•â• EPOCH {epoch+1}/{NUM_EPOCHS} â•â•â•â•â•â•â•â•")
        
        # è®­ç»ƒé˜¶æ®µ
        train_loss = train_epoch(model, train_loader, optimizer, criterion, DEVICE)
        print(f"&#128200; è®­ç»ƒé›†æŸå¤±: {train_loss:.4f}")
        
        # éªŒè¯é˜¶æ®µ
        val_loss, val_acc = eval_model(model, val_loader, criterion, DEVICE)
        print(f"&#128269; éªŒè¯é›†æŸå¤±: {val_loss:.4f} | å‡†ç¡®ç‡: {val_acc:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), "best_model.pth")
            print(f"&#128190; ä¿å­˜æœ€ä½³æ¨¡å‹ â†’ éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
    
    # æµ‹è¯•é˜¶æ®µ
    model.load_state_dict(torch.load("best_model.pth"))
    test_loss, test_acc = eval_model(model, test_loader, criterion, DEVICE)
    print("\n========== æœ€ç»ˆæµ‹è¯•ç»“æœ ==========")
    print(f"&#129514; æµ‹è¯•é›†æŸå¤±: {test_loss:.4f}")
    print(f"&#127942; æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}")
    print("===============================")


if __name__ == "__main__":
    main()