# ===========================================
# 3. é«˜æ•ˆç‰ˆï¼šTime-MoE ä¸»å¹² + è§£å†»æœ€å N å±‚ï¼ˆé€‚é… 4090ï¼‰
# ===========================================
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM


class TimeMoEBackbone(nn.Module):
    def __init__(self, backbone_path, device, unfreeze_last_n=0):
        super().__init__()
        self.device = device

        # åŠ è½½é¢„è®­ç»ƒ Time-MoE
        self.model = AutoModelForCausalLM.from_pretrained(
            backbone_path,
            trust_remote_code=True,
        ).to(device)

        # è·å–æ‰€æœ‰ transformer å±‚ï¼ˆå…³é”®ï¼šç¡®è®¤ Time-MoE çš„ç»“æ„ï¼‰
        # å¸¸è§å‘½åï¼š
        # - Mistral-like: model.layers
        # - T5-like: encoder.block æˆ– decoder.block
        # - è‡ªå®šä¹‰ MoE: å¯èƒ½æ˜¯ model.layers æˆ– transformer.h
        # æˆ‘ä»¬å…ˆå‡è®¾æ˜¯ model.layersï¼ˆæœ€å¸¸è§ï¼‰
        try:
            self.layers = self.model.model.layers  # å°è¯•æ ‡å‡†å‘½å
        except AttributeError:
            try:
                self.layers = self.model.model.decoder.block  # T5 é£æ ¼
            except AttributeError:
                raise ValueError("æ— æ³•æ‰¾åˆ° Time-MoE çš„ transformer å±‚ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç»“æ„")

        num_layers = len(self.layers)
        print(f"ğŸ“Š Time-MoE æ€»å±‚æ•°: {num_layers}")

        # å†»ç»“æ‰€æœ‰å‚æ•°
        for param in self.model.parameters():
            param.requires_grad = False

        # è§£å†»æœ€å N å±‚
        if unfreeze_last_n > 0:
            target_layers = self.layers[-unfreeze_last_n:]
            for layer in target_layers:
                for param in layer.parameters():
                    param.requires_grad = True
            print(f"ğŸ”“ å·²è§£å†»æœ€å {unfreeze_last_n} å±‚ Transformer")
        else:
            print(f"âœ… ä¸»å¹²å®Œå…¨å†»ç»“ï¼ˆä»…ä½¿ç”¨ç‰¹å¾æå–ï¼‰")

        # è·å–éšè—ç»´åº¦
        self.d_model = self.model.config.hidden_size

    def forward(self, x):
        """
        Args:
            x: [B, L]  å•ä¸ªçª—å£çš„éŸ³é¢‘ï¼ˆL=512ï¼‰
        Returns:
            pooled_feat: [B, D]  æ¯ä¸ªçª—å£çš„å…¨å±€ç‰¹å¾
        """
        x = x.to(self.device)  # [B, L]
        inputs = x.unsqueeze(-1)  # [B, L, 1] â†’ Time-MoE è¾“å…¥æ ¼å¼

        # ä¸å†ä½¿ç”¨ no_gradï¼Œå…è®¸æ¢¯åº¦å›ä¼ åˆ°è§£å†»å±‚
        outputs = self.model.model(input_ids=inputs, return_dict=True)
        hidden = outputs.last_hidden_state  # [B, L, D]

        # å…¨å±€å¹³å‡æ± åŒ–ï¼ˆæ—¶é—´ç»´åº¦ï¼‰
        pooled = hidden.mean(dim=1)  # [B, D]
        return pooled


class DiseaseClassifier(nn.Module):
    def __init__(self, backbone_path, num_classes=2, device='cuda', unfreeze_last_n=0):
        super().__init__()
        # åˆå§‹åŒ–ä¸»å¹²ï¼Œåªè§£å†»æœ€å N å±‚
        self.backbone = TimeMoEBackbone(
            backbone_path, 
            device, 
            unfreeze_last_n=unfreeze_last_n
        )
        d_model = self.backbone.d_model

        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.LayerNorm(d_model),
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(d_model // 2, num_classes)
        )

        # æ˜¾å¼åˆå§‹åŒ–åˆ†ç±»å¤´
        self._init_classifier()

    def _init_classifier(self):
        for m in self.classifier:
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1.0)

    def forward(self, x_windows, mask=None):
        B, N, L = x_windows.shape
        x_flat = x_windows.view(B * N, L)  # [B*N, L]

        window_features = self.backbone(x_flat)  # [B*N, D]
        window_features = window_features.view(B, N, -1)  # [B, N, D]

        # Masked æ± åŒ–
        if mask is not None:
            mask_expanded = mask.unsqueeze(-1)
            sum_features = (window_features * mask_expanded).sum(dim=1)
            num_valid = mask.sum(dim=1, keepdim=True).clamp(min=1.0)
            pooled = sum_features / num_valid
        else:
            pooled = window_features.mean(dim=1)

        logits = self.classifier(pooled)
        return logits