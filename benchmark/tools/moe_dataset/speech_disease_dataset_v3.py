# File: speech_disease_dataset.py
# è¯­éŸ³ç–¾ç—…åˆ†ç±»æ•°æ®é›† + åˆ†å±‚åˆ’åˆ† + ä¸€é”®è·å– DataLoader

import os
import librosa
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import StratifiedShuffleSplit
import logging
from typing import Dict, Optional, Tuple

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def collate_fn(batch, n_max: int):
    """
    åŠ¨æ€ç”Ÿæˆ mask çš„ collate_fn
    Args:
        batch: list of (windows, label, length)
        n_max: å…¨å±€ N_MAXï¼Œç”¨äº padding
    Returns:
        x: [B, N_MAX, L]
        y: [B]
        mask: [B, N_MAX] bool
    """
    B = len(batch)
    x = torch.zeros(B, n_max, 512, dtype=torch.float32)
    y = torch.tensor([b[1] for b in batch], dtype=torch.long)
    mask = torch.zeros(B, n_max, dtype=torch.bool)

    for i, (windows, _, length) in enumerate(batch):
        N_i = min(length, n_max)
        x[i, :N_i] = windows[:N_i]
        mask[i, :N_i] = True

    return x, y, mask


class SpeechDiseaseDataset(Dataset):
    """
    é€šç”¨è¯­éŸ³ç–¾ç—…åˆ†ç±»æ•°æ®é›†
    - è‡ªåŠ¨æ‰«æç›®å½•
    - è¿‡æ»¤ç©º/æŸåæ–‡ä»¶
    - åˆ†å¸§ + è¡¥é›¶
    - ç»Ÿè®¡æ¨è N_MAX
    - æ”¯æŒè‡ªå®šä¹‰æ ‡ç­¾æ˜ å°„
    - æ”¯æŒåˆ†å±‚åˆ’åˆ†ä¸ä¸€é”®è·å– dataloader
    """

    def __init__(
        self,
        data_root: str,
        sample_rate: int = 8000,
        n_fft: int = 512,
        hop_length: int = 358,
        preload_length: bool = False
    ):
        self.data_root = data_root
        self.sample_rate = sample_rate
        self.n_fft = n_fft
        self.hop_length = hop_length

        # ä»¥ä¸‹ä¸¤ä¸ªåˆ—è¡¨å¿…é¡»ç”±å­ç±»åœ¨ _scan_and_validate_files() ä¸­å¡«å……
        self.file_list: List[str] = []
        self.labels: List[int] = []

        # å¯é€‰ï¼šé¢„åŠ è½½æ¯ä¸ªæ ·æœ¬åˆ†çª—åçš„é•¿åº¦ï¼ˆç”¨äºæ¨è N_MAXï¼‰
        self.lengths: List[int] = []

        # ç¬¬ä¸€æ­¥ï¼šæ‰«æå¹¶éªŒè¯æ–‡ä»¶ï¼ˆç”±å­ç±»å®ç°ï¼‰
        self._scan_and_validate_files()

        if len(self.file_list) == 0:
            raise ValueError("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆéŸ³é¢‘æ–‡ä»¶ï¼è¯·æ£€æŸ¥æ•°æ®è·¯å¾„æˆ–é‡å†™ _scan_and_validate_filesã€‚")

        # ç¬¬äºŒæ­¥ï¼šé¢„åŠ è½½é•¿åº¦ï¼ˆå¯é€‰ï¼‰
        if preload_length:
            self._preload_lengths()

    def _scan_and_validate_files(self) -> None:
        """
        ã€æŠ½è±¡æ–¹æ³•ã€‘å­ç±»å¿…é¡»å®ç°æ­¤å‡½æ•°ï¼Œå®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
            - æ‰«æ data_root ä¸‹çš„éŸ³é¢‘æ–‡ä»¶
            - éªŒè¯æ–‡ä»¶æœ‰æ•ˆæ€§ï¼ˆéç©ºã€å¯è¯»ï¼‰
            - å¡«å…… self.file_list å’Œ self.labels

        ç¤ºä¾‹ï¼š
            self.file_list.append("/path/to/audio.wav")
            self.labels.append(0)  # 0: healthy, 1: dysphonia, etc.
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° _scan_and_validate_files æ–¹æ³•ï¼")

    def load_audio(self, path: str) -> np.ndarray:
        try:
            wav, sr = librosa.load(path, sr=None)
            if sr != self.sample_rate:
                wav = librosa.resample(wav, orig_sr=sr, target_sr=self.sample_rate)
            # âœ… å³°å€¼å½’ä¸€åŒ–ï¼šç¼©æ”¾åˆ° [-1, 1]
            wav = wav / (np.max(np.abs(wav)) + 1e-8)  # é˜²æ­¢é™¤é›¶
            return wav
        except Exception as e:
            logger.error(f"âŒ åŠ è½½éŸ³é¢‘å¤±è´¥ {path}: {e}")
            return np.zeros(1)

    def split_into_windows(self, wav: np.ndarray) -> np.ndarray:
        wav = np.asarray(wav, dtype=np.float32)
        if len(wav) == 0:
            return np.zeros((1, self.n_fft), dtype=np.float32)

        windows = []
        for i in range(0, len(wav) - self.n_fft + 1, self.hop_length):
            window = wav[i:i + self.n_fft]
            windows.append(window)

        if len(windows) == 0:
            padded = np.zeros(self.n_fft, dtype=np.float32)
            copy_len = min(len(wav), self.n_fft)
            padded[:copy_len] = wav[:copy_len]
            windows.append(padded)
        else:
            last_end = (len(windows) - 1) * self.hop_length + self.n_fft
            if last_end < len(wav):
                end = len(wav) - self.n_fft
                window = wav[end:end + self.n_fft]
                windows.append(window)

        return np.array(windows)

    def _preload_lengths(self):
        logger.info("ğŸ“Š é¢„åŠ è½½æ‰€æœ‰æ ·æœ¬çª—å£æ•°é‡...")
        self.lengths = []
        for file_path in self.file_list:
            try:
                wav = self.load_audio(file_path)
                windows = self.split_into_windows(wav)
                self.lengths.append(len(windows))
            except Exception as e:
                logger.warning(f"è·å–é•¿åº¦å¤±è´¥ {file_path}: {e}")
                self.lengths.append(1)
        logger.info(f"ğŸ“Š é¢„åŠ è½½å®Œæˆï¼Œå…± {len(self.lengths)} ä¸ªæ ·æœ¬")

    def get_recommended_N_max(self, q: float = 95) -> int:
        if hasattr(self, 'lengths') and len(self.lengths) > 0:
            lengths = self.lengths
        else:
            logger.info("ğŸ“ æ­£åœ¨ç»Ÿè®¡çª—å£æ•°é‡åˆ†å¸ƒï¼ˆé¦–æ¬¡è®¡ç®—ï¼‰...")
            lengths = []
            for i in range(len(self)):
                try:
                    _, _, length = self[i]
                    lengths.append(length)
                except Exception as e:
                    logger.warning(f"æ ·æœ¬ {i} è·å–é•¿åº¦å¤±è´¥: {e}")
            if not lengths:
                raise ValueError("æ— æ³•è·å–ä»»ä½•æ ·æœ¬é•¿åº¦")

        n_max = int(np.percentile(lengths, q))
        logger.info(f"ğŸ“ˆ {q}th ç™¾åˆ†ä½æ•° N_max = {n_max}")
        return n_max

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        wav = self.load_audio(self.file_list[idx])
        windows = self.split_into_windows(wav)
        label = self.labels[idx]
        length = len(windows)
        return torch.FloatTensor(windows), label, length

    # ================================================
    # æ–°å¢ï¼šä¸€é”®è·å–åˆ†å±‚åˆ’åˆ†çš„ dataloader
    # ================================================

    @classmethod
    def get_dataloaders(
        cls,
        data_root: str,
        sample_rate: int = 8000,
        n_fft: int = 512,
        hop_length: int = 358,
        label_map: Optional[Dict[str, int]] = None,
        train_ratio: float = 0.8,
        val_ratio: float = 0.1,
        test_ratio: float = 0.1,
        batch_size: int = 16,
        n_max: Optional[int] = None,
        q_percentile: float = 95,
        seed: int = 42,
        num_workers: int = 16
    ) -> Tuple[DataLoader, DataLoader, DataLoader, int]:
        """
        ä¸€è¡Œä»£ç è·å–è®­ç»ƒ/éªŒè¯/æµ‹è¯• DataLoaderï¼ˆæ”¯æŒåˆ†å±‚åˆ’åˆ†ï¼‰

        Returns:
            train_loader, val_loader, test_loader, N_MAX
        """
        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-5, "æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º 1"

        # Step 1: åˆ›å»ºå®Œæ•´æ•°æ®é›†
        dataset = cls(
            data_root=data_root,
            sample_rate=sample_rate,
            n_fft=n_fft,
            hop_length=hop_length,
            label_map=label_map,
            preload_length=True
        )

        # Step 2: è·å– N_MAX
        if n_max is None:
            n_max = dataset.get_recommended_N_max(q=q_percentile)
        logger.info(f"âœ… ä½¿ç”¨ N_MAX = {n_max}")

        # Step 3: åˆ†å±‚åˆ’åˆ†
        splitter = StratifiedShuffleSplit(n_splits=1, train_size=train_ratio, test_size=val_ratio+test_ratio, random_state=seed)
        train_idx, val_test_idx = next(splitter.split(dataset.file_list, dataset.labels))

        # å†æ¬¡åˆ’åˆ† val/test
        val_ratio_of_rest = val_ratio / (val_ratio + test_ratio)
        splitter2 = StratifiedShuffleSplit(n_splits=1, train_size=val_ratio_of_rest, random_state=seed)
        val_idx, test_idx = next(splitter2.split(
            [dataset.file_list[i] for i in val_test_idx],
            [dataset.labels[i] for i in val_test_idx]
        ))

        # æ˜ å°„å›åŸå§‹ç´¢å¼•
        val_idx = [val_test_idx[i] for i in val_idx]
        test_idx = [val_test_idx[i] for i in test_idx]

        logger.info(f"ğŸ”¢ åˆ’åˆ†å®Œæˆ: train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}")
        logger.info(f"ğŸ“Š è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ: {np.bincount([dataset.labels[i] for i in train_idx])}")
        logger.info(f"ğŸ“Š éªŒè¯é›†ç±»åˆ«åˆ†å¸ƒ: {np.bincount([dataset.labels[i] for i in val_idx])}")
        logger.info(f"ğŸ“Š æµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒ: {np.bincount([dataset.labels[i] for i in test_idx])}")

        # Step 4: åˆ›å»ºå­é›†
        train_dataset = torch.utils.data.Subset(dataset, train_idx)
        val_dataset = torch.utils.data.Subset(dataset, val_idx)
        test_dataset = torch.utils.data.Subset(dataset, test_idx)

        # Step 5: åˆ›å»º DataLoaderï¼ˆä½¿ç”¨å¸¦ n_max çš„ collate_fnï¼‰
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            collate_fn=lambda batch: collate_fn(batch, n_max=n_max),
            num_workers=num_workers
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            collate_fn=lambda batch: collate_fn(batch, n_max=n_max),
            num_workers=num_workers
        )
        test_loader = DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False,
            collate_fn=lambda batch: collate_fn(batch, n_max=n_max),
            num_workers=num_workers
        )

        return train_loader, val_loader, test_loader, n_max