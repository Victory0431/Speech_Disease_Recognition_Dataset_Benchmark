import os
import warnings
import numpy as np
import pandas as pd
import librosa
import torch
from tqdm import tqdm
from transformers import Wav2Vec2Processor, Wav2Vec2Model
import argparse


# ===================== 1. é…ç½®å‚æ•° =====================
class Config:
    MODEL_PATH = "/mnt/data/test1/repo/wav2vec_2/model"  # Wav2Vec2æ¨¡å‹è·¯å¾„
    TEMP_DIR = "./temp_windows"  # ä¸´æ—¶çª—å£æ•°æ®ä¿å­˜ç›®å½•
    SAVE_FEAT_DIR = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/768_npy_charactristic"  # æœ€ç»ˆç‰¹å¾ä¿å­˜ç›®å½•
    
    # éŸ³é¢‘å¤„ç†å‚æ•°ï¼ˆä¿æŒä½ çš„æœ€æ–°é…ç½®ï¼‰
    SAMPLE_RATE = 16000
    WINDOW_SIZE = 32000  # å·²ä¿®æ”¹ä¸º32000
    MAX_AUDIO_DURATION = 1800  # æœ€å¤§éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
    MAX_AUDIO_SAMPLES = SAMPLE_RATE * MAX_AUDIO_DURATION  # æœ€å¤§é‡‡æ ·ç‚¹æ•°
    
    # GPUæ¨ç†å‚æ•°ï¼ˆGPU_DEVICEå°†é€šè¿‡å‘½ä»¤è¡Œå‚æ•°åŠ¨æ€è®¾ç½®ï¼‰
    WINDOW_BATCH_SIZE = 64  # æ¯æ‰¹æ¨ç†çš„çª—å£æ•°
    GPU_DEVICE = "cuda:7"    # é»˜è®¤å€¼ï¼Œå°†è¢«å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    DTYPE = np.float32       # ç‰¹å¾æ•°æ®ç±»å‹


# åˆ›å»ºå¿…è¦ç›®å½•
os.makedirs(Config.TEMP_DIR, exist_ok=True)
os.makedirs(Config.SAVE_FEAT_DIR, exist_ok=True)
# è¿‡æ»¤Librosa/Transformersè­¦å‘Š
warnings.filterwarnings("ignore", message="PySoundFile failed. Trying audioread instead.")
warnings.filterwarnings("ignore", category=FutureWarning, message="librosa.core.audio.__audioread_load")


# ===================== 2. å‘½ä»¤è¡Œå‚æ•°è§£æï¼ˆæ–°å¢--gpuå‚æ•°ï¼‰ =====================
def parse_args():
    parser = argparse.ArgumentParser(description="Extract Wav2Vec2 features from audio dataset (class-level directory).")
    parser.add_argument("--input_dir", required=True, 
                        help="Path to the class-level audio directory (e.g., /path/to/DATASET/CLASS).")
    parser.add_argument("--gpu", type=int, required=True, 
                        help="GPU device ID (0-7) to use for inference.")
    args = parser.parse_args()
    
    # åŠ¨æ€è®¾ç½®GPUè®¾å¤‡
    Config.GPU_DEVICE = f"cuda:{args.gpu}"
    return args


# ===================== 3. éŸ³é¢‘åŠ è½½ä¸åˆ†çª—å·¥å…· =====================
def load_single_audio(file_path):
    """åŠ è½½å•æ¡éŸ³é¢‘å¹¶æˆªæ–­åˆ°æœ€å¤§æ—¶é•¿"""
    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            audio, _ = librosa.load(file_path, sr=Config.SAMPLE_RATE)
        if len(audio) > Config.MAX_AUDIO_SAMPLES:
            audio = audio[:Config.MAX_AUDIO_SAMPLES]
        return audio
    except Exception as e:
        print(f"âš ï¸ è·³è¿‡æŸåéŸ³é¢‘ï¼š{file_path} | é”™è¯¯ï¼š{str(e)[:100]}...")
        return None


def calculate_global_window_params(audio_paths):
    """è®¡ç®—å…¨å±€çª—å£æ•°ï¼ˆåŸºäº95%åˆ†ä½æ•°çš„éŸ³é¢‘é•¿åº¦ï¼‰"""
    print(f"\nğŸ“Š åˆ†æéŸ³é¢‘é•¿åº¦åˆ†å¸ƒï¼ˆå…± {len(audio_paths)} ä¸ªæ–‡ä»¶ï¼‰")
    valid_lengths = []
    for path in tqdm(audio_paths, desc="è®¡ç®—é•¿åº¦"):
        audio = load_single_audio(path)
        if audio is not None:
            valid_lengths.append(len(audio))
    if not valid_lengths:
        raise ValueError("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆéŸ³é¢‘æ–‡ä»¶ï¼")
    
    percentile_95 = np.percentile(valid_lengths, 95)
    window_count = int(np.ceil(percentile_95 / Config.WINDOW_SIZE))
    total_samples = window_count * Config.WINDOW_SIZE
    print(f"âœ… å…¨å±€çª—å£å‚æ•°ï¼šçª—å£æ•°={window_count} | æ€»é‡‡æ ·ç‚¹={total_samples}")
    return window_count, total_samples


def collect_audio_info(input_dir):
    """æ”¶é›†éŸ³é¢‘ä¿¡æ¯ï¼Œè‡ªåŠ¨è§£ææ•°æ®é›†åå’Œç±»åˆ«å"""
    # è§£ææ•°æ®é›†åï¼ˆå€’æ•°ç¬¬äºŒçº§ç›®å½•ï¼‰å’Œç±»åˆ«åï¼ˆæœ€åä¸€çº§ç›®å½•ï¼‰
    path_parts = os.path.normpath(input_dir).split(os.sep)
    dataset_name = path_parts[-2]
    class_name = path_parts[-1]
    print(f"ğŸ” æ£€æµ‹åˆ°ï¼šæ•°æ®é›†={dataset_name} | ç±»åˆ«={class_name}ï¼ˆä½¿ç”¨GPU {Config.GPU_DEVICE}ï¼‰")
    
    audio_info = []
    audio_id = 0
    # éå†è¾“å…¥ç›®å½•ä¸‹çš„WAV/MP3æ–‡ä»¶
    for f in os.listdir(input_dir):
        if f.endswith(".wav") or f.endswith(".mp3"):
            audio_info.append({
                "audio_id": audio_id,
                "path": os.path.join(input_dir, f),
                "label": class_name,      # æ ‡ç­¾ä¸ºç±»åˆ«å
                "dataset_name": dataset_name
            })
            audio_id += 1
    df = pd.DataFrame(audio_info)
    print(f"âœ… æ”¶é›†åˆ° {len(df)} ä¸ªæœ‰æ•ˆéŸ³é¢‘")
    return df, dataset_name, class_name


def split_all_audio_to_windows(audio_df, window_count, total_samples, dataset_name, class_name):
    """å¯¹æ‰€æœ‰éŸ³é¢‘åˆ†çª—å¹¶ä¿å­˜çª—å£æ•°æ®+å…ƒä¿¡æ¯"""
    print(f"\nğŸš€ å¼€å§‹ä¸ºã€Œ{dataset_name}__{class_name}ã€åˆ†çª—ï¼ˆå…± {len(audio_df)} ä¸ªéŸ³é¢‘ï¼‰")
    meta_info = []  # å­˜å‚¨æ¯ä¸ªéŸ³é¢‘çš„å…ƒä¿¡æ¯ï¼ˆç”¨äºåç»­æ¨ç†ï¼‰
    
    for _, row in tqdm(audio_df.iterrows(), total=len(audio_df), desc="åˆ†çª—è¿›åº¦"):
        audio_id = row["audio_id"]
        file_path = row["path"]
        current_label = row["label"]
        current_dataset = row["dataset_name"]
        
        # åŠ è½½å¹¶åˆ†çª—
        audio = load_single_audio(file_path)
        if audio is None:
            continue
        # è¡¥é›¶/æˆªæ–­åˆ°å›ºå®šé‡‡æ ·ç‚¹
        if len(audio) < total_samples:
            audio = np.pad(audio, (0, total_samples - len(audio)), mode="constant")
        else:
            audio = audio[:total_samples]
        # ç”Ÿæˆçª—å£
        windows = np.array([
            audio[i*Config.WINDOW_SIZE : (i+1)*Config.WINDOW_SIZE]
            for i in range(window_count)
        ], dtype=Config.DTYPE)  # å½¢çŠ¶ï¼š[window_count, WINDOW_SIZE]
        
        # ä¿å­˜çª—å£æ•°æ®
        window_save_path = os.path.join(
            Config.TEMP_DIR, 
            f"audio_{current_dataset}__{current_label}__{audio_id}_windows.npy"
        )
        np.save(window_save_path, windows)
        
        # è®°å½•å…ƒä¿¡æ¯
        meta_info.append({
            "audio_id": audio_id,
            "label": current_label,
            "window_count": window_count,
            "window_path": window_save_path,
            "dataset_name": current_dataset,
            "class_name": current_label
        })
    
    # ä¿å­˜å…ƒä¿¡æ¯åˆ°CSV
    meta_df = pd.DataFrame(meta_info)
    meta_save_path = os.path.join(
        Config.TEMP_DIR, 
        f"meta_{dataset_name}__{class_name}.csv"
    )
    meta_df.to_csv(meta_save_path, index=False)
    print(f"\nâœ… åˆ†çª—å®Œæˆï¼")
    print(f"   - æœ‰æ•ˆéŸ³é¢‘æ•°ï¼š{len(meta_df)}")
    print(f"   - å…ƒä¿¡æ¯ä¿å­˜ï¼š{meta_save_path}")
    print(f"   - çª—å£æ–‡ä»¶ç›®å½•ï¼š{Config.TEMP_DIR}")
    return meta_df, dataset_name, class_name


# ===================== 4. GPUæ‰¹é‡æ¨ç†çª—å£ç‰¹å¾ =====================
def gpu_batch_infer_windows(meta_df, dataset_name, class_name):
    """GPUæ‰¹é‡æ¨ç†çª—å£ç‰¹å¾å¹¶ä¿å­˜"""
    print(f"\nğŸ”§ åŠ è½½Wav2Vec2æ¨¡å‹ï¼ˆè®¾å¤‡ï¼š{Config.GPU_DEVICE}ï¼‰")
    device = torch.device(Config.GPU_DEVICE if torch.cuda.is_available() else "cpu")
    processor = Wav2Vec2Processor.from_pretrained(Config.MODEL_PATH)
    model = Wav2Vec2Model.from_pretrained(Config.MODEL_PATH).to(device)
    model.eval()  # æ¨ç†æ¨¡å¼ï¼Œå…³é—­è®­ç»ƒå±‚ï¼ˆå¦‚Dropoutï¼‰
    
    # æå–æ‰€æœ‰çª—å£è·¯å¾„å’ŒéŸ³é¢‘ID
    window_paths = meta_df["window_path"].tolist()
    audio_ids = meta_df["audio_id"].tolist()
    print(f"\nğŸš€ å¼€å§‹ä¸ºã€Œ{dataset_name}__{class_name}ã€æ¨ç†ï¼ˆå…± {len(window_paths)} ä¸ªéŸ³é¢‘ï¼Œæ‰¹æ¬¡å¤§å°={Config.WINDOW_BATCH_SIZE}ï¼‰")
    
    with torch.no_grad(), torch.cuda.amp.autocast():  # æ··åˆç²¾åº¦æ¨ç†ï¼ˆæé€Ÿ+çœæ˜¾å­˜ï¼‰
        for aid, wpath in tqdm(zip(audio_ids, window_paths), total=len(audio_ids), desc="æ¨ç†è¿›åº¦"):
            # åŠ è½½å•éŸ³é¢‘çš„æ‰€æœ‰çª—å£
            windows = np.load(wpath)  # å½¢çŠ¶ï¼š[window_count, WINDOW_SIZE]
            win_count = windows.shape[0]
            
            # æŒ‰æ‰¹æ¬¡æ¨ç†
            batch_feats_list = []
            for i in range(0, win_count, Config.WINDOW_BATCH_SIZE):
                batch_wins = windows[i:i+Config.WINDOW_BATCH_SIZE]  # å½¢çŠ¶ï¼š[batch_size, WINDOW_SIZE]
                # é¢„å¤„ç†ä¸ºæ¨¡å‹è¾“å…¥
                inputs = processor(
                    batch_wins.tolist(),
                    sampling_rate=Config.SAMPLE_RATE,
                    return_tensors="pt",
                    padding=False
                )["input_values"].to(device)
                # æ¨¡å‹æ¨ç† + æ—¶åºæ± åŒ–ï¼ˆç¬¬ä¸€æ¬¡æ± åŒ–ï¼‰
                outputs = model(input_values=inputs)
                batch_feats = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()  # å½¢çŠ¶ï¼š[batch_size, 768]
                batch_feats_list.append(batch_feats)
            
            # æ‹¼æ¥æ‰€æœ‰æ‰¹æ¬¡çš„ç‰¹å¾å¹¶ä¿å­˜
            all_feats = np.concatenate(batch_feats_list, axis=0)  # å½¢çŠ¶ï¼š[window_count, 768]
            feat_save_path = os.path.join(
                Config.TEMP_DIR, 
                f"audio_{dataset_name}__{class_name}__{aid}_window_feats.npy"
            )
            np.save(feat_save_path, all_feats)
    
    print(f"\nâœ… GPUæ¨ç†å®Œæˆï¼çª—å£ç‰¹å¾ä¿å­˜äºï¼š{Config.TEMP_DIR}")
    return dataset_name, class_name


# ===================== 5. èšåˆçª—å£ç‰¹å¾å¹¶ä¿å­˜æœ€ç»ˆç»“æœ =====================
def aggregate_window_feats(meta_df, dataset_name, class_name):
    """èšåˆçª—å£ç‰¹å¾ï¼Œä¿å­˜ä¸ºã€Œæ•°æ®é›†å__and__ç±»åˆ«å.npyã€"""
    print(f"\nğŸ“Š èšåˆçª—å£ç‰¹å¾ï¼ˆã€Œ{dataset_name}__{class_name}ã€ï¼Œå…± {len(meta_df)} ä¸ªéŸ³é¢‘ï¼‰")
    # ç±»åˆ«æ˜ å°„ï¼ˆå½“å‰ç±»åˆ«å›ºå®šä¸ºclass_nameï¼ŒIDä¸º0ï¼‰
    label_mapping = {class_name: 0}
    all_feats = []
    all_labels = []
    
    for _, row in tqdm(meta_df.iterrows(), total=len(meta_df), desc="èšåˆè¿›åº¦"):
        audio_id = row["audio_id"]
        label = row["label"]
        # åŠ è½½çª—å£ç‰¹å¾
        feat_path = os.path.join(
            Config.TEMP_DIR, 
            f"audio_{dataset_name}__{class_name}__{audio_id}_window_feats.npy"
        )
        win_feats = np.load(feat_path)  # å½¢çŠ¶ï¼š[window_count, 768]
        # çª—å£ç»´åº¦æ± åŒ–ï¼ˆç¬¬äºŒæ¬¡æ± åŒ–ï¼‰
        audio_feat = np.mean(win_feats, axis=0)  # å½¢çŠ¶ï¼š[768]
        # æ”¶é›†ç‰¹å¾å’Œæ ‡ç­¾
        all_feats.append(audio_feat)
        all_labels.append(label_mapping[label])
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_feats_np = np.array(all_feats, dtype=Config.DTYPE)  # å½¢çŠ¶ï¼š[N, 768]
    all_labels_np = np.array(all_labels, dtype=np.int64)    # å½¢çŠ¶ï¼š[N]
    
    # ä¿å­˜æœ€ç»ˆç‰¹å¾å’Œæ ‡ç­¾
    final_feat_name = f"{dataset_name}__and__{class_name}.npy"
    final_feat_path = os.path.join(Config.SAVE_FEAT_DIR, final_feat_name)
    np.save(final_feat_path, all_feats_np)
    
    final_label_name = f"{dataset_name}__and__{class_name}_labels.npy"
    final_label_path = os.path.join(Config.SAVE_FEAT_DIR, final_label_name)
    np.save(final_label_path, all_labels_np)
    
    print(f"\nğŸ‰ ç‰¹å¾èšåˆå®Œæˆï¼")
    print(f"   - æœ‰æ•ˆéŸ³é¢‘æ•°ï¼š{len(all_feats_np)}")
    print(f"   - ç‰¹å¾æ–‡ä»¶ï¼š{final_feat_path}")
    print(f"   - æ ‡ç­¾æ–‡ä»¶ï¼š{final_label_path}")
    return final_feat_path, final_label_path


# ===================== 6. ä¸»å‡½æ•°ï¼ˆæ–°å¢æ–‡ä»¶å­˜åœ¨æ£€æŸ¥ï¼‰ =====================
def main():
    args = parse_args()
    input_dir = args.input_dir
    
    # è§£ææ•°æ®é›†åå’Œç±»åˆ«åï¼Œç”¨äºæ£€æŸ¥ç‰¹å¾æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    path_parts = os.path.normpath(input_dir).split(os.sep)
    dataset_name = path_parts[-2]
    class_name = path_parts[-1]
    
    # æ£€æŸ¥æœ€ç»ˆç‰¹å¾æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™ç›´æ¥è·³è¿‡
    final_feat_path = os.path.join(
        Config.SAVE_FEAT_DIR, 
        f"{dataset_name}__and__{class_name}.npy"
    )
    final_label_path = os.path.join(
        Config.SAVE_FEAT_DIR, 
        f"{dataset_name}__and__{class_name}_labels.npy"
    )
    
    if os.path.exists(final_feat_path) and os.path.exists(final_label_path):
        print(f"â„¹ï¸ ç‰¹å¾æ–‡ä»¶å·²å­˜åœ¨ï¼š{final_feat_path}ï¼Œå°†è·³è¿‡å¤„ç†")
        return
    
    # Step 1ï¼šæ”¶é›†éŸ³é¢‘ä¿¡æ¯ + è®¡ç®—çª—å£å‚æ•° + åˆ†çª—
    audio_df, dataset_name, class_name = collect_audio_info(input_dir)
    if len(audio_df) == 0:
        print("âŒ æ— æœ‰æ•ˆéŸ³é¢‘æ–‡ä»¶ï¼Œç»ˆæ­¢æµç¨‹ã€‚")
        return
    window_count, total_samples = calculate_global_window_params(audio_df["path"].tolist())
    meta_df, dataset_name, class_name = split_all_audio_to_windows(audio_df, window_count, total_samples, dataset_name, class_name)
    
    # Step 2ï¼šGPUæ‰¹é‡æ¨ç†çª—å£ç‰¹å¾
    dataset_name, class_name = gpu_batch_infer_windows(meta_df, dataset_name, class_name)
    
    # Step 3ï¼šèšåˆç‰¹å¾å¹¶ä¿å­˜ä¸ºã€Œæ•°æ®é›†å__and__ç±»åˆ«å.npyã€
    aggregate_window_feats(meta_df, dataset_name, class_name)


if __name__ == "__main__":
    main()
