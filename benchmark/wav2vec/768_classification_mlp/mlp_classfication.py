import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import GradScaler, autocast
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
)
from imblearn.over_sampling import SMOTE  # è¿‡é‡‡æ ·åº“
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")


# ===================== 1. é…ç½®å‚æ•°ï¼ˆå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰ =====================
class Config:
    # æ ¸å¿ƒè·¯å¾„
    FEAT_ROOT = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/768_npy_charactristic"  # ç‰¹å¾æ–‡ä»¶ç›®å½•
    SAVE_MODEL_DIR = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/benchmark/wav2vec/768_classification_mlp/mlp_multi_class_best"  # æœ€ä¼˜æ¨¡å‹ä¿å­˜ç›®å½•
    PLOT_DIR = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/benchmark/wav2vec/768_classification_mlp/mlp_plots"  # æ··æ·†çŸ©é˜µç­‰å›¾è¡¨ä¿å­˜ç›®å½•
    
    # è®­ç»ƒå‚æ•°
    BATCH_SIZE = 64        # æ‰¹æ¬¡å¤§å°ï¼ˆRTX 4090å¯è®¾64-128ï¼‰
    EPOCHS = 50            # è®­ç»ƒè½®æ¬¡
    LEARNING_RATE = 1e-4   # å­¦ä¹ ç‡
    WEIGHT_DECAY = 1e-5    # æƒé‡è¡°å‡ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰
    DROPOUT = 0.3          # Dropoutæ¯”ä¾‹
    DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  # GPUè®¾å¤‡
    
    # æ•°æ®å¤„ç†å‚æ•°
    TEST_SIZE = 0.2        # æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆ8:2åˆ’åˆ†ï¼‰
    RANDOM_STATE = 42      # éšæœºç§å­ï¼ˆä¿è¯ç»“æœå¯å¤ç°ï¼‰
    N_SMOTE_NEIGHBORS = 5  # SMOTEè¿‡é‡‡æ ·çš„è¿‘é‚»æ•°ï¼ˆé»˜è®¤5ï¼Œå°æ ·æœ¬ç±»åˆ«å¯å‡å°è‡³3ï¼‰
    
    # è¯„ä¼°å‚æ•°
    EVAL_METRIC = "weighted"  # æŒ‡æ ‡è®¡ç®—æ–¹å¼ï¼ˆweighted=è€ƒè™‘ç±»åˆ«ä¸å¹³è¡¡ï¼‰
    PLOT_CONFUSION_MATRIX = True  # æ˜¯å¦ç»˜åˆ¶æ··æ·†çŸ©é˜µ


# åˆ›å»ºå¿…è¦ç›®å½•
os.makedirs(Config.SAVE_MODEL_DIR, exist_ok=True)
os.makedirs(Config.PLOT_DIR, exist_ok=True)


# ===================== 2. æ•°æ®åŠ è½½ä¸æ ‡ç­¾å¤„ç†ï¼ˆæ ¸å¿ƒï¼šæŒ‰æ–‡ä»¶åè§£æç±»åˆ«ï¼‰ =====================
def parse_class_from_filename(filename):
    """ä»ç‰¹å¾æ–‡ä»¶åè§£æç±»åˆ«ï¼ˆæ ¼å¼ï¼šæ•°æ®é›†å__and__ç±»åˆ«å.npyï¼‰"""
    # å»æ‰.npyåç¼€ï¼ŒæŒ‰"__and__"æ‹†åˆ†
    prefix = filename.replace(".npy", "")
    if "__and__" not in prefix:
        raise ValueError(f"æ–‡ä»¶åæ ¼å¼é”™è¯¯ï¼ˆéœ€ä¸ºã€Œæ•°æ®é›†å__and__ç±»åˆ«å.npyã€ï¼‰ï¼š{filename}")
    dataset_name, class_name = prefix.split("__and__", 1)  # ä»…æ‹†åˆ†ä¸€æ¬¡ï¼ˆé¿å…ç±»åˆ«åå«"__and__"ï¼‰
    return dataset_name, class_name


def load_all_features_and_labels(feat_root):
    """åŠ è½½æ‰€æœ‰ç‰¹å¾æ–‡ä»¶ï¼Œç”Ÿæˆæ ‡ç­¾æ˜ å°„ï¼ˆæ¯ä¸ªæ–‡ä»¶å¯¹åº”ä¸€ä¸ªç±»åˆ«ï¼‰"""
    print(f"ğŸ“Š ä» {feat_root} åŠ è½½ç‰¹å¾æ–‡ä»¶...")
    
    # 1. éå†ç›®å½•ï¼Œç­›é€‰ç‰¹å¾æ–‡ä»¶ï¼ˆå¿½ç•¥_labels.npyæ ‡ç­¾æ–‡ä»¶ï¼‰
    feat_files = []
    for f in os.listdir(feat_root):
        # å¿½ç•¥æ ‡ç­¾æ–‡ä»¶ï¼Œåªä¿ç•™ç‰¹å¾æ–‡ä»¶ï¼ˆæ ¼å¼ï¼šXXX__and__XXX.npyï¼‰
        if f.endswith(".npy") and "_labels.npy" not in f:
            feat_files.append(f)
    
    if len(feat_files) == 0:
        raise FileNotFoundError(f"âŒ åœ¨ {feat_root} æœªæ‰¾åˆ°æœ‰æ•ˆç‰¹å¾æ–‡ä»¶ï¼ˆéœ€ä¸ºã€Œæ•°æ®é›†å__and__ç±»åˆ«å.npyã€æ ¼å¼ï¼‰")
    
    # 2. ç”Ÿæˆç±»åˆ«æ˜ å°„ï¼ˆæ¯ä¸ªç±»åˆ«åˆ†é…å”¯ä¸€IDï¼‰
    all_classes = []
    for f in feat_files:
        _, class_name = parse_class_from_filename(f)
        all_classes.append(class_name)
    unique_classes = sorted(list(set(all_classes)))  # å»é‡å¹¶æ’åºï¼ˆä¿è¯IDç¨³å®šï¼‰
    class2id = {cls: idx for idx, cls in enumerate(unique_classes)}
    num_classes = len(unique_classes)
    print(f"âœ… ç”Ÿæˆç±»åˆ«æ˜ å°„ï¼šå…± {num_classes} ä¸ªç±»åˆ«")
    for cls, idx in class2id.items():
        print(f"   - ç±»åˆ«ï¼š{cls} â†’ IDï¼š{idx}")
    
    # 3. åŠ è½½ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆæ¯ä¸ªæ–‡ä»¶çš„æ‰€æœ‰æ ·æœ¬å¯¹åº”åŒä¸€ç±»åˆ«ï¼‰
    all_feats = []
    all_labels = []
    all_metadata = []  # å¯é€‰ï¼šè®°å½•æ ·æœ¬æ¥è‡ªå“ªä¸ªæ–‡ä»¶
    
    for f in tqdm(feat_files, desc="åŠ è½½ç‰¹å¾æ–‡ä»¶"):
        file_path = os.path.join(feat_root, f)
        dataset_name, class_name = parse_class_from_filename(f)
        current_label = class2id[class_name]
        
        # åŠ è½½ç‰¹å¾ï¼ˆå½¢çŠ¶ï¼š[æ ·æœ¬æ•°, 768]ï¼‰
        feats = np.load(file_path).astype(np.float32)
        if feats.shape[1] != 768:
            raise ValueError(f"âŒ {f} ç‰¹å¾ç»´åº¦é”™è¯¯ï¼ˆéœ€ä¸º768ç»´ï¼‰ï¼Œå®é™…ç»´åº¦ï¼š{feats.shape[1]}")
        
        # æ”¶é›†ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆè¯¥æ–‡ä»¶æ‰€æœ‰æ ·æœ¬æ ‡ç­¾ç›¸åŒï¼‰
        all_feats.append(feats)
        all_labels.extend([current_label] * feats.shape[0])  # æ¯ä¸ªæ ·æœ¬å¯¹åº”åŒä¸€ç±»åˆ«
        all_metadata.extend([(dataset_name, class_name)] * feats.shape[0])  # å¯é€‰ï¼šè®°å½•å…ƒä¿¡æ¯
    
    # 4. æ‹¼æ¥ä¸ºå…¨å±€æ•°ç»„
    all_feats = np.concatenate(all_feats, axis=0)  # [æ€»æ ·æœ¬æ•°, 768]
    all_labels = np.array(all_labels, dtype=np.int64)  # [æ€»æ ·æœ¬æ•°]
    
    print(f"\nğŸ“ˆ æ•°æ®åŠ è½½å®Œæˆï¼š")
    print(f"   - æ€»æ ·æœ¬æ•°ï¼š{all_feats.shape[0]}")
    print(f"   - ç‰¹å¾ç»´åº¦ï¼š{all_feats.shape[1]}")
    print(f"   - ç±»åˆ«æ•°ï¼š{num_classes}")
    print(f"   - ç±»åˆ«åˆ†å¸ƒï¼š")
    for cls, idx in class2id.items():
        cnt = np.sum(all_labels == idx)
        print(f"     * {cls}ï¼š{cnt} ä¸ªæ ·æœ¬ï¼ˆ{cnt/all_labels.shape[0]*100:.1f}%ï¼‰")
    
    return all_feats, all_labels, class2id, unique_classes


# ===================== 3. æ•°æ®é¢„å¤„ç†ï¼ˆåˆ’åˆ†+å½’ä¸€åŒ–+è¿‡é‡‡æ ·ï¼‰ =====================
def preprocess_data(all_feats, all_labels):
    """
    æ•°æ®é¢„å¤„ç†æµç¨‹ï¼š
    1. 8:2åˆ†å±‚åˆ’åˆ†è®­ç»ƒé›†/æµ‹è¯•é›†ï¼ˆä¿è¯ç±»åˆ«åˆ†å¸ƒä¸€è‡´ï¼‰
    2. åŸºäºè®­ç»ƒé›†ç»Ÿè®¡é‡åšå½’ä¸€åŒ–ï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰
    3. è®­ç»ƒé›†è¿‡é‡‡æ ·ï¼ˆSMOTEï¼‰è§£å†³ç±»åˆ«ä¸å¹³è¡¡
    """
    print(f"\nğŸ”§ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
    
    # 1. åˆ†å±‚åˆ’åˆ†è®­ç»ƒé›†/æµ‹è¯•é›†ï¼ˆstratify=all_labels ä¿è¯ç±»åˆ«åˆ†å¸ƒï¼‰
    train_feat, test_feat, train_label, test_label = train_test_split(
        all_feats, all_labels,
        test_size=Config.TEST_SIZE,
        stratify=all_labels,
        random_state=Config.RANDOM_STATE
    )
    print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆï¼š")
    print(f"   - è®­ç»ƒé›†ï¼š{train_feat.shape[0]} æ ·æœ¬")
    print(f"   - æµ‹è¯•é›†ï¼š{test_feat.shape[0]} æ ·æœ¬")
    
    # 2. åŸºäºè®­ç»ƒé›†ç»Ÿè®¡é‡åšå½’ä¸€åŒ–ï¼ˆZ-Scoreï¼šå‡å€¼=0ï¼Œæ ‡å‡†å·®=1ï¼‰
    # æŒ‰ç‰¹å¾ç»´åº¦è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆ768ä¸ªç»´åº¦å„1ä¸ªç»Ÿè®¡é‡ï¼‰
    train_mean = np.mean(train_feat, axis=0)  # [768]
    train_std = np.std(train_feat, axis=0)    # [768]
    # é¿å…æ ‡å‡†å·®ä¸º0å¯¼è‡´é™¤ä»¥0ï¼ˆåŠ æå°å€¼1e-8ï¼‰
    train_std = np.where(train_std < 1e-8, 1e-8, train_std)
    
    # å½’ä¸€åŒ–è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆæµ‹è¯•é›†ç”¨è®­ç»ƒé›†ç»Ÿè®¡é‡ï¼ï¼‰
    train_feat_norm = (train_feat - train_mean) / train_std
    test_feat_norm = (test_feat - train_mean) / train_std
    
    print(f"âœ… å½’ä¸€åŒ–å®Œæˆï¼ˆåŸºäºè®­ç»ƒé›†ç»Ÿè®¡é‡ï¼‰ï¼š")
    print(f"   - è®­ç»ƒé›†å½’ä¸€åŒ–å‰èŒƒå›´ï¼š[{train_feat.min():.4f}, {train_feat.max():.4f}]")
    print(f"   - è®­ç»ƒé›†å½’ä¸€åŒ–åèŒƒå›´ï¼š[{train_feat_norm.min():.4f}, {train_feat_norm.max():.4f}]")
    print(f"   - æµ‹è¯•é›†å½’ä¸€åŒ–åèŒƒå›´ï¼š[{test_feat_norm.min():.4f}, {test_feat_norm.max():.4f}]")
    
    # 3. è®­ç»ƒé›†è¿‡é‡‡æ ·ï¼ˆSMOTEï¼‰ï¼šä»…å¯¹è®­ç»ƒé›†åšï¼Œæµ‹è¯•é›†ä¿æŒåŸå§‹
    print(f"\nâš–ï¸ å¼€å§‹è®­ç»ƒé›†è¿‡é‡‡æ ·ï¼ˆSMOTEï¼‰...")
    print(f"   - è¿‡é‡‡æ ·å‰è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒï¼š")
    for idx in np.unique(train_label):
        cnt = np.sum(train_label == idx)
        print(f"     * ç±»åˆ«{idx}ï¼š{cnt} æ ·æœ¬")
    
    # åˆå§‹åŒ–SMOTEï¼ˆè¿‘é‚»æ•°æ ¹æ®å°æ ·æœ¬ç±»åˆ«è°ƒæ•´ï¼‰
    smote = SMOTE(
        k_neighbors=Config.N_SMOTE_NEIGHBORS,
        random_state=Config.RANDOM_STATE
    )
    # è¿‡é‡‡æ ·ï¼ˆç”Ÿæˆåˆæˆæ ·æœ¬ï¼Œå¹³è¡¡ç±»åˆ«ï¼‰
    train_feat_smote, train_label_smote = smote.fit_resample(train_feat_norm, train_label)
    
    print(f"   - è¿‡é‡‡æ ·åè®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒï¼š")
    for idx in np.unique(train_label_smote):
        cnt = np.sum(train_label_smote == idx)
        print(f"     * ç±»åˆ«{idx}ï¼š{cnt} æ ·æœ¬")
    print(f"   - è¿‡é‡‡æ ·åè®­ç»ƒé›†æ€»æ ·æœ¬æ•°ï¼š{train_feat_smote.shape[0]}")
    
    # è¿”å›é¢„å¤„ç†åçš„æ•°æ®
    return (
        train_feat_smote, train_label_smote,  # è¿‡é‡‡æ ·åçš„è®­ç»ƒé›†
        test_feat_norm, test_label,           # å½’ä¸€åŒ–åçš„æµ‹è¯•é›†
        train_mean, train_std                 # å½’ä¸€åŒ–ç»Ÿè®¡é‡ï¼ˆåç»­æ¨ç†ç”¨ï¼‰
    )


# ===================== 4. æ•°æ®é›†ä¸DataLoaderå®šä¹‰ =====================
class MLPFeatDataset(Dataset):
    """ç‰¹å¾æ•°æ®é›†ï¼šé€‚é…PyTorch DataLoader"""
    def __init__(self, feats, labels):
        self.feats = torch.tensor(feats, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.long)
        assert len(self.feats) == len(self.labels), "ç‰¹å¾ä¸æ ‡ç­¾æ•°é‡ä¸åŒ¹é…ï¼"
    
    def __len__(self):
        return len(self.feats)
    
    def __getitem__(self, idx):
        return {"feat": self.feats[idx], "label": self.labels[idx]}


def create_dataloaders(train_feat, train_label, test_feat, test_label):
    """åˆ›å»ºè®­ç»ƒé›†/æµ‹è¯•é›†DataLoaderï¼ˆè®­ç»ƒé›†æ‰“ä¹±ï¼Œæµ‹è¯•é›†ä¸æ‰“ä¹±ï¼‰"""
    train_dataset = MLPFeatDataset(train_feat, train_label)
    test_dataset = MLPFeatDataset(test_feat, test_label)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=Config.BATCH_SIZE,
        shuffle=True,  # è®­ç»ƒé›†æ‰“ä¹±
        drop_last=True,  # ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´æ‰¹æ¬¡
        pin_memory=True  # åŠ é€ŸGPUæ•°æ®ä¼ è¾“
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=Config.BATCH_SIZE,
        shuffle=False,  # æµ‹è¯•é›†ä¸æ‰“ä¹±
        pin_memory=True
    )
    
    print(f"\nğŸš€ DataLoaderåˆ›å»ºå®Œæˆï¼š")
    print(f"   - è®­ç»ƒé›†æ‰¹æ¬¡ï¼š{len(train_loader)} æ‰¹ï¼ˆæ¯æ‰¹{Config.BATCH_SIZE}æ ·æœ¬ï¼‰")
    print(f"   - æµ‹è¯•é›†æ‰¹æ¬¡ï¼š{len(test_loader)} æ‰¹ï¼ˆæ¯æ‰¹{Config.BATCH_SIZE}æ ·æœ¬ï¼‰")
    
    return train_loader, test_loader


# ===================== 5. MLPåˆ†ç±»æ¨¡å‹å®šä¹‰ï¼ˆé€‚é…768ç»´è¾“å…¥ï¼‰ =====================
class MLPClassifier(nn.Module):
    """è½»é‡çº§MLPåˆ†ç±»å™¨ï¼šè¾“å…¥768ç»´ç‰¹å¾ï¼Œè¾“å‡ºç±»åˆ«æ¦‚ç‡"""
    def __init__(self, input_dim=768, num_classes=2, dropout=Config.DROPOUT):
        super().__init__()
        self.classifier = nn.Sequential(
            # ç¬¬ä¸€å±‚ï¼š768â†’512ï¼ŒReLUæ¿€æ´»+Dropout
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Dropout(dropout),
            # ç¬¬äºŒå±‚ï¼š512â†’256ï¼ŒReLUæ¿€æ´»+Dropout
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(dropout),
            # è¾“å‡ºå±‚ï¼š256â†’ç±»åˆ«æ•°ï¼ˆæ— æ¿€æ´»ï¼ŒCrossEntropyLossè‡ªå¸¦Softmaxï¼‰
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        """x: [batch_size, 768] â†’ logits: [batch_size, num_classes]"""
        return self.classifier(x)


# ===================== 6. è®­ç»ƒä¸è¯„ä¼°å‡½æ•° =====================
def train_one_epoch(model, dataloader, criterion, optimizer, scaler):
    """è®­ç»ƒä¸€è½®ï¼Œè¿”å›è®­ç»ƒé›†æŸå¤±å’ŒæŒ‡æ ‡"""
    model.train()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    
    for batch in tqdm(dataloader, desc="è®­ç»ƒä¸­"):
        # æ•°æ®ç§»è‡³GPU
        feats = batch["feat"].to(Config.DEVICE)
        labels = batch["label"].to(Config.DEVICE)
        
        # æ¢¯åº¦æ¸…é›¶
        optimizer.zero_grad()
        
        # æ··åˆç²¾åº¦è®­ç»ƒï¼ˆèŠ‚çœæ˜¾å­˜+åŠ é€Ÿï¼‰
        with autocast():
            logits = model(feats)
            loss = criterion(logits, labels)
        
        # åå‘ä¼ æ’­ä¸ä¼˜åŒ–
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        # è®°å½•æŸå¤±å’Œé¢„æµ‹ç»“æœ
        total_loss += loss.item() * feats.size(0)
        preds = torch.argmax(logits, dim=1).cpu().numpy()  # å–æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«
        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())
    
    # è®¡ç®—è®­ç»ƒé›†æŒ‡æ ‡
    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    recall = recall_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    f1 = f1_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    
    return avg_loss, accuracy, precision, recall, f1


def evaluate_model(model, dataloader, criterion, class2id, is_test=False):
    """è¯„ä¼°æ¨¡å‹ï¼Œè¿”å›æŸå¤±å’Œå¤šæŒ‡æ ‡ï¼ˆæµ‹è¯•é›†è¾“å‡ºè¯¦ç»†ç±»åˆ«æŒ‡æ ‡ï¼‰"""
    model.eval()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():  # å…³é—­æ¢¯åº¦è®¡ç®—
        for batch in tqdm(dataloader, desc="è¯„ä¼°ä¸­"):
            feats = batch["feat"].to(Config.DEVICE)
            labels = batch["label"].to(Config.DEVICE)
            
            with autocast():
                logits = model(feats)
                loss = criterion(logits, labels)
            
            # è®°å½•ç»“æœ
            total_loss += loss.item() * feats.size(0)
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())
    
    # è®¡ç®—æ€»ä½“æŒ‡æ ‡
    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    recall = recall_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    f1 = f1_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    
    # æµ‹è¯•é›†é¢å¤–è¾“å‡ºï¼šè¯¦ç»†ç±»åˆ«æŒ‡æ ‡+æ··æ·†çŸ©é˜µ
    if is_test:
        print(f"\n========== æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°ç»“æœ ==========")
        print(f"1. æ€»ä½“æŒ‡æ ‡ï¼ˆ{Config.EVAL_METRIC}å¹³å‡ï¼‰ï¼š")
        print(f"   - æŸå¤±ï¼š{avg_loss:.4f}")
        print(f"   - å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰ï¼š{accuracy:.4f}")
        print(f"   - ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰ï¼š{precision:.4f}")
        print(f"   - å¬å›ç‡ï¼ˆRecallï¼‰ï¼š{recall:.4f}")
        print(f"   - F1åˆ†æ•°ï¼š{f1:.4f}\n")
        
        # è¾“å‡ºæ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
        print(f"2. å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡ï¼š")
        id2class = {idx: cls for cls, idx in class2id.items()}  # IDâ†’ç±»åˆ«åæ˜ å°„
        class_precision = precision_score(all_labels, all_preds, average=None, zero_division=0)
        class_recall = recall_score(all_labels, all_preds, average=None, zero_division=0)
        class_f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)
        
        for idx in np.unique(all_labels):
            cls_name = id2class[idx]
            print(f"   - {cls_name}ï¼ˆID:{idx}ï¼‰ï¼š")
            print(f"     ç²¾ç¡®ç‡ï¼š{class_precision[idx]:.4f} | å¬å›ç‡ï¼š{class_recall[idx]:.4f} | F1ï¼š{class_f1[idx]:.4f}")
        
        # ç»˜åˆ¶æ··æ·†çŸ©é˜µï¼ˆå¯é€‰ï¼‰
        if Config.PLOT_CONFUSION_MATRIX:
            cm = confusion_matrix(all_labels, all_preds)
            plt.figure(figsize=(12, 10))  # ç±»åˆ«å¤šåˆ™è°ƒå¤§å°ºå¯¸
            sns.heatmap(
                cm, 
                annot=True, 
                fmt="d", 
                cmap="Blues",
                xticklabels=[id2class[idx] for idx in range(len(class2id))],
                yticklabels=[id2class[idx] for idx in range(len(class2id))],
                annot_kws={"fontsize": 8}  # ç±»åˆ«å¤šåˆ™è°ƒå°å­—ä½“
            )
            plt.xlabel("é¢„æµ‹ç±»åˆ«", fontsize=12)
            plt.ylabel("çœŸå®ç±»åˆ«", fontsize=12)
            plt.title("æµ‹è¯•é›†æ··æ·†çŸ©é˜µ", fontsize=14)
            plt.xticks(rotation=45, ha="right")  # ç±»åˆ«åè¿‡é•¿åˆ™æ—‹è½¬
            plt.tight_layout()
            cm_save_path = os.path.join(Config.PLOT_DIR, "test_confusion_matrix.png")
            plt.savefig(cm_save_path, dpi=300, bbox_inches="tight")
            plt.close()
            print(f"\n3. æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³ï¼š{cm_save_path}")
    
    return avg_loss, accuracy, precision, recall, f1


# ===================== 7. ä¸»å‡½æ•°ï¼ˆä¸²è”å…¨æµç¨‹ï¼‰ =====================
def main():
    # Step 1ï¼šåŠ è½½ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆæŒ‰æ–‡ä»¶åè§£æç±»åˆ«ï¼‰
    all_feats, all_labels, class2id, unique_classes = load_all_features_and_labels(Config.FEAT_ROOT)
    num_classes = len(class2id)
    
    # Step 2ï¼šæ•°æ®é¢„å¤„ç†ï¼ˆåˆ’åˆ†+å½’ä¸€åŒ–+è¿‡é‡‡æ ·ï¼‰
    (train_feat_smote, train_label_smote, 
     test_feat_norm, test_label, 
     train_mean, train_std) = preprocess_data(all_feats, all_labels)
    
    # Step 3ï¼šåˆ›å»ºDataLoader
    train_loader, test_loader = create_dataloaders(
        train_feat_smote, train_label_smote,
        test_feat_norm, test_label
    )
    
    # Step 4ï¼šåˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
    model = MLPClassifier(
        input_dim=768,
        num_classes=num_classes,
        dropout=Config.DROPOUT
    ).to(Config.DEVICE)
    print(f"\nğŸ“Œ MLPæ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ˆè®¾å¤‡ï¼š{Config.DEVICE}ï¼‰")
    print(f"   - è¾“å…¥ç»´åº¦ï¼š768")
    print(f"   - è¾“å‡ºç»´åº¦ï¼š{num_classes}ï¼ˆç±»åˆ«æ•°ï¼‰")
    
    # æŸå¤±å‡½æ•°ï¼ˆäº¤å‰ç†µï¼šé€‚é…å¤šåˆ†ç±»ï¼‰
    criterion = nn.CrossEntropyLoss()
    # ä¼˜åŒ–å™¨ï¼ˆAdamWï¼šå¸¦æƒé‡è¡°å‡ï¼Œé˜²è¿‡æ‹Ÿåˆï¼‰
    optimizer = optim.AdamW(
        model.parameters(),
        lr=Config.LEARNING_RATE,
        weight_decay=Config.WEIGHT_DECAY
    )
    # æ··åˆç²¾åº¦è®­ç»ƒå™¨
    scaler = GradScaler()
    
    # Step 5ï¼šè®­ç»ƒå¾ªç¯ï¼ˆè·Ÿè¸ªéªŒè¯é›†F1ï¼Œä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼‰
    best_test_f1 = 0.0  # ç”¨æµ‹è¯•é›†F1ä½œä¸ºæœ€ä¼˜æ¨¡å‹æŒ‡æ ‡ï¼ˆå®é™…é¡¹ç›®å»ºè®®ç”¨éªŒè¯é›†ï¼Œæ­¤å¤„ç®€åŒ–ï¼‰
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒï¼ˆå…±{Config.EPOCHS}è½®ï¼‰")
    
    for epoch in range(1, Config.EPOCHS + 1):
        print(f"\n===== Epoch {epoch}/{Config.EPOCHS} =====")
        
        # è®­ç»ƒä¸€è½®
        train_loss, train_acc, train_prec, train_rec, train_f1 = train_one_epoch(
            model, train_loader, criterion, optimizer, scaler
        )
        
        # è¯„ä¼°æµ‹è¯•é›†ï¼ˆç®€åŒ–ï¼šå®é™…é¡¹ç›®å»ºè®®æ‹†åˆ†éªŒè¯é›†ï¼Œæ­¤å¤„ç”¨æµ‹è¯•é›†æ›¿ä»£ï¼‰
        test_loss, test_acc, test_prec, test_rec, test_f1 = evaluate_model(
            model, test_loader, criterion, class2id, is_test=False  # éæœ€ç»ˆæµ‹è¯•ï¼Œä¸è¾“å‡ºè¯¦ç»†æŒ‡æ ‡
        )
        
        # æ‰“å°æ—¥å¿—
        print(f"ğŸ“Š è®­ç»ƒé›†ï¼š")
        print(f"   æŸå¤±ï¼š{train_loss:.4f} | å‡†ç¡®ç‡ï¼š{train_acc:.4f} | F1ï¼š{train_f1:.4f}")
        print(f"ğŸ“Š æµ‹è¯•é›†ï¼š")
        print(f"   æŸå¤±ï¼š{test_loss:.4f} | å‡†ç¡®ç‡ï¼š{test_acc:.4f} | F1ï¼š{test_f1:.4f}")
        
        # ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆåŸºäºæµ‹è¯•é›†F1ï¼‰
        if test_f1 > best_test_f1:
            best_test_f1 = test_f1
            save_path = os.path.join(Config.SAVE_MODEL_DIR, "best_model.pth")
            torch.save({
                "model_state_dict": model.state_dict(),
                "class2id": class2id,
                "train_mean": train_mean,
                "train_std": train_std
            }, save_path)
            print(f"âœ… ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆæµ‹è¯•é›†F1ï¼š{best_test_f1:.4f}ï¼‰è‡³ {save_path}")
    
    # Step 6ï¼šåŠ è½½æœ€ä¼˜æ¨¡å‹ï¼Œè¿›è¡Œæœ€ç»ˆæµ‹è¯•ï¼ˆè¾“å‡ºè¯¦ç»†æŒ‡æ ‡ï¼‰
    print(f"\n========== åŠ è½½æœ€ä¼˜æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯• ==========")
    checkpoint = torch.load(os.path.join(Config.SAVE_MODEL_DIR, "best_model.pth"))
    best_model = MLPClassifier(input_dim=768, num_classes=num_classes).to(Config.DEVICE)
    best_model.load_state_dict(checkpoint["model_state_dict"])
    
    # æœ€ç»ˆæµ‹è¯•ï¼ˆè¾“å‡ºè¯¦ç»†ç±»åˆ«æŒ‡æ ‡å’Œæ··æ·†çŸ©é˜µï¼‰
    evaluate_model(best_model, test_loader, criterion, class2id, is_test=True)
    
    print(f"\nğŸ‰ MLPå¤šç±»åˆ«åˆ†ç±»ä»»åŠ¡å®Œæˆï¼")
    print(f"   - æœ€ä¼˜æ¨¡å‹è·¯å¾„ï¼š{Config.SAVE_MODEL_DIR}/best_model.pth")
    print(f"   - æ··æ·†çŸ©é˜µè·¯å¾„ï¼ˆè‹¥å¼€å¯ï¼‰ï¼š{Config.PLOT_DIR}/test_confusion_matrix.png")


if __name__ == "__main__":
    main()