import os
import numpy as np
import torch
import torch.nn as nn
import pandas as pd
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import GradScaler, autocast
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
)
from imblearn.over_sampling import SMOTE  # è¿‡é‡‡æ ·åº“
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
from torch.utils.tensorboard import SummaryWriter  # æ–°å¢ï¼šTensorBoardå¯¼å…¥

warnings.filterwarnings("ignore")


# ===================== 1. é…ç½®å‚æ•°ï¼ˆå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰ =====================
class Config:
    # æ ¸å¿ƒè·¯å¾„
    FEAT_ROOT = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/768_npy_charactristic"  # ç‰¹å¾æ–‡ä»¶ç›®å½•
    SAVE_MODEL_DIR = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/benchmark/wav2vec/768_classification_mlp/mlp_rid_healthy"  # æœ€ä¼˜æ¨¡å‹ä¿å­˜ç›®å½•
    PLOT_DIR = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/benchmark/wav2vec/768_classification_mlp/mlp_cut_in_200_batch_128_epoch_160_300_2e4"  # å›¾è¡¨/æ—¥å¿—ä¿å­˜ç›®å½•
    SAVE_MODEL_DIR = PLOT_DIR
    TB_LOG_DIR = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/benchmark/wav2vec/768_classification_mlp/tb_logs"  # TensorBoardæ—¥å¿—ç›®å½•
    
    # è®­ç»ƒå‚æ•°
    BATCH_SIZE = 128        # æ‰¹æ¬¡å¤§å°
    EPOCHS = 160            # è®­ç»ƒè½®æ¬¡
    LEARNING_RATE = 2e-4   # å­¦ä¹ ç‡
    WEIGHT_DECAY = 1e-5    # æƒé‡è¡°å‡ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰
    DROPOUT = 0.3          # Dropoutæ¯”ä¾‹
    DEVICE = torch.device("cuda:5" if torch.cuda.is_available() else "cpu")  # GPUè®¾å¤‡
    
    # æ•°æ®å¤„ç†å‚æ•°
    TEST_SIZE = 0.2        # æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆ8:2åˆ’åˆ†ï¼‰
    RANDOM_STATE = 42      # éšæœºç§å­ï¼ˆä¿è¯ç»“æœå¯å¤ç°ï¼‰
    N_SMOTE_NEIGHBORS = 5  # SMOTEè¿‡é‡‡æ ·çš„è¿‘é‚»æ•°
    OVERSAMPLING_STRATEGY = "smote"  # è¿‡é‡‡æ ·ç­–ç•¥
    MAX_SAMPLES_PER_CLASS = 300      # æ¯ç±»è¿‡é‡‡æ ·åçš„æœ€å¤§æ ·æœ¬æ•°
    
    # è¯„ä¼°å‚æ•°
    EVAL_METRIC = "weighted"  # æŒ‡æ ‡è®¡ç®—æ–¹å¼ï¼ˆweighted=è€ƒè™‘ç±»åˆ«ä¸å¹³è¡¡ï¼‰
    PLOT_CONFUSION_MATRIX = True  # æ˜¯å¦ç»˜åˆ¶æ··æ·†çŸ©é˜µ


# åˆ›å»ºå¿…è¦ç›®å½•
os.makedirs(Config.SAVE_MODEL_DIR, exist_ok=True)
os.makedirs(Config.PLOT_DIR, exist_ok=True)
os.makedirs(Config.TB_LOG_DIR, exist_ok=True)


# ===================== 2. æ•°æ®åŠ è½½ä¸æ ‡ç­¾å¤„ç†ï¼ˆæ ¸å¿ƒï¼šæŒ‰æ–‡ä»¶åè§£æç±»åˆ«ï¼‰ =====================
def parse_class_from_filename(filename):
    """ä»ç‰¹å¾æ–‡ä»¶åè§£æç±»åˆ«ï¼ˆæ ¼å¼ï¼šæ•°æ®é›†å__and__ç±»åˆ«å.npyï¼‰"""
    prefix = filename.replace(".npy", "")
    if "__and__" not in prefix:
        raise ValueError(f"æ–‡ä»¶åæ ¼å¼é”™è¯¯ï¼ˆéœ€ä¸ºã€Œæ•°æ®é›†å__and__ç±»åˆ«å.npyã€ï¼‰ï¼š{filename}")
    dataset_name, class_name = prefix.split("__and__", 1)
    return dataset_name, class_name


def load_all_features_and_labels(feat_root):
    """åŠ è½½æ‰€æœ‰ç‰¹å¾æ–‡ä»¶ï¼Œç”Ÿæˆæ ‡ç­¾æ˜ å°„ï¼ˆæ¯ä¸ªæ–‡ä»¶å¯¹åº”ä¸€ä¸ªç±»åˆ«ï¼‰"""
    print(f"ğŸ“Š ä» {feat_root} åŠ è½½ç‰¹å¾æ–‡ä»¶...")
    
    # ç­›é€‰ç‰¹å¾æ–‡ä»¶ï¼ˆå¿½ç•¥_labels.npyå’Œå«'Healthy/healthy'çš„æ–‡ä»¶ï¼‰
    feat_files = []
    for f in os.listdir(feat_root):
        if f.endswith(".npy") and "_labels.npy" not in f and "Healthy" not in f and "healthy" not in f:
            feat_files.append(f)
    
    if len(feat_files) == 0:
        raise FileNotFoundError(f"âŒ åœ¨ {feat_root} æœªæ‰¾åˆ°æœ‰æ•ˆç‰¹å¾æ–‡ä»¶")
    
    # ç”Ÿæˆç±»åˆ«æ˜ å°„
    all_classes = []
    for f in feat_files:
        dataset_name, class_name = parse_class_from_filename(f)
        all_classes.append(f"{dataset_name}_and_{class_name}")
    unique_classes = sorted(list(set(all_classes)))
    class2id = {cls: idx for idx, cls in enumerate(unique_classes)}
    num_classes = len(unique_classes)
    print(f"âœ… ç”Ÿæˆç±»åˆ«æ˜ å°„ï¼šå…± {num_classes} ä¸ªç±»åˆ«")
    for cls, idx in class2id.items():
        print(f"   - ç±»åˆ«ï¼š{cls} â†’ IDï¼š{idx}")
    
    # åŠ è½½ç‰¹å¾å’Œæ ‡ç­¾
    all_feats = []
    all_labels = []
    for f in tqdm(feat_files, desc="åŠ è½½ç‰¹å¾æ–‡ä»¶"):
        file_path = os.path.join(feat_root, f)
        dataset_name, class_name = parse_class_from_filename(f)
        class_name = f"{dataset_name}_and_{class_name}"
        current_label = class2id[class_name]
        
        feats = np.load(file_path).astype(np.float32)
        if feats.shape[1] != 768:
            raise ValueError(f"âŒ {f} ç‰¹å¾ç»´åº¦é”™è¯¯ï¼ˆéœ€ä¸º768ç»´ï¼‰ï¼Œå®é™…ç»´åº¦ï¼š{feats.shape[1]}")
        
        all_feats.append(feats)
        all_labels.extend([current_label] * feats.shape[0])
    
    # æ‹¼æ¥ä¸ºå…¨å±€æ•°ç»„
    all_feats = np.concatenate(all_feats, axis=0)
    all_labels = np.array(all_labels, dtype=np.int64)
    
    print(f"\nğŸ“ˆ æ•°æ®åŠ è½½å®Œæˆï¼š")
    print(f"   - æ€»æ ·æœ¬æ•°ï¼š{all_feats.shape[0]}")
    print(f"   - ç‰¹å¾ç»´åº¦ï¼š{all_feats.shape[1]}")
    print(f"   - ç±»åˆ«æ•°ï¼š{num_classes}")
    print(f"   - ç±»åˆ«åˆ†å¸ƒï¼š")
    for cls, idx in class2id.items():
        cnt = np.sum(all_labels == idx)
        print(f"     * {cls}ï¼š{cnt} ä¸ªæ ·æœ¬ï¼ˆ{cnt/all_labels.shape[0]*100:.1f}%ï¼‰")
    
    return all_feats, all_labels, class2id, unique_classes


# ===================== 3. æ•°æ®é¢„å¤„ç†ï¼ˆåˆ’åˆ†+å½’ä¸€åŒ–+è¿‡é‡‡æ ·ï¼‰ =====================
def preprocess_data(all_feats, all_labels):
    """ç»å…¸æ•°æ®é¢„å¤„ç†ï¼šåˆ†å±‚åˆ’åˆ†+å½’ä¸€åŒ–+å¤šæ•°ç±»ä¸‹é‡‡æ ·+å°‘æ•°ç±»SMOTEè¿‡é‡‡æ ·"""
    print(f"\nğŸ”§ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
    
    # 1. åˆ†å±‚åˆ’åˆ†è®­ç»ƒé›†/æµ‹è¯•é›†
    train_feat, test_feat, train_label, test_label = train_test_split(
        all_feats, all_labels,
        test_size=Config.TEST_SIZE,
        stratify=all_labels,
        random_state=Config.RANDOM_STATE
    )
    print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆï¼š")
    print(f"   - è®­ç»ƒé›†ï¼š{train_feat.shape[0]} æ ·æœ¬")
    print(f"   - æµ‹è¯•é›†ï¼š{test_feat.shape[0]} æ ·æœ¬")
    
    # 2. å½’ä¸€åŒ–ï¼ˆZ-Scoreï¼‰
    train_mean = np.mean(train_feat, axis=0)
    train_std = np.std(train_feat, axis=0)
    train_std = np.where(train_std < 1e-8, 1e-8, train_std)  # é¿å…é™¤é›¶
    
    train_feat_norm = (train_feat - train_mean) / train_std
    test_feat_norm = (test_feat - train_mean) / train_std
    
    print(f"âœ… å½’ä¸€åŒ–å®Œæˆï¼š")
    print(f"   - è®­ç»ƒé›†å½’ä¸€åŒ–åèŒƒå›´ï¼š[{train_feat_norm.min():.4f}, {train_feat_norm.max():.4f}]")
    print(f"   - æµ‹è¯•é›†å½’ä¸€åŒ–åèŒƒå›´ï¼š[{test_feat_norm.min():.4f}, {test_feat_norm.max():.4f}]")
    
    # 3. ç»å…¸è¿‡é‡‡æ ·ç­–ç•¥ï¼ˆå¤šæ•°ç±»ä¸‹é‡‡æ ·+å°‘æ•°ç±»SMOTEï¼‰
    print(f"\nâš–ï¸ å¼€å§‹è¿‡é‡‡æ ·ï¼ˆæ¯ç±»é™åˆ¶æœ€å¤§{Config.MAX_SAMPLES_PER_CLASS}æ ·æœ¬ï¼‰...")
    print(f"   - è¿‡é‡‡æ ·å‰åˆ†å¸ƒï¼š")
    unique_labels = np.unique(train_label)
    for label in unique_labels:
        print(f"     * ç±»åˆ«{label}ï¼š{np.sum(train_label == label)} æ ·æœ¬")
    
    # æ­¥éª¤1ï¼šå¯¹æ‰€æœ‰ç±»åˆ«å…ˆæˆªæ–­åˆ°æœ€å¤§æ ·æœ¬æ•°ï¼ˆå¤šæ•°ç±»ä¸‹é‡‡æ ·ï¼‰
    from collections import defaultdict
    np.random.seed(Config.RANDOM_STATE)
    class_data = defaultdict(list)
    
    for feat, label in zip(train_feat_norm, train_label):
        class_data[label].append(feat)
    
    truncated_data = []
    truncated_labels = []
    for label in class_data:
        samples = np.array(class_data[label])
        n_samples = len(samples)
        
        if n_samples > Config.MAX_SAMPLES_PER_CLASS:
            selected_idx = np.random.choice(n_samples, Config.MAX_SAMPLES_PER_CLASS, replace=False)
            truncated_samples = samples[selected_idx]
        else:
            truncated_samples = samples
        
        truncated_data.append(truncated_samples)
        truncated_labels.append(np.full(len(truncated_samples), label))
    
    truncated_data = np.concatenate(truncated_data, axis=0)
    truncated_labels = np.concatenate(truncated_labels, axis=0)
    
    # æ­¥éª¤2ï¼šå¯¹å°‘æ•°ç±»ä½¿ç”¨SMOTEè¿‡é‡‡æ ·ï¼ˆè¡¥åˆ°æœ€å¤§æ ·æœ¬æ•°ï¼‰
    from imblearn.over_sampling import SMOTE
    sampling_strategy = {label: Config.MAX_SAMPLES_PER_CLASS for label in unique_labels}
    
    if len(unique_labels) == 1:
        print(f"âš ï¸ ä»…1ä¸ªç±»åˆ«ï¼Œæ— éœ€SMOTEï¼Œä½¿ç”¨æˆªæ–­åæ•°æ®")
        train_feat_smote = truncated_data
        train_label_smote = truncated_labels
    else:
        smote = SMOTE(
            sampling_strategy=sampling_strategy,
            k_neighbors=min(5, min(np.bincount(truncated_labels)) - 1),
            random_state=Config.RANDOM_STATE
        )
        train_feat_smote, train_label_smote = smote.fit_resample(truncated_data, truncated_labels)
    
    # è¾“å‡ºè¿‡é‡‡æ ·ç»“æœ
    print(f"   - è¿‡é‡‡æ ·ååˆ†å¸ƒï¼š")
    for label in np.unique(train_label_smote):
        print(f"     * ç±»åˆ«{label}ï¼š{np.sum(train_label_smote == label)} æ ·æœ¬")
    print(f"   - æ€»æ ·æœ¬æ•°ï¼š{train_feat_smote.shape[0]}")
    
    return (
        train_feat_smote, train_label_smote,
        test_feat_norm, test_label,
        train_mean, train_std
    )


# ===================== 4. æ•°æ®é›†ä¸DataLoaderå®šä¹‰ =====================
class MLPFeatDataset(Dataset):
    """ç‰¹å¾æ•°æ®é›†ï¼šé€‚é…PyTorch DataLoader"""
    def __init__(self, feats, labels):
        self.feats = torch.tensor(feats, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.long)
        assert len(self.feats) == len(self.labels), "ç‰¹å¾ä¸æ ‡ç­¾æ•°é‡ä¸åŒ¹é…ï¼"
    
    def __len__(self):
        return len(self.feats)
    
    def __getitem__(self, idx):
        return {"feat": self.feats[idx], "label": self.labels[idx]}


def create_dataloaders(train_feat, train_label, test_feat, test_label):
    """åˆ›å»ºè®­ç»ƒé›†/æµ‹è¯•é›†DataLoader"""
    train_dataset = MLPFeatDataset(train_feat, train_label)
    test_dataset = MLPFeatDataset(test_feat, test_label)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=Config.BATCH_SIZE,
        shuffle=True,
        drop_last=True,
        pin_memory=True
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=Config.BATCH_SIZE,
        shuffle=False,
        pin_memory=True
    )
    
    print(f"\nğŸš€ DataLoaderåˆ›å»ºå®Œæˆï¼š")
    print(f"   - è®­ç»ƒé›†æ‰¹æ¬¡ï¼š{len(train_loader)} æ‰¹ï¼ˆæ¯æ‰¹{Config.BATCH_SIZE}æ ·æœ¬ï¼‰")
    print(f"   - æµ‹è¯•é›†æ‰¹æ¬¡ï¼š{len(test_loader)} æ‰¹ï¼ˆæ¯æ‰¹{Config.BATCH_SIZE}æ ·æœ¬ï¼‰")
    
    return train_loader, test_loader


# ===================== 5. MLPåˆ†ç±»æ¨¡å‹å®šä¹‰ï¼ˆé€‚é…768ç»´è¾“å…¥ï¼‰ =====================
class MLPClassifier(nn.Module):
    """è½»é‡çº§MLPåˆ†ç±»å™¨ï¼šè¾“å…¥768ç»´ç‰¹å¾ï¼Œè¾“å‡ºç±»åˆ«æ¦‚ç‡"""
    def __init__(self, input_dim=768, num_classes=2, dropout=Config.DROPOUT):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        return self.classifier(x)


# ===================== 6. è®­ç»ƒä¸è¯„ä¼°å‡½æ•° =====================
def train_one_epoch(model, dataloader, criterion, optimizer, scaler):
    """è®­ç»ƒä¸€è½®ï¼Œè¿”å›è®­ç»ƒé›†å…¨æŒ‡æ ‡"""
    model.train()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    
    for batch in tqdm(dataloader, desc="è®­ç»ƒä¸­"):
        feats = batch["feat"].to(Config.DEVICE)
        labels = batch["label"].to(Config.DEVICE)
        
        optimizer.zero_grad()
        with autocast():
            logits = model(feats)
            loss = criterion(logits, labels)
        
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        total_loss += loss.item() * feats.size(0)
        preds = torch.argmax(logits, dim=1).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())
    
    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    recall = recall_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    f1 = f1_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    
    return avg_loss, accuracy, precision, recall, f1


def evaluate_model(model, dataloader, criterion, class2id, is_test=False):
    """è¯„ä¼°æ¨¡å‹ï¼Œè¿”å›å…¨æŒ‡æ ‡ï¼Œæµ‹è¯•é›†è¾“å‡ºè¯¦ç»†ä¿¡æ¯"""
    model.eval()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="è¯„ä¼°ä¸­"):
            feats = batch["feat"].to(Config.DEVICE)
            labels = batch["label"].to(Config.DEVICE)
            
            with autocast():
                logits = model(feats)
                loss = criterion(logits, labels)
            
            total_loss += loss.item() * feats.size(0)
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())
    
    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    recall = recall_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    f1 = f1_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    
    if is_test:
        print(f"\n========== æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°ç»“æœ ==========")
        print(f"1. æ€»ä½“æŒ‡æ ‡ï¼ˆ{Config.EVAL_METRIC}å¹³å‡ï¼‰ï¼š")
        print(f"   - æŸå¤±ï¼š{avg_loss:.4f}")
        print(f"   - å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰ï¼š{accuracy:.4f}")
        print(f"   - ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰ï¼š{precision:.4f}")
        print(f"   - å¬å›ç‡ï¼ˆRecallï¼‰ï¼š{recall:.4f}")
        print(f"   - F1åˆ†æ•°ï¼š{f1:.4f}\n")
        
        id2class = {idx: cls for cls, idx in class2id.items()}
        class_prec = precision_score(all_labels, all_preds, average=None, zero_division=0)
        class_rec = recall_score(all_labels, all_preds, average=None, zero_division=0)
        class_f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)
        
        print(f"2. å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡ï¼š")
        for idx in np.unique(all_labels):
            cls_name = id2class[idx]
            print(f"   - {cls_name}ï¼ˆID:{idx}ï¼‰ï¼š")
            print(f"     ç²¾ç¡®ç‡ï¼š{class_prec[idx]:.4f} | å¬å›ç‡ï¼š{class_rec[idx]:.4f} | F1ï¼š{class_f1[idx]:.4f}")
        
        if Config.PLOT_CONFUSION_MATRIX:
            cm = confusion_matrix(all_labels, all_preds)
            plt.figure(figsize=(12 + len(class2id)//5, 10 + len(class2id)//5))
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
            
            sns.heatmap(
                cm, 
                annot=True, 
                fmt="d", 
                cmap="Blues",
                xticklabels=[id2class[idx] for idx in range(len(class2id))],
                yticklabels=[id2class[idx] for idx in range(len(class2id))],
                annot_kws={"fontsize": 6 if len(class2id) > 10 else 8}
            )
            plt.xlabel("Predicted Class", fontsize=12)
            plt.ylabel("True Class", fontsize=12)
            plt.title("Test Set Confusion Matrix", fontsize=14, pad=20)
            plt.xticks(rotation=45 if len(class2id) <= 15 else 90, ha="right")
            plt.tight_layout()
            cm_save_path = os.path.join(Config.PLOT_DIR, "test_confusion_matrix.png")
            plt.savefig(cm_save_path, dpi=300, bbox_inches="tight")
            plt.close()
            print(f"\n3. æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³ï¼š{cm_save_path}")
    
    return avg_loss, accuracy, precision, recall, f1


# ===================== æ–°å¢ï¼šæ—¥å¿—ä¿å­˜ï¼ˆä¿ç•™CSVç”¨äºç¦»çº¿åˆ†æï¼‰ =====================
def save_training_logs(logs, save_dir):
    log_df = pd.DataFrame(logs, columns=[
        "epoch", 
        "train_loss", "train_accuracy", "train_precision", "train_recall", "train_f1",
        "test_loss", "test_accuracy", "test_precision", "test_recall", "test_f1"
    ])
    csv_path = os.path.join(save_dir, "mlp_training_logs.csv")
    log_df.to_csv(csv_path, index=False, encoding="utf-8")
    print(f"\nğŸ“„ è®­ç»ƒæ—¥å¿—å·²ä¿å­˜è‡³ï¼š{csv_path}")
    return csv_path


# ===================== 7. ä¸»å‡½æ•°ï¼ˆé›†æˆTensorBoardï¼‰ =====================
def main():
    # Step 1ï¼šåŠ è½½ç‰¹å¾å’Œæ ‡ç­¾
    all_feats, all_labels, class2id, unique_classes = load_all_features_and_labels(Config.FEAT_ROOT)
    num_classes = len(class2id)
    
    # Step 2ï¼šæ•°æ®é¢„å¤„ç†
    (train_feat_smote, train_label_smote, 
     test_feat_norm, test_label, 
     train_mean, train_std) = preprocess_data(all_feats, all_labels)
    
    # Step 3ï¼šåˆ›å»ºDataLoader
    train_loader, test_loader = create_dataloaders(
        train_feat_smote, train_label_smote,
        test_feat_norm, test_label
    )
    
    # Step 4ï¼šåˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
    model = MLPClassifier(
        input_dim=768,
        num_classes=num_classes,
        dropout=Config.DROPOUT
    ).to(Config.DEVICE)
    print(f"\nğŸ“Œ MLPæ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ˆè®¾å¤‡ï¼š{Config.DEVICE}ï¼‰")
    print(f"   - è¾“å…¥ç»´åº¦ï¼š768")
    print(f"   - è¾“å‡ºç»´åº¦ï¼š{num_classes}ï¼ˆç±»åˆ«æ•°ï¼‰")
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(
        model.parameters(),
        lr=Config.LEARNING_RATE,
        weight_decay=Config.WEIGHT_DECAY
    )
    scaler = GradScaler()
    
    # åˆå§‹åŒ–TensorBoard SummaryWriter
    tb_writer = SummaryWriter(log_dir=Config.TB_LOG_DIR)
    train_logs = []  # ä¿ç•™CSVæ—¥å¿—
    best_test_f1 = 0.0
    
    # Step 5ï¼šè®­ç»ƒå¾ªç¯ï¼ˆé›†æˆTensorBoardè®°å½•ï¼‰
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒï¼ˆå…±{Config.EPOCHS}è½®ï¼‰")
    for epoch in range(1, Config.EPOCHS + 1):
        print(f"\n===== Epoch {epoch}/{Config.EPOCHS} =====")
        
        # è®­ç»ƒ+è¯„ä¼°ï¼Œè·å–å…¨æŒ‡æ ‡
        train_loss, train_acc, train_prec, train_rec, train_f1 = train_one_epoch(
            model, train_loader, criterion, optimizer, scaler
        )
        test_loss, test_acc, test_prec, test_rec, test_f1 = evaluate_model(
            model, test_loader, criterion, class2id, is_test=False
        )
        
        # 1. æ‰“å°æ¯è½®æ—¥å¿—
        print(f"ğŸ“Š è®­ç»ƒé›†ï¼š")
        print(f"   æŸå¤±ï¼š{train_loss:.4f} | å‡†ç¡®ç‡ï¼š{train_acc:.4f} | ç²¾ç¡®ç‡ï¼š{train_prec:.4f} | å¬å›ç‡ï¼š{train_rec:.4f} | F1ï¼š{train_f1:.4f}")
        print(f"ğŸ“Š æµ‹è¯•é›†ï¼š")
        print(f"   æŸå¤±ï¼š{test_loss:.4f} | å‡†ç¡®ç‡ï¼š{test_acc:.4f} | ç²¾ç¡®ç‡ï¼š{test_prec:.4f} | å¬å›ç‡ï¼š{test_rec:.4f} | F1ï¼š{test_f1:.4f}")
        
        # 2. è®°å½•åˆ°TensorBoard
        tb_writer.add_scalar("Loss/train", train_loss, epoch)
        tb_writer.add_scalar("Accuracy/train", train_acc, epoch)
        tb_writer.add_scalar("Precision/train", train_prec, epoch)
        tb_writer.add_scalar("Recall/train", train_rec, epoch)
        tb_writer.add_scalar("F1/train", train_f1, epoch)
        
        tb_writer.add_scalar("Loss/test", test_loss, epoch)
        tb_writer.add_scalar("Accuracy/test", test_acc, epoch)
        tb_writer.add_scalar("Precision/test", test_prec, epoch)
        tb_writer.add_scalar("Recall/test", test_rec, epoch)
        tb_writer.add_scalar("F1/test", test_f1, epoch)
        
        # 3. ä¿ç•™CSVæ—¥å¿—è®°å½•
        train_logs.append({
            "epoch": epoch,
            "train_loss": train_loss, "train_accuracy": train_acc, 
            "train_precision": train_prec, "train_recall": train_rec, "train_f1": train_f1,
            "test_loss": test_loss, "test_accuracy": test_acc,
            "test_precision": test_prec, "test_recall": test_rec, "test_f1": test_f1
        })
        
        # 4. ä¿å­˜æœ€ä¼˜æ¨¡å‹
        if test_f1 > best_test_f1:
            best_test_f1 = test_f1
            save_path = os.path.join(Config.SAVE_MODEL_DIR, "best_model.pth")
            torch.save({
                "model_state_dict": model.state_dict(),
                "class2id": class2id,
                "train_mean": train_mean,
                "train_std": train_std
            }, save_path)
            print(f"âœ… ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆæµ‹è¯•é›†F1ï¼š{best_test_f1:.4f}ï¼‰è‡³ {save_path}")
    
    # å…³é—­TensorBoardå†™å…¥å™¨
    tb_writer.close()
    
    # ä¿å­˜è®­ç»ƒæ—¥å¿—ä¸ºCSVï¼ˆç¦»çº¿åˆ†æç”¨ï¼‰
    save_training_logs(train_logs, Config.PLOT_DIR)
    
    # Step 6ï¼šåŠ è½½æœ€ä¼˜æ¨¡å‹åšæœ€ç»ˆæµ‹è¯•
    print(f"\n========== åŠ è½½æœ€ä¼˜æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯• ==========")
    checkpoint = torch.load(os.path.join(Config.SAVE_MODEL_DIR, "best_model.pth"))
    best_model = MLPClassifier(input_dim=768, num_classes=num_classes).to(Config.DEVICE)
    best_model.load_state_dict(checkpoint["model_state_dict"])
    evaluate_model(best_model, test_loader, criterion, class2id, is_test=True)
    
    print(f"\nğŸ‰ MLPå¤šç±»åˆ«åˆ†ç±»ä»»åŠ¡å®Œæˆï¼")
    print(f"   - æœ€ä¼˜æ¨¡å‹è·¯å¾„ï¼š{Config.SAVE_MODEL_DIR}/best_model.pth")
    print(f"   - è®­ç»ƒæ—¥å¿—è·¯å¾„ï¼š{os.path.join(Config.PLOT_DIR, 'mlp_training_logs.csv')}")
    print(f"   - TensorBoardæ—¥å¿—ç›®å½•ï¼š{Config.TB_LOG_DIR}ï¼ˆå¯é€šè¿‡å‘½ä»¤ `tensorboard --logdir={Config.TB_LOG_DIR}` å¯åŠ¨æŸ¥çœ‹ï¼‰")
    if Config.PLOT_CONFUSION_MATRIX:
        print(f"   - æ··æ·†çŸ©é˜µè·¯å¾„ï¼š{os.path.join(Config.PLOT_DIR, 'test_confusion_matrix.png')}")


if __name__ == "__main__":
    main()