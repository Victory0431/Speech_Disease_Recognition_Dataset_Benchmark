import os
import warnings
import numpy as np
import pandas as pd
import librosa
import soundfile as sf
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import GradScaler, autocast
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from transformers import Wav2Vec2Processor, Wav2Vec2Model
from tqdm import tqdm
import concurrent.futures


# ===================== 1. é…ç½®å‚æ•°ï¼ˆæŒ‰éœ€è°ƒæ•´ï¼‰ =====================
class Config:
    # æ•°æ®è·¯å¾„
    DATA_ROOT = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/fresh_datasets/EDAC"
    MODEL_PATH = "/mnt/data/test1/repo/wav2vec_2/model"  # æœ¬åœ°Wav2Vec2æ¨¡å‹è·¯å¾„
    SAVE_DIR = "./wav2vec2_mlp_finetune"  # æ¨¡å‹ä¿å­˜ç›®å½•
    
    # éŸ³é¢‘å‚æ•°
    SAMPLE_RATE = 16000  # Wav2Vec2è¦æ±‚çš„é‡‡æ ·ç‡
    WINDOW_SIZE = 1024  # å•ä¸ªçª—å£é‡‡æ ·ç‚¹ï¼ˆ64msï¼Œå¯¹åº”åŸ8kHzçš„512é‡‡æ ·ç‚¹æ—¶é•¿ï¼‰
    MAX_AUDIO_DURATION = 180  # æœ€å¤§éŸ³é¢‘æ—¶é•¿ï¼ˆ3åˆ†é’Ÿ=180ç§’ï¼‰
    MAX_AUDIO_SAMPLES = SAMPLE_RATE * MAX_AUDIO_DURATION  # 3åˆ†é’Ÿå¯¹åº”çš„é‡‡æ ·ç‚¹
    
    # è®­ç»ƒå‚æ•°
    BATCH_SIZE = 32  # RTX 4090å¯è®¾32ï¼Œæ˜¾å­˜ä¸è¶³é™ä¸º16
    EPOCHS = 30
    LEARNING_RATE = 1e-4
    WEIGHT_DECAY = 1e-5  # æ­£åˆ™åŒ–é˜²è¿‡æ‹Ÿåˆ
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    NUM_WORKERS = 16  # æ•°æ®åŠ è½½çº¿ç¨‹æ•°ï¼ˆæ ¹æ®CPUæ ¸å¿ƒè°ƒæ•´ï¼‰
    
    # æ•°æ®é›†åˆ’åˆ†
    TRAIN_RATIO = 0.7
    VAL_RATIO = 0.15
    TEST_RATIO = 0.15


# åˆ›å»ºä¿å­˜ç›®å½•
os.makedirs(Config.SAVE_DIR, exist_ok=True)


# ===================== 2. éŸ³é¢‘é¢„å¤„ç†ç±»ï¼ˆé€‚é…åˆ†çª—é€»è¾‘ï¼‰ =====================
class AudioPreprocessor:
    def __init__(self):
        self.sample_rate = Config.SAMPLE_RATE
        self.window_size = Config.WINDOW_SIZE  # å•ä¸ªçª—å£é‡‡æ ·ç‚¹ï¼ˆ1024ï¼‰
        self.max_audio_samples = Config.MAX_AUDIO_SAMPLES  # 3åˆ†é’Ÿé‡‡æ ·ç‚¹
        self.window_count = None  # å…¨å±€å›ºå®šçª—å£æ•°ï¼ˆ95åˆ†ä½æ•°è®¡ç®—ï¼‰
        self.total_window_samples = None  # æ€»é‡‡æ ·ç‚¹ = window_count Ã— window_size
        self._setup_warnings()

    def _setup_warnings(self):
        """è¿‡æ»¤æ— å…³è­¦å‘Š"""
        warnings.filterwarnings("ignore", message="PySoundFile failed. Trying audioread instead.")
        warnings.filterwarnings("ignore", category=FutureWarning, message="librosa.core.audio.__audioread_load")

    def load_audio(self, file_path):
        """åŠ è½½éŸ³é¢‘å¹¶ç»Ÿä¸€é‡‡æ ·ç‡ä¸º16kHz"""
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                # åŠ è½½æ—¶å¼ºåˆ¶é‡é‡‡æ ·åˆ°16kHz
                audio, sr = librosa.load(file_path, sr=self.sample_rate)
            # æˆªæ–­è¶…è¿‡3åˆ†é’Ÿçš„éƒ¨åˆ†
            if len(audio) > self.max_audio_samples:
                audio = audio[:self.max_audio_samples]
            return audio
        except Exception as e:
            print(f"âš ï¸ åŠ è½½éŸ³é¢‘å¤±è´¥ {file_path}: {str(e)[:50]}")
            return None

    def calculate_window_params(self, audio_files):
        """å¤šçº¿ç¨‹è®¡ç®—å…¨å±€çª—å£æ•°ï¼ˆåŸºäºéŸ³é¢‘é•¿åº¦95åˆ†ä½æ•°ï¼‰"""
        print(f"\nğŸ“Š è®¡ç®—éŸ³é¢‘é•¿åº¦åˆ†å¸ƒï¼ˆ{len(audio_files)}ä¸ªæ–‡ä»¶ï¼Œ3åˆ†é’Ÿæˆªæ–­ï¼‰")
        
        # å•æ–‡ä»¶å¤„ç†å‡½æ•°
        def process_single_file(file_path):
            audio = self.load_audio(file_path)
            return len(audio) if audio is not None else 0

        # 128çº¿ç¨‹å¹¶è¡Œè®¡ç®—éŸ³é¢‘é•¿åº¦
        lengths = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=128) as executor:
            futures = [executor.submit(process_single_file, f) for f in audio_files]
            for future in tqdm(
                concurrent.futures.as_completed(futures),
                total=len(audio_files),
                desc="è®¡ç®—éŸ³é¢‘é•¿åº¦"
            ):
                length = future.result()
                if length > 0:  # è¿‡æ»¤åŠ è½½å¤±è´¥çš„éŸ³é¢‘
                    lengths.append(length)

        # å¤„ç†æ— æœ‰æ•ˆéŸ³é¢‘çš„æç«¯æƒ…å†µ
        if not lengths:
            raise ValueError("âŒ æ— æœ‰æ•ˆéŸ³é¢‘æ–‡ä»¶ï¼Œæ£€æŸ¥æ•°æ®é›†è·¯å¾„æˆ–æ–‡ä»¶æ ¼å¼")
        
        # è®¡ç®—95åˆ†ä½æ•°å¹¶ç¡®å®šçª—å£æ•°
        percentile_95 = np.percentile(lengths, 95)
        self.window_count = int(np.ceil(percentile_95 / self.window_size))  # å‘ä¸Šå–æ•´ç¡®ä¿è¦†ç›–95%éŸ³é¢‘
        self.total_window_samples = self.window_count * self.window_size
        
        # æ—¥å¿—è¾“å‡º
        print(f"âœ… éŸ³é¢‘é•¿åº¦ç»Ÿè®¡ï¼š")
        print(f"   - 95åˆ†ä½æ•°é•¿åº¦ï¼š{percentile_95:.0f} é‡‡æ ·ç‚¹ï¼ˆ{percentile_95/self.sample_rate:.2f}ç§’ï¼‰")
        print(f"   - å…¨å±€çª—å£æ•°ï¼š{self.window_count} ä¸ª")
        print(f"   - å•éŸ³é¢‘æ€»é‡‡æ ·ç‚¹ï¼š{self.total_window_samples}ï¼ˆ{self.total_window_samples/self.sample_rate:.2f}ç§’ï¼‰")
        
        # æ˜¾å­˜é¢„è­¦
        if self.window_count > 3000:
            print(f"âš ï¸ çª—å£æ•°è¾ƒå¤šï¼ˆ{self.window_count}ï¼‰ï¼Œå»ºè®®é™ä½95åˆ†ä½æ•°æˆ–å‡å°çª—å£å¤§å°")
        return self.window_count, self.total_window_samples

    def split_audio_to_windows(self, audio):
        """å°†å•éŸ³é¢‘åˆ‡åˆ†ä¸ºå›ºå®šçª—å£ï¼ˆæˆªæ–­/è¡¥é›¶ï¼‰"""
        if self.window_count is None:
            raise RuntimeError("âŒ è¯·å…ˆè°ƒç”¨ calculate_window_params è®¡ç®—çª—å£å‚æ•°")
        
        # å¤„ç†ç©ºéŸ³é¢‘
        if audio is None or len(audio) == 0:
            return np.zeros((self.window_count, self.window_size), dtype=np.float32)
        
        # æˆªæ–­/è¡¥é›¶åˆ°æ€»çª—å£é‡‡æ ·ç‚¹
        if len(audio) > self.total_window_samples:
            audio = audio[:self.total_window_samples]
        else:
            audio = np.pad(audio, (0, self.total_window_samples - len(audio)), mode="constant")
        
        # åˆ‡åˆ†çª—å£ï¼ˆshape: [window_count, window_size]ï¼‰
        windows = np.array([
            audio[i*self.window_size : (i+1)*self.window_size]
            for i in range(self.window_count)
        ], dtype=np.float32)
        return windows


# ===================== 3. æ•°æ®é›†æ„å»ºï¼ˆåŠ è½½+åˆ’åˆ†+ç‰¹å¾æå–ï¼‰ =====================
def collect_audio_paths(data_root):
    """éå†EDACç›®å½•ï¼Œæ”¶é›†æ‰€æœ‰WAVæ–‡ä»¶è·¯å¾„å’Œæ ‡ç­¾ï¼ˆç±»åˆ«=æ–‡ä»¶å¤¹åï¼‰"""
    audio_info = []
    class_folders = [f for f in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, f))]
    if not class_folders:
        raise ValueError(f"âŒ åœ¨ {data_root} æœªæ‰¾åˆ°ç±»åˆ«æ–‡ä»¶å¤¹")
    
    print(f"\nğŸ“ å‘ç° {len(class_folders)} ä¸ªç±»åˆ«ï¼š{class_folders}")
    for class_name in class_folders:
        class_dir = os.path.join(data_root, class_name)
        wav_files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if f.endswith(".wav")]
        if not wav_files:
            print(f"âš ï¸ ç±»åˆ« {class_name} ä¸‹æ— WAVæ–‡ä»¶ï¼Œè·³è¿‡")
            continue
        
        # è®°å½•è·¯å¾„å’Œæ ‡ç­¾
        for file_path in wav_files:
            audio_info.append({"path": file_path, "label": class_name})
    
    if not audio_info:
        raise ValueError("âŒ æœªæ”¶é›†åˆ°ä»»ä½•WAVæ–‡ä»¶")
    df = pd.DataFrame(audio_info)
    print(f"âœ… å…±æ”¶é›† {len(df)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
    print(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒï¼š\n{df['label'].value_counts()}")
    return df


def split_dataset_stratified(df):
    """åˆ†å±‚åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†ï¼ˆä¿æŒç±»åˆ«æ¯”ä¾‹ï¼‰"""
    # 1. å…ˆåˆ†è®­ç»ƒé›†å’Œæš‚å­˜é›†ï¼ˆ7:3ï¼‰
    train_df, temp_df = train_test_split(
        df,
        test_size=1 - Config.TRAIN_RATIO,
        stratify=df["label"],
        random_state=42
    )
    # 2. æš‚å­˜é›†åˆ†éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼ˆ1:1ï¼‰
    val_df, test_df = train_test_split(
        temp_df,
        test_size=Config.TEST_RATIO / (Config.VAL_RATIO + Config.TEST_RATIO),
        stratify=temp_df["label"],
        random_state=42
    )
    
    # éªŒè¯ç±»åˆ«æ¯”ä¾‹
    print(f"\nğŸ“ˆ æ•°æ®é›†åˆ’åˆ†ç»“æœï¼š")
    print(f"   - è®­ç»ƒé›†ï¼š{len(train_df)} æ¡ï¼ˆ{len(train_df)/len(df)*100:.1f}%ï¼‰")
    print(f"   - éªŒè¯é›†ï¼š{len(val_df)} æ¡ï¼ˆ{len(val_df)/len(df)*100:.1f}%ï¼‰")
    print(f"   - æµ‹è¯•é›†ï¼š{len(test_df)} æ¡ï¼ˆ{len(test_df)/len(df)*100:.1f}%ï¼‰")
    print(f"   - è®­ç»ƒé›†ç±»åˆ«æ¯”ä¾‹ï¼š\n{train_df['label'].value_counts(normalize=True).round(3)}")
    print(f"   - æµ‹è¯•é›†ç±»åˆ«æ¯”ä¾‹ï¼š\n{test_df['label'].value_counts(normalize=True).round(3)}")
    return train_df, val_df, test_df


class DiseaseAudioDataset(Dataset):
    def __init__(self, df, preprocessor, processor, label2id):
        self.df = df
        self.preprocessor = preprocessor  # éŸ³é¢‘åˆ†çª—å¤„ç†å™¨
        self.processor = processor  # Wav2Vec2å¤„ç†å™¨
        self.label2id = label2id  # æ ‡ç­¾â†’IDæ˜ å°„
        self.num_classes = len(label2id)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        file_path = row["path"]
        label = self.label2id[row["label"]]
        
        # 1. åŠ è½½å¹¶åˆ†çª—éŸ³é¢‘
        audio = self.preprocessor.load_audio(file_path)
        windows = self.preprocessor.split_audio_to_windows(audio)  # [window_count, 1024]
        
        # 2. Wav2Vec2ç‰¹å¾æå–ï¼ˆå•çª—å£ç‰¹å¾â†’å…¨å±€æ± åŒ–ï¼‰
        window_features = []
        with torch.no_grad():  # å†»ç»“Wav2Vec2ï¼Œæ— æ¢¯åº¦è®¡ç®—
            for window in windows:
                # éŸ³é¢‘é¢„å¤„ç†ï¼ˆå½’ä¸€åŒ–+å¼ é‡è½¬æ¢ï¼‰
                inputs = self.processor(
                    window,
                    sampling_rate=Config.SAMPLE_RATE,
                    return_tensors="pt",
                    padding=False
                )["input_values"].squeeze(0)  # [1024]
                
                # ç‰¹å¾æå–ï¼ˆWav2Vec2è¾“å‡ºï¼š[T, 768]ï¼ŒTä¸ºæ—¶åºé•¿åº¦ï¼‰
                outputs = self.wav2vec2(input_values=inputs.unsqueeze(0).to(Config.DEVICE))
                # å•çª—å£ç‰¹å¾æ± åŒ–ï¼ˆ[1, T, 768] â†’ [1, 768]ï¼‰
                pooled = torch.mean(outputs.last_hidden_state, dim=1).cpu()
                window_features.append(pooled)
        
        # 3. æ•´éŸ³é¢‘ç‰¹å¾ï¼ˆæ‰€æœ‰çª—å£ç‰¹å¾å…¨å±€å¹³å‡ï¼š[window_count, 768] â†’ [768]ï¼‰
        audio_feature = torch.mean(torch.cat(window_features, dim=0), dim=0)
        return {"feature": audio_feature, "label": torch.tensor(label, dtype=torch.long)}

    def set_wav2vec2(self, wav2vec2):
        """æ³¨å…¥Wav2Vec2æ¨¡å‹ï¼ˆé¿å…å¤šè¿›ç¨‹é‡å¤åŠ è½½ï¼‰"""
        self.wav2vec2 = wav2vec2
        self.wav2vec2.eval()  # ç‰¹å¾æå–æ¨¡å¼


# ===================== 4. æ¨¡å‹å®šä¹‰ï¼ˆå†»ç»“Wav2Vec2 + MLPåˆ†ç±»å¤´ï¼‰ =====================
class Wav2Vec2MLPClassifier(nn.Module):
    def __init__(self, wav2vec2, num_classes):
        super().__init__()
        # 1. å†»ç»“Wav2Vec2ä¸»ä½“ï¼ˆä»…ç‰¹å¾æå–ï¼‰
        self.wav2vec2 = wav2vec2
        for param in self.wav2vec2.parameters():
            param.requires_grad = False  # å†»ç»“æ‰€æœ‰å‚æ•°
        
        # 2. MLPåˆ†ç±»å¤´ï¼ˆä»…è®­ç»ƒè¿™éƒ¨åˆ†ï¼‰
        self.mlp_head = nn.Sequential(
            nn.Linear(in_features=wav2vec2.config.hidden_size, out_features=512),
            nn.ReLU(),
            nn.Dropout(p=0.3),  # é˜²è¿‡æ‹Ÿåˆ
            nn.Linear(in_features=512, out_features=256),
            nn.ReLU(),
            nn.Dropout(p=0.3),
            nn.Linear(in_features=256, out_features=num_classes)
        )

    def forward(self, x):
        """x: [batch_size, 768]ï¼ˆéŸ³é¢‘å…¨å±€ç‰¹å¾ï¼‰"""
        return self.mlp_head(x)


# ===================== 5. è®­ç»ƒä¸è¯„ä¼°å‡½æ•° =====================
def train_one_epoch(model, dataloader, criterion, optimizer, scaler):
    model.train()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    
    for batch in tqdm(dataloader, desc="è®­ç»ƒä¸­"):
        features = batch["feature"].to(Config.DEVICE)
        labels = batch["label"].to(Config.DEVICE)
        
        optimizer.zero_grad()
        
        # æ··åˆç²¾åº¦è®­ç»ƒï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
        with autocast():
            logits = model(features)
            loss = criterion(logits, labels)
        
        # åå‘ä¼ æ’­
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        # è®°å½• metrics
        total_loss += loss.item() * features.size(0)
        preds = torch.argmax(logits, dim=1).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())
    
    # è®¡ç®— epoch æŒ‡æ ‡
    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="weighted")  # åº”å¯¹ç±»åˆ«ä¸å¹³è¡¡
    return avg_loss, accuracy, f1


def evaluate(model, dataloader, criterion):
    model.eval()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="è¯„ä¼°ä¸­"):
            features = batch["feature"].to(Config.DEVICE)
            labels = batch["label"].to(Config.DEVICE)
            
            with autocast():
                logits = model(features)
                loss = criterion(logits, labels)
            
            total_loss += loss.item() * features.size(0)
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())
    
    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="weighted")
    return avg_loss, accuracy, f1


# ===================== 6. ä¸»æµç¨‹ï¼ˆæ•°æ®â†’è®­ç»ƒâ†’æµ‹è¯•ï¼‰ =====================
def main():
    # Step 1: æ”¶é›†æ•°æ®è·¯å¾„å¹¶åˆ†å±‚åˆ’åˆ†
    df = collect_audio_paths(Config.DATA_ROOT)
    train_df, val_df, test_df = split_dataset_stratified(df)
    
    # Step 2: æ„å»ºæ ‡ç­¾æ˜ å°„ï¼ˆç±»åˆ«â†’IDï¼‰
    labels = df["label"].unique()
    label2id = {cls: idx for idx, cls in enumerate(labels)}
    id2label = {idx: cls for cls, idx in label2id.items()}
    num_classes = len(labels)
    print(f"\nğŸ·ï¸ æ ‡ç­¾æ˜ å°„ï¼š{label2id}")

    # Step 3: åˆå§‹åŒ–é¢„å¤„ç†å·¥å…·
    preprocessor = AudioPreprocessor()
    # è®¡ç®—å…¨å±€çª—å£å‚æ•°ï¼ˆç”¨æ‰€æœ‰éŸ³é¢‘çš„é•¿åº¦ï¼‰
    all_audio_paths = df["path"].tolist()
    preprocessor.calculate_window_params(all_audio_paths)
    
    # Step 4: åŠ è½½Wav2Vec2æ¨¡å‹å’Œå¤„ç†å™¨
    print(f"\nğŸ”§ åŠ è½½Wav2Vec2æ¨¡å‹ï¼ˆ{Config.MODEL_PATH}ï¼‰")
    processor = Wav2Vec2Processor.from_pretrained(Config.MODEL_PATH)
    wav2vec2 = Wav2Vec2Model.from_pretrained(Config.MODEL_PATH).to(Config.DEVICE)
    wav2vec2.eval()  # ç‰¹å¾æå–æ¨¡å¼ï¼Œä¸è®­ç»ƒ

    # Step 5: æ„å»ºæ•°æ®é›†å’ŒDataLoader
    print(f"\nğŸš€ æ„å»ºæ•°æ®é›†...")
    # è®­ç»ƒé›†
    train_dataset = DiseaseAudioDataset(train_df, preprocessor, processor, label2id)
    train_dataset.set_wav2vec2(wav2vec2)
    train_loader = DataLoader(
        train_dataset,
        batch_size=Config.BATCH_SIZE,
        shuffle=True,
        num_workers=Config.NUM_WORKERS,
        drop_last=True,
        multiprocessing_context='spawn'  # æ–°å¢
    )
    # éªŒè¯é›†
    val_dataset = DiseaseAudioDataset(val_df, preprocessor, processor, label2id)
    val_dataset.set_wav2vec2(wav2vec2)
    val_loader = DataLoader(
        val_dataset,
        batch_size=Config.BATCH_SIZE,
        shuffle=False,
        num_workers=Config.NUM_WORKERS,
        multiprocessing_context='spawn'  # æ–°å¢
    )
    # æµ‹è¯•é›†
    test_dataset = DiseaseAudioDataset(test_df, preprocessor, processor, label2id)
    test_dataset.set_wav2vec2(wav2vec2)
    test_loader = DataLoader(
        test_dataset,
        batch_size=Config.BATCH_SIZE,
        shuffle=False,
        num_workers=Config.NUM_WORKERS,
        multiprocessing_context='spawn'  # æ–°å¢
    )

    # Step 6: åˆå§‹åŒ–åˆ†ç±»æ¨¡å‹ã€æŸå¤±ã€ä¼˜åŒ–å™¨
    model = Wav2Vec2MLPClassifier(wav2vec2, num_classes=num_classes).to(Config.DEVICE)
    criterion = nn.CrossEntropyLoss()
    # ä»…ä¼˜åŒ–MLPå¤´å‚æ•°ï¼ˆWav2Vec2å·²å†»ç»“ï¼‰
    optimizer = optim.AdamW(
        model.mlp_head.parameters(),
        lr=Config.LEARNING_RATE,
        weight_decay=Config.WEIGHT_DECAY
    )
    scaler = GradScaler()  # æ··åˆç²¾åº¦
    best_val_f1 = 0.0  # ä¿å­˜æœ€ä¼˜æ¨¡å‹ç”¨

    # Step 7: è®­ç»ƒå¾ªç¯
    print(f"\nğŸ“Œ å¼€å§‹è®­ç»ƒï¼ˆ{Config.EPOCHS}è½®ï¼Œè®¾å¤‡ï¼š{Config.DEVICE}ï¼‰")
    for epoch in range(1, Config.EPOCHS + 1):
        print(f"\n===== Epoch {epoch}/{Config.EPOCHS} =====")
        # è®­ç»ƒ
        train_loss, train_acc, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer, scaler)
        # éªŒè¯
        val_loss, val_acc, val_f1 = evaluate(model, val_loader, criterion)
        
        # æ—¥å¿—è¾“å‡º
        print(f"ğŸ“Š è®­ç»ƒé›†ï¼šLoss={train_loss:.4f} | Acc={train_acc:.4f} | F1={train_f1:.4f}")
        print(f"ğŸ“Š éªŒè¯é›†ï¼šLoss={val_loss:.4f} | Acc={val_acc:.4f} | F1={val_f1:.4f}")
        
        # ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆåŸºäºéªŒè¯é›†F1ï¼‰
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            save_path = os.path.join(Config.SAVE_DIR, "best_model.pth")
            torch.save(model.state_dict(), save_path)
            print(f"âœ… ä¿å­˜æœ€ä¼˜æ¨¡å‹åˆ° {save_path}ï¼ˆVal F1: {best_val_f1:.4f}ï¼‰")

    # Step 8: æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°
    print(f"\n===== æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼° =====")
    # åŠ è½½æœ€ä¼˜æ¨¡å‹
    best_model = Wav2Vec2MLPClassifier(wav2vec2, num_classes=num_classes).to(Config.DEVICE)
    best_model.load_state_dict(torch.load(os.path.join(Config.SAVE_DIR, "best_model.pth")))
    # è¯„ä¼°
    test_loss, test_acc, test_f1 = evaluate(best_model, test_loader, criterion)
    print(f"ğŸ† æµ‹è¯•é›†ç»“æœï¼šLoss={test_loss:.4f} | Acc={test_acc:.4f} | F1={test_f1:.4f}")
    print(f"\nğŸ‰ å¾®è°ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜è·¯å¾„ï¼š{Config.SAVE_DIR}")


if __name__ == "__main__":
    main()