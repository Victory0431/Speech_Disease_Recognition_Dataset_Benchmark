import os
import warnings
import numpy as np
import pandas as pd
import librosa
import torch
from tqdm import tqdm
from transformers import Wav2Vec2Processor, Wav2Vec2Model
from sklearn.model_selection import train_test_split


# ===================== 1. é…ç½®å‚æ•°ï¼ˆçº¿æ€§é€»è¾‘ï¼Œæ— éœ€å¹¶è¡Œå‚æ•°ï¼‰ =====================
class Config:
    # è·¯å¾„é…ç½®
    DATA_ROOT = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/fresh_datasets/Asthma_Detection_Tawfik"
    MODEL_PATH = "/mnt/data/test1/repo/wav2vec_2/model"
    TEMP_DIR = "./temp_windows"  # ä¿å­˜åˆ†çª—ç»“æœçš„ä¸´æ—¶ç›®å½•
    SAVE_FEAT_DIR = "/mnt/data/test1/wav2vec2_parallel_features/a180s_640000"  # æœ€ç»ˆç‰¹å¾ä¿å­˜ç›®å½•
    
    # éŸ³é¢‘å‚æ•°
    SAMPLE_RATE = 16000
    WINDOW_SIZE = 512
    MAX_AUDIO_DURATION = 180  # 20ç§’æˆªæ–­
    MAX_AUDIO_SAMPLES = SAMPLE_RATE * MAX_AUDIO_DURATION
    
    # æ¨ç†å‚æ•°
    WINDOW_BATCH_SIZE = 64  # GPUæ‰¹é‡æ¨ç†å¤§å°ï¼ˆRTX 4090å¯è®¾64ï¼‰
    GPU_DEVICE = "cuda:0"
    DTYPE = np.float32


# åˆ›å»ºç›®å½•
os.makedirs(Config.TEMP_DIR, exist_ok=True)
os.makedirs(Config.SAVE_FEAT_DIR, exist_ok=True)
# è¿‡æ»¤è­¦å‘Š
warnings.filterwarnings("ignore", message="PySoundFile failed. Trying audioread instead.")
warnings.filterwarnings("ignore", category=FutureWarning, message="librosa.core.audio.__audioread_load")


# ===================== 2. Step 1ï¼šCPUå…¨éƒ¨åˆ†çª—ï¼Œä¿å­˜çª—å£+å…ƒä¿¡æ¯ =====================
def load_single_audio(file_path):
    """åŠ è½½éŸ³é¢‘ï¼š16kHz+20ç§’æˆªæ–­"""
    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            audio, _ = librosa.load(file_path, sr=Config.SAMPLE_RATE)
        if len(audio) > Config.MAX_AUDIO_SAMPLES:
            audio = audio[:Config.MAX_AUDIO_SAMPLES]
        return audio
    except Exception as e:
        print(f"âš ï¸ è·³è¿‡æŸåéŸ³é¢‘ï¼š{file_path}")
        return None


def calculate_global_window_params(all_audio_paths):
    """è®¡ç®—å…¨å±€çª—å£æ•°ï¼ˆ95åˆ†ä½æ•°ï¼‰"""
    print(f"\nğŸ“Š è®¡ç®—éŸ³é¢‘é•¿åº¦åˆ†å¸ƒï¼ˆ{len(all_audio_paths)}ä¸ªæ–‡ä»¶ï¼‰")
    audio_lengths = []
    for path in tqdm(all_audio_paths, desc="è®¡ç®—é•¿åº¦"):
        audio = load_single_audio(path)
        if audio is not None:
            audio_lengths.append(len(audio))
    percentile_95 = np.percentile(audio_lengths, 95)
    window_count = int(np.ceil(percentile_95 / Config.WINDOW_SIZE))
    total_samples = window_count * Config.WINDOW_SIZE
    print(f"âœ… å…¨å±€çª—å£æ•°ï¼š{window_count} | å•éŸ³é¢‘æ€»é‡‡æ ·ç‚¹ï¼š{total_samples}")
    return window_count, total_samples


def collect_audio_info(data_root):
    """æ”¶é›†éŸ³é¢‘IDã€è·¯å¾„ã€æ ‡ç­¾"""
    audio_info = []
    class_folders = [f for f in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, f))]
    audio_id = 0
    for class_name in class_folders:
        for f in os.listdir(os.path.join(data_root, class_name)):
            if f.endswith(".wav"):
                audio_info.append({
                    "audio_id": audio_id,
                    "path": os.path.join(data_root, class_name, f),
                    "label": class_name
                })
                audio_id += 1
    return pd.DataFrame(audio_info)


def split_all_audio_to_windows(audio_df, window_count, total_samples):
    """å…¨éƒ¨åˆ†çª—ï¼Œä¿å­˜çª—å£ï¼ˆnumpyæ–‡ä»¶ï¼‰å’Œå…ƒä¿¡æ¯"""
    print(f"\nğŸš€ å¼€å§‹å…¨éƒ¨åˆ†çª—ï¼ˆå…± {len(audio_df)} ä¸ªéŸ³é¢‘ï¼‰")
    meta_info = []  # è®°å½•æ¯ä¸ªéŸ³é¢‘çš„å…ƒä¿¡æ¯ï¼ˆaudio_id, label, window_count, window_pathï¼‰
    
    for _, row in tqdm(audio_df.iterrows(), total=len(audio_df), desc="åˆ†çª—è¿›åº¦"):
        audio_id = row["audio_id"]
        file_path = row["path"]
        label = row["label"]
        
        # åŠ è½½å¹¶åˆ†çª—
        audio = load_single_audio(file_path)
        if audio is None:
            continue
        # è¡¥é›¶/æˆªæ–­
        if len(audio) < total_samples:
            audio = np.pad(audio, (0, total_samples - len(audio)), mode="constant")
        else:
            audio = audio[:total_samples]
        # åˆ†çª—
        windows = np.array([
            audio[i*Config.WINDOW_SIZE : (i+1)*Config.WINDOW_SIZE]
            for i in range(window_count)
        ], dtype=Config.DTYPE)  # [window_count, 1024]
        
        # ä¿å­˜çª—å£ï¼ˆæŒ‰audio_idå‘½åï¼Œæ–¹ä¾¿åç»­åŠ è½½ï¼‰
        window_save_path = os.path.join(Config.TEMP_DIR, f"audio_{audio_id}_windows.npy")
        np.save(window_save_path, windows)
        
        # è®°å½•å…ƒä¿¡æ¯
        meta_info.append({
            "audio_id": audio_id,
            "label": label,
            "window_count": window_count,
            "window_path": window_save_path
        })
    
    # ä¿å­˜å…ƒä¿¡æ¯ï¼ˆåç»­æ¨ç†å’Œèšåˆç”¨ï¼‰
    meta_df = pd.DataFrame(meta_info)
    meta_save_path = os.path.join(Config.TEMP_DIR, "audio_meta.csv")
    meta_df.to_csv(meta_save_path, index=False)
    print(f"\nâœ… åˆ†çª—å®Œæˆï¼")
    print(f"   - æœ‰æ•ˆéŸ³é¢‘æ•°ï¼š{len(meta_df)}")
    print(f"   - å…ƒä¿¡æ¯ä¿å­˜è·¯å¾„ï¼š{meta_save_path}")
    print(f"   - çª—å£æ–‡ä»¶ä¿å­˜è·¯å¾„ï¼š{Config.TEMP_DIR}")
    return meta_df


# ===================== 3. Step 2ï¼šGPUæ‰¹é‡åŠ è½½çª—å£ï¼Œæ¨ç†å¹¶ä¿å­˜çª—å£ç‰¹å¾ =====================
def gpu_batch_infer_windows(meta_df):
    """GPUæ‰¹é‡æ¨ç†çª—å£ç‰¹å¾ï¼Œä¿å­˜çª—å£çº§ç‰¹å¾"""
    print(f"\nğŸ”§ åŠ è½½Wav2Vec2æ¨¡å‹ï¼ˆè®¾å¤‡ï¼š{Config.GPU_DEVICE}ï¼‰")
    device = torch.device(Config.GPU_DEVICE if torch.cuda.is_available() else "cpu")
    processor = Wav2Vec2Processor.from_pretrained(Config.MODEL_PATH)
    model = Wav2Vec2Model.from_pretrained(Config.MODEL_PATH).to(device)
    model.eval()
    
    # æ”¶é›†æ‰€æœ‰çª—å£è·¯å¾„å’Œaudio_id
    all_window_paths = meta_df["window_path"].tolist()
    all_audio_ids = meta_df["audio_id"].tolist()
    print(f"\nğŸš€ å¼€å§‹GPUæ‰¹é‡æ¨ç†ï¼ˆå…± {len(all_window_paths)} ä¸ªéŸ³é¢‘ï¼Œæ‰¹æ¬¡å¤§å°ï¼š{Config.WINDOW_BATCH_SIZE}ï¼‰")
    
    with torch.no_grad(), torch.cuda.amp.autocast():  # æ··åˆç²¾åº¦æ¨ç†
        for audio_id, window_path in tqdm(zip(all_audio_ids, all_window_paths), total=len(all_audio_ids), desc="æ¨ç†è¿›åº¦"):
            # åŠ è½½å•ä¸ªéŸ³é¢‘çš„æ‰€æœ‰çª—å£
            windows = np.load(window_path)  # [window_count, 1024]
            window_count = windows.shape[0]
            
            # æŒ‰æ‰¹æ¬¡æ¨ç†å½“å‰éŸ³é¢‘çš„çª—å£
            window_feats = []
            for i in range(0, window_count, Config.WINDOW_BATCH_SIZE):
                batch_windows = windows[i:i+Config.WINDOW_BATCH_SIZE]  # [batch_size, 1024]
                # é¢„å¤„ç†
                inputs = processor(
                    batch_windows.tolist(),
                    sampling_rate=Config.SAMPLE_RATE,
                    return_tensors="pt",
                    padding=False
                )["input_values"].to(device)
                # æ¨ç†+æ—¶åºæ± åŒ–ï¼ˆç¬¬ä¸€æ¬¡æ± åŒ–ï¼‰
                outputs = model(input_values=inputs)
                batch_feats = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()  # [batch_size, 768]
                window_feats.append(batch_feats)
            
            # æ‹¼æ¥å½“å‰éŸ³é¢‘çš„æ‰€æœ‰çª—å£ç‰¹å¾ï¼Œä¿å­˜
            window_feats = np.concatenate(window_feats, axis=0)  # [window_count, 768]
            feat_save_path = os.path.join(Config.TEMP_DIR, f"audio_{audio_id}_window_feats.npy")
            np.save(feat_save_path, window_feats)
            
            # ï¼ˆå¯é€‰ï¼‰åˆ é™¤åŸå§‹çª—å£æ–‡ä»¶ï¼ŒèŠ‚çœç£ç›˜ç©ºé—´
            # os.remove(window_path)
    
    print(f"\nâœ… GPUæ¨ç†å®Œæˆï¼çª—å£ç‰¹å¾ä¿å­˜è·¯å¾„ï¼š{Config.TEMP_DIR}")


# ===================== 4. Step 3ï¼šèšåˆçª—å£ç‰¹å¾â†’éŸ³é¢‘çº§ç‰¹å¾ï¼Œä¿å­˜æœ€ç»ˆç»“æœ =====================
def aggregate_window_feats(meta_df):
    """æŒ‰audio_idèšåˆçª—å£ç‰¹å¾ï¼ˆç¬¬äºŒæ¬¡æ± åŒ–ï¼‰ï¼Œåˆ’åˆ†æ•°æ®é›†å¹¶ä¿å­˜"""
    print(f"\nğŸ“Š èšåˆçª—å£ç‰¹å¾ï¼ˆå…± {len(meta_df)} ä¸ªéŸ³é¢‘ï¼‰")
    # æ„å»ºç±»åˆ«â†’IDæ˜ å°„
    label2id = {cls: idx for idx, cls in enumerate(meta_df["label"].unique())}
    all_features = []
    all_labels = []
    
    for _, row in tqdm(meta_df.iterrows(), total=len(meta_df), desc="èšåˆè¿›åº¦"):
        audio_id = row["audio_id"]
        label = row["label"]
        # åŠ è½½çª—å£ç‰¹å¾
        feat_path = os.path.join(Config.TEMP_DIR, f"audio_{audio_id}_window_feats.npy")
        window_feats = np.load(feat_path)  # [window_count, 768]
        # çª—å£ç»´åº¦æ± åŒ–ï¼ˆç¬¬äºŒæ¬¡æ± åŒ–ï¼‰
        audio_feat = np.mean(window_feats, axis=0)  # [768]
        # ä¿å­˜
        all_features.append(audio_feat)
        all_labels.append(label2id[label])
        
        # ï¼ˆå¯é€‰ï¼‰åˆ é™¤çª—å£ç‰¹å¾æ–‡ä»¶ï¼ŒèŠ‚çœç£ç›˜ç©ºé—´
        # os.remove(feat_path)
    
    # è½¬ä¸ºnumpyæ•°ç»„
    all_features = np.array(all_features, dtype=Config.DTYPE)  # [N, 768]
    all_labels = np.array(all_labels, dtype=np.int64)        # [N]
    
    # åˆ†å±‚åˆ’åˆ†æ•°æ®é›†
    train_feat, temp_feat, train_label, temp_label = train_test_split(
        all_features, all_labels, test_size=0.3, stratify=all_labels, random_state=42
    )
    val_feat, test_feat, val_label, test_label = train_test_split(
        temp_feat, temp_label, test_size=0.5, stratify=temp_label, random_state=42
    )
    
    # ä¿å­˜æœ€ç»ˆç‰¹å¾
    save_paths = {
        "train_feat": os.path.join(Config.SAVE_FEAT_DIR, "train_feat.npy"),
        "train_label": os.path.join(Config.SAVE_FEAT_DIR, "train_label.npy"),
        "val_feat": os.path.join(Config.SAVE_FEAT_DIR, "val_feat.npy"),
        "val_label": os.path.join(Config.SAVE_FEAT_DIR, "val_label.npy"),
        "test_feat": os.path.join(Config.SAVE_FEAT_DIR, "test_feat.npy"),
        "test_label": os.path.join(Config.SAVE_FEAT_DIR, "test_label.npy"),
        "label2id": os.path.join(Config.SAVE_FEAT_DIR, "label2id.npy")
    }
    np.save(save_paths["train_feat"], train_feat)
    np.save(save_paths["train_label"], train_label)
    np.save(save_paths["val_feat"], val_feat)
    np.save(save_paths["val_label"], val_label)
    np.save(save_paths["test_feat"], test_feat)
    np.save(save_paths["test_label"], test_label)
    np.save(save_paths["label2id"], label2id, allow_pickle=True)
    
    # æ‰“å°ç»“æœ
    print(f"\nğŸ‰ å…¨æµç¨‹å®Œæˆï¼æœ€ç»ˆç‰¹å¾ä¿¡æ¯ï¼š")
    print(f"   - æ€»æœ‰æ•ˆéŸ³é¢‘æ•°ï¼š{len(all_features)}")
    print(f"   - è®­ç»ƒé›†ï¼š{len(train_feat)} æ ·æœ¬ | éªŒè¯é›†ï¼š{len(val_feat)} | æµ‹è¯•é›†ï¼š{len(test_feat)}")
    print(f"   - ç‰¹å¾ä¿å­˜è·¯å¾„ï¼š{Config.SAVE_FEAT_DIR}")


# ===================== 5. ä¸»å‡½æ•°ï¼ˆçº¿æ€§æ‰§è¡Œ3æ­¥ï¼‰ =====================
def main():
    # Step 1ï¼šæ”¶é›†éŸ³é¢‘ä¿¡æ¯â†’è®¡ç®—çª—å£å‚æ•°â†’å…¨éƒ¨åˆ†çª—
    audio_df = collect_audio_info(Config.DATA_ROOT)
    window_count, total_samples = calculate_global_window_params(audio_df["path"].tolist())
    meta_df = split_all_audio_to_windows(audio_df, window_count, total_samples)
    
    # Step 2ï¼šGPUæ‰¹é‡æ¨ç†çª—å£ç‰¹å¾
    gpu_batch_infer_windows(meta_df)
    
    # Step 3ï¼šèšåˆç‰¹å¾â†’ä¿å­˜æœ€ç»ˆç»“æœ
    aggregate_window_feats(meta_df)


if __name__ == "__main__":
    main()