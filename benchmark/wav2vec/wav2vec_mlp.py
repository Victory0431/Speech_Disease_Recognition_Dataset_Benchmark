import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import GradScaler, autocast
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm


# ===================== 1. é…ç½®å‚æ•°ï¼ˆä»…éœ€ç¡®è®¤ç‰¹å¾è·¯å¾„ï¼‰ =====================
class Config:
    # å…³é”®ï¼šé¢„æå–ç‰¹å¾çš„ä¿å­˜è·¯å¾„ï¼ˆå³ä½ ä¹‹å‰çš„/mnt/data/test1/wav2vec2_parallel_featuresï¼‰
    FEAT_ROOT = "/mnt/data/test1/wav2vec2_parallel_features/02"
    
    # è®­ç»ƒå‚æ•°ï¼ˆRTX 4090å¯æŒ‰æ­¤é…ç½®ï¼Œæ˜¾å­˜ä¸è¶³å¯å‡å°BATCH_SIZEï¼‰
    BATCH_SIZE = 64        # æ‰¹é‡å¤§å°ï¼ˆ64é€‚åˆ24GBæ˜¾å­˜ï¼‰
    EPOCHS = 50            # è®­ç»ƒè½®æ¬¡ï¼ˆè¶³å¤Ÿæ”¶æ•›ï¼Œæ”¯æŒæ—©åœï¼‰
    LEARNING_RATE = 1e-4   # å­¦ä¹ ç‡ï¼ˆMLPåˆ†ç±»å¤´é€‚é…å€¼ï¼‰
    WEIGHT_DECAY = 1e-5    # æƒé‡è¡°å‡ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰
    DROPOUT = 0.3          # Dropoutæ¯”ä¾‹
    DEVICE = torch.device("cuda:4" if torch.cuda.is_available() else "cpu")  # ä½ çš„GPUè®¾å¤‡
    
    # è¯„ä¼°å‚æ•°
    EVAL_METRIC = "weighted"  # æŒ‡æ ‡è®¡ç®—æ–¹å¼ï¼ˆweighted=è€ƒè™‘ç±»åˆ«ä¸å¹³è¡¡ï¼Œmacro=ç±»åˆ«å¹³ç­‰ï¼‰
    PLOT_CONFUSION_MATRIX = True  # æ˜¯å¦ç»˜åˆ¶æ··æ·†çŸ©é˜µï¼ˆå¯é€‰ï¼‰
    SAVE_MODEL_DIR = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/benchmark/wav2vec"  # æœ€ä¼˜æ¨¡å‹ä¿å­˜ç›®å½•


# åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
os.makedirs(Config.SAVE_MODEL_DIR, exist_ok=True)


# ===================== 2. æ•°æ®åŠ è½½ï¼ˆä»…è¯»å–NPYæ–‡ä»¶ï¼Œå®Œå…¨ç‹¬ç«‹ï¼‰ =====================
class FeatDataset(Dataset):
    """åŠ è½½é¢„æå–çš„ç‰¹å¾å’Œæ ‡ç­¾ï¼Œé€‚é…PyTorch DataLoader"""
    def __init__(self, feat_path, label_path):
        # åŠ è½½ç‰¹å¾ï¼ˆ[N, 768]ï¼‰å’Œæ ‡ç­¾ï¼ˆ[N]ï¼‰
        self.feats = torch.tensor(np.load(feat_path), dtype=torch.float32)
        self.labels = torch.tensor(np.load(label_path), dtype=torch.long)
        
        # éªŒè¯ç‰¹å¾å’Œæ ‡ç­¾ç»´åº¦åŒ¹é…
        assert len(self.feats) == len(self.labels), f"ç‰¹å¾æ•°ï¼ˆ{len(self.feats)}ï¼‰ä¸æ ‡ç­¾æ•°ï¼ˆ{len(self.labels)}ï¼‰ä¸åŒ¹é…ï¼"

    def __len__(self):
        return len(self.feats)

    def __getitem__(self, idx):
        return {"feat": self.feats[idx], "label": self.labels[idx]}


def load_all_data(feat_root):
    """åŠ è½½è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ï¼Œè¿”å›DataLoaderå’Œç±»åˆ«æ˜ å°„"""
    # 1. å®šä¹‰æ–‡ä»¶è·¯å¾„ï¼ˆä¸ç‰¹å¾æå–é˜¶æ®µçš„ä¿å­˜æ–‡ä»¶åä¸€è‡´ï¼‰
    file_paths = {
        "train_feat": os.path.join(feat_root, "train_feat.npy"),
        "train_label": os.path.join(feat_root, "train_label.npy"),
        "val_feat": os.path.join(feat_root, "val_feat.npy"),
        "val_label": os.path.join(feat_root, "val_label.npy"),
        "test_feat": os.path.join(feat_root, "test_feat.npy"),
        "test_label": os.path.join(feat_root, "test_label.npy"),
        "label2id": os.path.join(feat_root, "label2id.npy")
    }
    
    # 2. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    for name, path in file_paths.items():
        if not os.path.exists(path):
            raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°æ–‡ä»¶ï¼š{path}ï¼Œè¯·ç¡®è®¤FEAT_ROOTè·¯å¾„æ­£ç¡®")
    
    # 3. åŠ è½½ç±»åˆ«æ˜ å°„ï¼ˆlabelâ†’IDï¼ŒIDâ†’labelï¼‰
    label2id = np.load(file_paths["label2id"], allow_pickle=True).item()  # å­—å…¸æ ¼å¼
    id2label = {idx: cls for cls, idx in label2id.items()}
    num_classes = len(label2id)
    print(f"âœ… åŠ è½½ç±»åˆ«æ˜ å°„ï¼š{label2id} | æ€»ç±»åˆ«æ•°ï¼š{num_classes}")
    
    # 4. åŠ è½½æ•°æ®é›†å¹¶åˆ›å»ºDataLoader
    train_dataset = FeatDataset(file_paths["train_feat"], file_paths["train_label"])
    val_dataset = FeatDataset(file_paths["val_feat"], file_paths["val_label"])
    test_dataset = FeatDataset(file_paths["test_feat"], file_paths["test_label"])
    
    train_loader = DataLoader(
        train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, drop_last=True
    )
    val_loader = DataLoader(
        val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False
    )
    test_loader = DataLoader(
        test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False
    )
    
    # æ‰“å°æ•°æ®ç»Ÿè®¡
    print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡ï¼š")
    print(f"   - è®­ç»ƒé›†ï¼š{len(train_dataset)} æ ·æœ¬ï¼ˆ{len(train_loader)} æ‰¹ï¼‰")
    print(f"   - éªŒè¯é›†ï¼š{len(val_dataset)} æ ·æœ¬ï¼ˆ{len(val_loader)} æ‰¹ï¼‰")
    print(f"   - æµ‹è¯•é›†ï¼š{len(test_dataset)} æ ·æœ¬ï¼ˆ{len(test_loader)} æ‰¹ï¼‰")
    print(f"   - ç‰¹å¾ç»´åº¦ï¼š{train_dataset.feats.shape[1]}ï¼ˆWav2Vec2è¾“å‡ºç»´åº¦ï¼‰")
    
    return train_loader, val_loader, test_loader, label2id, id2label, num_classes


# ===================== 3. MLPåˆ†ç±»æ¨¡å‹å®šä¹‰ï¼ˆé€‚é…768ç»´è¾“å…¥ï¼‰ =====================
class MLPClassifier(nn.Module):
    """è½»é‡çº§MLPåˆ†ç±»å¤´ï¼Œè¾“å…¥768ç»´ç‰¹å¾ï¼Œè¾“å‡ºç±»åˆ«æ¦‚ç‡"""
    def __init__(self, input_dim=768, num_classes=2, dropout=Config.DROPOUT):
        super().__init__()
        self.classifier = nn.Sequential(
            # ç¬¬ä¸€å±‚ï¼š768â†’512ï¼ŒReLUæ¿€æ´»+Dropout
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Dropout(dropout),
            # ç¬¬äºŒå±‚ï¼š512â†’256ï¼ŒReLUæ¿€æ´»+Dropout
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(dropout),
            # è¾“å‡ºå±‚ï¼š256â†’ç±»åˆ«æ•°ï¼ˆæ— æ¿€æ´»ï¼Œåç»­ç”¨CrossEntropyLossï¼‰
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        """x: [batch_size, 768] â†’ logits: [batch_size, num_classes]"""
        return self.classifier(x)


# ===================== 4. è®­ç»ƒä¸è¯„ä¼°å‡½æ•°ï¼ˆå¤šæŒ‡æ ‡è®¡ç®—ï¼‰ =====================
def train_one_epoch(model, dataloader, criterion, optimizer, scaler):
    """è®­ç»ƒä¸€è½®ï¼Œè¿”å›è®­ç»ƒé›†æŸå¤±ã€å‡†ç¡®ç‡ã€F1"""
    model.train()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    
    for batch in tqdm(dataloader, desc="è®­ç»ƒä¸­"):
        # æ•°æ®ç§»è‡³GPU
        feats = batch["feat"].to(Config.DEVICE)
        labels = batch["label"].to(Config.DEVICE)
        
        # æ¢¯åº¦æ¸…é›¶
        optimizer.zero_grad()
        
        # æ··åˆç²¾åº¦è®­ç»ƒï¼ˆèŠ‚çœæ˜¾å­˜+åŠ é€Ÿï¼‰
        with autocast():
            logits = model(feats)
            loss = criterion(logits, labels)
        
        # åå‘ä¼ æ’­ä¸ä¼˜åŒ–
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        # è®°å½•æŸå¤±å’Œé¢„æµ‹ç»“æœ
        total_loss += loss.item() * feats.size(0)
        preds = torch.argmax(logits, dim=1).cpu().numpy()  # å–æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«
        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())
    
    # è®¡ç®—è®­ç»ƒé›†æŒ‡æ ‡
    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    recall = recall_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    f1 = f1_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    
    return avg_loss, accuracy, precision, recall, f1


def evaluate_model(model, dataloader, criterion, id2label=None, is_test=False):
    """è¯„ä¼°æ¨¡å‹ï¼Œè¿”å›æŸå¤±å’Œå¤šæŒ‡æ ‡ï¼Œæµ‹è¯•æ—¶å¯è¾“å‡ºç±»åˆ«è¯¦ç»†æŒ‡æ ‡"""
    model.eval()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():  # å…³é—­æ¢¯åº¦è®¡ç®—
        for batch in tqdm(dataloader, desc="è¯„ä¼°ä¸­"):
            feats = batch["feat"].to(Config.DEVICE)
            labels = batch["label"].to(Config.DEVICE)
            
            with autocast():
                logits = model(feats)
                loss = criterion(logits, labels)
            
            # è®°å½•ç»“æœ
            total_loss += loss.item() * feats.size(0)
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())
    
    # è®¡ç®—æ€»ä½“æŒ‡æ ‡
    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    recall = recall_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    f1 = f1_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    
    # æµ‹è¯•é›†é¢å¤–è¾“å‡ºï¼šç±»åˆ«è¯¦ç»†æŒ‡æ ‡+æ··æ·†çŸ©é˜µ
    if is_test and id2label is not None:
        print(f"\n========== æµ‹è¯•é›†è¯¦ç»†è¯„ä¼°ç»“æœ ==========")
        print(f"1. æ€»ä½“æŒ‡æ ‡ï¼ˆ{Config.EVAL_METRIC}å¹³å‡ï¼‰ï¼š")
        print(f"   - æŸå¤±ï¼š{avg_loss:.4f}")
        print(f"   - å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰ï¼š{accuracy:.4f}")
        print(f"   - ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰ï¼š{precision:.4f}")
        print(f"   - å¬å›ç‡ï¼ˆRecallï¼‰ï¼š{recall:.4f}")
        print(f"   - F1åˆ†æ•°ï¼š{f1:.4f}\n")
        
        # è¾“å‡ºæ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
        print(f"2. å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡ï¼š")
        class_precision = precision_score(all_labels, all_preds, average=None, zero_division=0)
        class_recall = recall_score(all_labels, all_preds, average=None, zero_division=0)
        class_f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)
        
        for idx in range(len(id2label)):
            cls_name = id2label[idx]
            print(f"   - {cls_name}ï¼š")
            print(f"     ç²¾ç¡®ç‡ï¼š{class_precision[idx]:.4f} | å¬å›ç‡ï¼š{class_recall[idx]:.4f} | F1ï¼š{class_f1[idx]:.4f}")
        
        # ç»˜åˆ¶æ··æ·†çŸ©é˜µï¼ˆå¯é€‰ï¼‰
        if Config.PLOT_CONFUSION_MATRIX:
            cm = confusion_matrix(all_labels, all_preds)
            plt.figure(figsize=(8, 6))
            sns.heatmap(
                cm, 
                annot=True, 
                fmt="d", 
                cmap="Blues",
                xticklabels=[id2label[idx] for idx in range(len(id2label))],
                yticklabels=[id2label[idx] for idx in range(len(id2label))]
            )
            plt.xlabel("é¢„æµ‹ç±»åˆ«")
            plt.ylabel("çœŸå®ç±»åˆ«")
            plt.title("æµ‹è¯•é›†æ··æ·†çŸ©é˜µ")
            plt.savefig(os.path.join(Config.SAVE_MODEL_DIR, "confusion_matrix_180s_1024.png"), dpi=300, bbox_inches="tight")
            plt.close()
            print(f"\n3. æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³ï¼š{os.path.join(Config.SAVE_MODEL_DIR, 'confusion_matrix.png')}")
    
    return avg_loss, accuracy, precision, recall, f1


# ===================== 5. ä¸»å‡½æ•°ï¼ˆä¸²è”è®­ç»ƒ+éªŒè¯+æµ‹è¯•ï¼‰ =====================
def main():
    # Step 1ï¼šåŠ è½½æ•°æ®ï¼ˆä»…NPYæ–‡ä»¶ï¼‰
    train_loader, val_loader, test_loader, label2id, id2label, num_classes = load_all_data(Config.FEAT_ROOT)
    
    # Step 2ï¼šåˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
    model = MLPClassifier(
        input_dim=768,  # Wav2Vec2å›ºå®šè¾“å‡ºç»´åº¦
        num_classes=num_classes,
        dropout=Config.DROPOUT
    ).to(Config.DEVICE)
    
    # æŸå¤±å‡½æ•°ï¼ˆäº¤å‰ç†µï¼Œé€‚é…å¤šåˆ†ç±»ï¼‰
    criterion = nn.CrossEntropyLoss()
    # ä¼˜åŒ–å™¨ï¼ˆä»…ä¼˜åŒ–MLPå‚æ•°ï¼Œæ— é¢„è®­ç»ƒæ¨¡å‹å‚æ•°ï¼‰
    optimizer = optim.AdamW(
        model.parameters(),
        lr=Config.LEARNING_RATE,
        weight_decay=Config.WEIGHT_DECAY
    )
    # æ··åˆç²¾åº¦è®­ç»ƒå™¨
    scaler = GradScaler()
    
    # è®°å½•æœ€ä¼˜æ¨¡å‹ï¼ˆåŸºäºéªŒè¯é›†F1ï¼‰
    best_val_f1 = 0.0
    print(f"\nğŸ“Œ å¼€å§‹MLPåˆ†ç±»è®­ç»ƒï¼ˆ{Config.EPOCHS}è½®ï¼Œè®¾å¤‡ï¼š{Config.DEVICE}ï¼‰")
    
    # Step 3ï¼šè®­ç»ƒå¾ªç¯
    for epoch in range(1, Config.EPOCHS + 1):
        print(f"\n===== Epoch {epoch}/{Config.EPOCHS} =====")
        
        # è®­ç»ƒä¸€è½®
        train_loss, train_acc, train_prec, train_rec, train_f1 = train_one_epoch(
            model, train_loader, criterion, optimizer, scaler
        )
        
        # éªŒè¯ä¸€è½®
        val_loss, val_acc, val_prec, val_rec, val_f1 = evaluate_model(
            model, val_loader, criterion, is_test=False
        )
        
        # æ‰“å°è®­ç»ƒ/éªŒè¯æ—¥å¿—
        print(f"ğŸ“Š è®­ç»ƒé›†ï¼š")
        print(f"   æŸå¤±ï¼š{train_loss:.4f} | å‡†ç¡®ç‡ï¼š{train_acc:.4f} | F1ï¼š{train_f1:.4f}")
        print(f"ğŸ“Š éªŒè¯é›†ï¼š")
        print(f"   æŸå¤±ï¼š{val_loss:.4f} | å‡†ç¡®ç‡ï¼š{val_acc:.4f} | F1ï¼š{val_f1:.4f}")
        
        # ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆåŸºäºéªŒè¯é›†F1ï¼‰
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            save_path = os.path.join(Config.SAVE_MODEL_DIR, "best_model.pth")
            torch.save(model.state_dict(), save_path)
            print(f"âœ… ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆéªŒè¯é›†F1ï¼š{best_val_f1:.4f}ï¼‰è‡³ {save_path}")
    
    # Step 4ï¼šæµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°ï¼ˆåŠ è½½æœ€ä¼˜æ¨¡å‹ï¼‰
    print(f"\n========== å¼€å§‹æµ‹è¯•é›†è¯„ä¼°ï¼ˆåŠ è½½æœ€ä¼˜æ¨¡å‹ï¼‰ ==========")
    best_model = MLPClassifier(input_dim=768, num_classes=num_classes).to(Config.DEVICE)
    best_model.load_state_dict(torch.load(os.path.join(Config.SAVE_MODEL_DIR, "best_model.pth")))
    
    # æµ‹è¯•è¯„ä¼°ï¼ˆè¾“å‡ºè¯¦ç»†æŒ‡æ ‡ï¼‰
    evaluate_model(best_model, test_loader, criterion, id2label=id2label, is_test=True)
    
    print(f"\nğŸ‰ MLPåˆ†ç±»ä»»åŠ¡å®Œæˆï¼æœ€ä¼˜æ¨¡å‹ä¿å­˜è·¯å¾„ï¼š{Config.SAVE_MODEL_DIR}")


if __name__ == "__main__":
    main()