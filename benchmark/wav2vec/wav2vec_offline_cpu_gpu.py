import os
import warnings
import numpy as np
import pandas as pd
import librosa
import torch
import threading
from queue import Queue, Empty
import concurrent.futures
from tqdm import tqdm
from transformers import Wav2Vec2Processor, Wav2Vec2Model
from sklearn.model_selection import train_test_split


# ===================== 1. é…ç½®å‚æ•°ï¼ˆæ ¸å¿ƒå‚æ•°å¯æŒ‰éœ€è°ƒæ•´ï¼‰ =====================
class Config:
    # æ•°æ®ä¸æ¨¡å‹è·¯å¾„
    DATA_ROOT = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/fresh_datasets/EDAC"
    MODEL_PATH = "/mnt/data/test1/repo/wav2vec_2/model"
    SAVE_FEAT_DIR = "./wav2vec2_parallel_features"  # ç‰¹å¾ä¿å­˜ç›®å½•
    
    # éŸ³é¢‘æ ¸å¿ƒå‚æ•°ï¼ˆä¸ä¹‹å‰ä¿æŒä¸€è‡´ï¼‰
    SAMPLE_RATE = 16000
    WINDOW_SIZE = 1024  # å•ä¸ªçª—å£é‡‡æ ·ç‚¹ï¼ˆ64msï¼‰
    MAX_AUDIO_DURATION = 20  # æœ€å¤§éŸ³é¢‘æ—¶é•¿ï¼ˆ20ç§’ï¼Œè¦†ç›–ç»å¤§å¤šæ•°éŸ³é¢‘ï¼‰
    MAX_AUDIO_SAMPLES = SAMPLE_RATE * MAX_AUDIO_DURATION  # 16000*20=320000é‡‡æ ·ç‚¹
    
    # å¹¶è¡Œä¸æ‰¹é‡å‚æ•°ï¼ˆå…³é”®æé€Ÿé…ç½®ï¼‰
    CPU_THREADS = 128  # CPUåˆ†çª—çº¿ç¨‹æ•°ï¼ˆåŒ¹é…ç”¨æˆ·128çº¿ç¨‹éœ€æ±‚ï¼‰
    WINDOW_BATCH_SIZE = 32  # GPUä¸€æ¬¡æ¨ç†çš„çª—å£æ•°é‡ï¼ˆ32/64ï¼Œæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰
    QUEUE_MAX_SIZE = 100  # é˜Ÿåˆ—æœ€å¤§å®¹é‡ï¼ˆé¿å…CPUåˆ†çª—è¿‡å¿«å æ»¡å†…å­˜ï¼‰
    GPU_DEVICE = "cuda:3"  # ç”¨æˆ·æŒ‡å®šçš„GPUè®¾å¤‡ï¼ˆå¦‚cuda:0ã€cuda:2ï¼‰
    
    # å…¶ä»–é…ç½®
    TIMEOUT = 10  # é˜Ÿåˆ—å–æ•°æ®è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    DTYPE = np.float32  # ç‰¹å¾æ•°æ®ç±»å‹ï¼ˆèŠ‚çœå†…å­˜ï¼‰


# åˆ›å»ºä¿å­˜ç›®å½•
os.makedirs(Config.SAVE_FEAT_DIR, exist_ok=True)
# è¿‡æ»¤å†—ä½™è­¦å‘Š
warnings.filterwarnings("ignore", message="PySoundFile failed. Trying audioread instead.")
warnings.filterwarnings("ignore", category=FutureWarning, message="librosa.core.audio.__audioread_load")


# ===================== 2. åŸºç¡€å·¥å…·å‡½æ•°ï¼ˆéŸ³é¢‘åŠ è½½ã€åˆ†çª—ã€å…¨å±€å‚æ•°è®¡ç®—ï¼‰ =====================
def load_single_audio(file_path):
    """åŠ è½½å•éŸ³é¢‘ï¼š16kHzé‡‡æ ·ç‡+20ç§’æˆªæ–­ï¼Œè¿”å›éŸ³é¢‘æ•°ç»„æˆ–Noneï¼ˆæŸåæ–‡ä»¶ï¼‰"""
    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            audio, _ = librosa.load(file_path, sr=Config.SAMPLE_RATE)
        # æˆªæ–­è¶…é•¿éŸ³é¢‘
        if len(audio) > Config.MAX_AUDIO_SAMPLES:
            audio = audio[:Config.MAX_AUDIO_SAMPLES]
        return audio
    except Exception as e:
        print(f"âš ï¸ è·³è¿‡æŸåéŸ³é¢‘ï¼š{file_path} | é”™è¯¯ï¼š{str(e)[:50]}")
        return None


def calculate_global_window_params(all_audio_paths):
    """å¤šçº¿ç¨‹è®¡ç®—å…¨å±€çª—å£å‚æ•°ï¼ˆ95åˆ†ä½æ•°å®šçª—å£æ•°ï¼‰"""
    print(f"\nğŸ“Š è®¡ç®—éŸ³é¢‘é•¿åº¦åˆ†å¸ƒï¼ˆ{len(all_audio_paths)}ä¸ªæ–‡ä»¶ï¼Œ{Config.MAX_AUDIO_DURATION}ç§’æˆªæ–­ï¼‰")
    
    def process_audio_length(file_path):
        audio = load_single_audio(file_path)
        return len(audio) if audio is not None else 0
    
    # 128çº¿ç¨‹å¹¶è¡Œè®¡ç®—é•¿åº¦
    audio_lengths = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=Config.CPU_THREADS) as executor:
        futures = [executor.submit(process_audio_length, path) for path in all_audio_paths]
        for future in tqdm(futures, total=len(all_audio_paths), desc="è®¡ç®—éŸ³é¢‘é•¿åº¦"):
            length = future.result()
            if length > 0:
                audio_lengths.append(length)
    
    if not audio_lengths:
        raise ValueError(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆéŸ³é¢‘æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥{Config.DATA_ROOT}")
    
    # è®¡ç®—95åˆ†ä½æ•°ä¸çª—å£æ•°
    percentile_95 = np.percentile(audio_lengths, 95)
    window_count = int(np.ceil(percentile_95 / Config.WINDOW_SIZE))  # æ¯ä¸ªéŸ³é¢‘çš„å›ºå®šçª—å£æ•°
    total_window_samples = window_count * Config.WINDOW_SIZE  # æ¯ä¸ªéŸ³é¢‘çš„æ€»é‡‡æ ·ç‚¹
    print(f"\nâœ… å…¨å±€çª—å£å‚æ•°ï¼š")
    print(f"   - 95åˆ†ä½æ•°é•¿åº¦ï¼š{percentile_95:.0f}é‡‡æ ·ç‚¹ï¼ˆ{percentile_95/Config.SAMPLE_RATE:.2f}ç§’ï¼‰")
    print(f"   - å•éŸ³é¢‘çª—å£æ•°ï¼š{window_count}ä¸ª | å•çª—å£æ—¶é•¿ï¼š{Config.WINDOW_SIZE/Config.SAMPLE_RATE*1000:.1f}ms")
    print(f"   - å•éŸ³é¢‘æ€»é‡‡æ ·ç‚¹ï¼š{total_window_samples}ï¼ˆ{total_window_samples/Config.SAMPLE_RATE:.2f}ç§’ï¼‰")
    return window_count, total_window_samples


def split_audio_to_windows(audio, window_count, total_window_samples):
    """å°†å•éŸ³é¢‘åˆ†çª—ï¼šè¡¥é›¶/æˆªæ–­åˆ°å›ºå®šçª—å£æ•°ï¼Œè¿”å›çª—å£æ•°ç»„ï¼ˆshape: [window_count, WINDOW_SIZE]ï¼‰"""
    if audio is None or len(audio) == 0:
        return np.zeros((window_count, Config.WINDOW_SIZE), dtype=Config.DTYPE)
    # è¡¥é›¶/æˆªæ–­åˆ°æ€»é‡‡æ ·ç‚¹
    if len(audio) < total_window_samples:
        audio = np.pad(audio, (0, total_window_samples - len(audio)), mode="constant")
    else:
        audio = audio[:total_window_samples]
    # åˆ†çª—
    windows = np.array([
        audio[i*Config.WINDOW_SIZE : (i+1)*Config.WINDOW_SIZE]
        for i in range(window_count)
    ], dtype=Config.DTYPE)
    return windows


def collect_audio_info(data_root):
    """æ”¶é›†æ‰€æœ‰éŸ³é¢‘çš„è·¯å¾„ã€æ ‡ç­¾ã€IDï¼Œè¿”å›DataFrameï¼ˆIDç”¨äºåç»­ç‰¹å¾å¯¹é½ï¼‰"""
    audio_info = []
    class_folders = [f for f in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, f))]
    if not class_folders:
        raise ValueError(f"âŒ æœªæ‰¾åˆ°ç±»åˆ«æ–‡ä»¶å¤¹ï¼š{data_root}")
    
    print(f"\nğŸ“ å‘ç° {len(class_folders)} ä¸ªç±»åˆ«ï¼š{class_folders}")
    audio_id = 0  # å”¯ä¸€éŸ³é¢‘IDï¼Œç”¨äºç‰¹å¾å¯¹é½
    for class_name in class_folders:
        class_dir = os.path.join(data_root, class_name)
        wav_files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if f.endswith(".wav")]
        if not wav_files:
            print(f"âš ï¸ ç±»åˆ« {class_name} æ— WAVæ–‡ä»¶ï¼Œè·³è¿‡")
            continue
        for file_path in wav_files:
            audio_info.append({
                "audio_id": audio_id,
                "path": file_path,
                "label": class_name
            })
            audio_id += 1
    
    df = pd.DataFrame(audio_info)
    if len(df) == 0:
        raise ValueError("âŒ æœªæ”¶é›†åˆ°æœ‰æ•ˆWAVæ–‡ä»¶")
    print(f"âœ… å…±æ”¶é›† {len(df)} ä¸ªéŸ³é¢‘ | ç±»åˆ«åˆ†å¸ƒï¼š\n{df['label'].value_counts()}")
    return df


# ===================== 3. ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ ¸å¿ƒé€»è¾‘ï¼ˆCPUåˆ†çª—+GPUæ¨ç†å¹¶è¡Œï¼‰ =====================
def audio_producer(audio_df, window_count, total_window_samples, queue, audio_feature_dict, lock):
    """
    CPUç”Ÿäº§è€…çº¿ç¨‹ï¼šåŠ è½½éŸ³é¢‘â†’åˆ†çª—â†’æŒ‰æ‰¹æ¬¡æ‰“åŒ…â†’æ”¾å…¥é˜Ÿåˆ—
    :param audio_df: å•ä¸ªéŸ³é¢‘çš„ä¿¡æ¯ï¼ˆaudio_id, path, labelï¼‰
    :param window_count: å•éŸ³é¢‘å›ºå®šçª—å£æ•°
    :param total_window_samples: å•éŸ³é¢‘æ€»é‡‡æ ·ç‚¹
    :param queue: CPUâ†’GPUçš„ç¼“å†²é˜Ÿåˆ—
    :param audio_feature_dict: è®°å½•éŸ³é¢‘ç‰¹å¾çš„å­—å…¸ï¼ˆaudio_id: {"window_feats": [], "window_count": window_count}ï¼‰
    :param lock: æ“ä½œaudio_feature_dictçš„é”ï¼ˆé¿å…å¤šçº¿ç¨‹å†²çªï¼‰
    """
    audio_id = audio_df["audio_id"]
    file_path = audio_df["path"]
    label = audio_df["label"]
    
    # 1. åŠ è½½éŸ³é¢‘å¹¶åˆ†çª—
    audio = load_single_audio(file_path)
    windows = split_audio_to_windows(audio, window_count, total_window_samples)  # [window_count, 1024]
    
    # 2. åˆå§‹åŒ–å½“å‰éŸ³é¢‘çš„ç‰¹å¾è®°å½•ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    with lock:
        audio_feature_dict[audio_id] = {
            "label": label,
            "window_count": window_count,
            "window_feats": [],  # å­˜å‚¨è¯¥éŸ³é¢‘çš„æ‰€æœ‰çª—å£ç‰¹å¾
            "is_complete": False  # æ ‡è®°çª—å£ç‰¹å¾æ˜¯å¦å…¨éƒ¨æ¥æ”¶
        }
    
    # 3. æŒ‰æ‰¹æ¬¡å¤§å°æ‹†åˆ†çª—å£ï¼Œæ”¾å…¥é˜Ÿåˆ—ï¼ˆæ ¼å¼ï¼š(audio_id, çª—å£æ‰¹æ¬¡)ï¼‰
    for i in range(0, window_count, Config.WINDOW_BATCH_SIZE):
        batch_windows = windows[i:i+Config.WINDOW_BATCH_SIZE]  # [batch_size, 1024]
        # ç­‰å¾…é˜Ÿåˆ—æœ‰ç©ºé—´ï¼ˆé¿å…å†…å­˜çˆ†ç‚¸ï¼‰
        while queue.qsize() >= Config.QUEUE_MAX_SIZE:
            threading.Event().wait(0.1)  # æ¯0.1ç§’æ£€æŸ¥ä¸€æ¬¡é˜Ÿåˆ—
        # æ”¾å…¥é˜Ÿåˆ—
        queue.put((audio_id, batch_windows))
    
    # 4. æ ‡è®°å½“å‰éŸ³é¢‘çš„çª—å£å·²å…¨éƒ¨æ”¾å…¥é˜Ÿåˆ—ï¼ˆä¾›æ¶ˆè´¹è€…åˆ¤æ–­æ˜¯å¦å®Œæˆï¼‰
    with lock:
        audio_feature_dict[audio_id]["is_complete"] = True


def gpu_consumer(queue, audio_feature_dict, lock, processor, model, device):
    """
    GPUæ¶ˆè´¹è€…çº¿ç¨‹ï¼šä»é˜Ÿåˆ—å–çª—å£æ‰¹æ¬¡â†’æ‰¹é‡æ¨ç†â†’ä¿å­˜çª—å£ç‰¹å¾
    :param queue: CPUâ†’GPUçš„ç¼“å†²é˜Ÿåˆ—
    :param audio_feature_dict: éŸ³é¢‘ç‰¹å¾è®°å½•å­—å…¸
    :param lock: çº¿ç¨‹é”
    :param processor: Wav2Vec2å¤„ç†å™¨
    :param model: Wav2Vec2æ¨¡å‹
    :param device: GPUè®¾å¤‡
    """
    model.eval()
    with torch.no_grad(), torch.cuda.amp.autocast():  # æ··åˆç²¾åº¦æ¨ç†ï¼Œæé€Ÿ+çœæ˜¾å­˜
        while True:
            try:
                # ä»é˜Ÿåˆ—å–æ‰¹æ¬¡ï¼ˆè¶…æ—¶10ç§’ï¼Œè‹¥é˜Ÿåˆ—ç©ºä¸”æ‰€æœ‰ç”Ÿäº§è€…é€€å‡ºï¼Œåˆ™ç»“æŸï¼‰
                audio_id, batch_windows = queue.get(timeout=Config.TIMEOUT)
                
                # 1. æ‰¹é‡é¢„å¤„ç†ï¼šçª—å£æ‰¹æ¬¡â†’æ¨¡å‹è¾“å…¥
                inputs = processor(
                    batch_windows.tolist(),  # processoræ”¯æŒåˆ—è¡¨è¾“å…¥ï¼Œè‡ªåŠ¨æ‹¼batch
                    sampling_rate=Config.SAMPLE_RATE,
                    return_tensors="pt",
                    padding=False  # çª—å£éƒ½æ˜¯1024ï¼Œæ— éœ€padding
                )["input_values"].to(device)  # [batch_size, 1024]
                
                # 2. æ‰¹é‡æ¨ç†ï¼ˆä¸€æ¬¡å¤„ç†Config.WINDOW_BATCH_SIZEä¸ªçª—å£ï¼‰
                outputs = model(input_values=inputs)  # [batch_size, seq_len, 768]
                # æ—¶åºç»´åº¦æ± åŒ–ï¼ˆç¬¬ä¸€æ¬¡æ± åŒ–ï¼šçª—å£å†…æ—¶åºç‰¹å¾â†’çª—å£çº§ç‰¹å¾ï¼‰
                batch_window_feats = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()  # [batch_size, 768]
                
                # 3. ä¿å­˜å½“å‰æ‰¹æ¬¡çš„çª—å£ç‰¹å¾åˆ°éŸ³é¢‘å­—å…¸ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
                with lock:
                    if audio_id in audio_feature_dict:  # é˜²æ­¢éŸ³é¢‘å·²è¢«è·³è¿‡
                        audio_feature_dict[audio_id]["window_feats"].append(batch_window_feats)
                
                # æ ‡è®°é˜Ÿåˆ—ä»»åŠ¡å®Œæˆ
                queue.task_done()
            
            except Empty:
                # æ£€æŸ¥æ‰€æœ‰ç”Ÿäº§è€…æ˜¯å¦å·²å®Œæˆï¼Œä¸”é˜Ÿåˆ—ç©ºâ†’é€€å‡ºæ¶ˆè´¹è€…
                all_producers_done = True
                with lock:
                    for audio_info in audio_feature_dict.values():
                        # è‹¥å­˜åœ¨éŸ³é¢‘æœªæ ‡è®°ä¸ºâ€œçª—å£å…¨éƒ¨æ”¾å…¥é˜Ÿåˆ—â€ï¼Œåˆ™ç”Ÿäº§è€…ä»åœ¨å·¥ä½œ
                        if not audio_info["is_complete"]:
                            all_producers_done = False
                            break
                if all_producers_done and queue.empty():
                    print(f"\nâœ… GPUæ¶ˆè´¹è€…ï¼šæ‰€æœ‰çª—å£æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œé€€å‡ºæ¨ç†çº¿ç¨‹")
                    break
            except Exception as e:
                print(f"âš ï¸ GPUæ¨ç†å¼‚å¸¸ï¼š{str(e)[:100]} | è·³è¿‡å½“å‰æ‰¹æ¬¡")
                queue.task_done()  # é¿å…é˜Ÿåˆ—é˜»å¡


# ===================== 4. ä¸»æµç¨‹ï¼ˆä¸²è”æ‰€æœ‰æ¨¡å—ï¼Œæ‰§è¡Œç‰¹å¾æå–ï¼‰ =====================
def main():
    # Step 1ï¼šæ”¶é›†éŸ³é¢‘ä¿¡æ¯ï¼ˆIDã€è·¯å¾„ã€æ ‡ç­¾ï¼‰
    audio_df = collect_audio_info(Config.DATA_ROOT)
    all_audio_ids = audio_df["audio_id"].tolist()
    label2id = {cls: idx for idx, cls in enumerate(audio_df["label"].unique())}  # ç±»åˆ«â†’IDæ˜ å°„
    
    # Step 2ï¼šè®¡ç®—å…¨å±€çª—å£å‚æ•°ï¼ˆæ‰€æœ‰éŸ³é¢‘ç»Ÿä¸€çª—å£æ•°ï¼‰
    window_count, total_window_samples = calculate_global_window_params(audio_df["path"].tolist())
    
    # Step 3ï¼šåˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ï¼ˆé˜Ÿåˆ—ã€ç‰¹å¾å­—å…¸ã€çº¿ç¨‹é”ï¼‰
    # é˜Ÿåˆ—ï¼šå­˜å‚¨ (audio_id, çª—å£æ‰¹æ¬¡)ï¼ŒCPUâ†’GPUç¼“å†²
    queue = Queue(maxsize=Config.QUEUE_MAX_SIZE)
    # å­—å…¸ï¼šè®°å½•æ¯ä¸ªéŸ³é¢‘çš„ç‰¹å¾ã€æ ‡ç­¾ã€çª—å£çŠ¶æ€ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    audio_feature_dict = {}
    # é”ï¼šä¿æŠ¤audio_feature_dictçš„å¤šçº¿ç¨‹æ“ä½œ
    lock = threading.Lock()
    
    # Step 4ï¼šåŠ è½½Wav2Vec2æ¨¡å‹å’Œå¤„ç†å™¨ï¼ˆGPUè®¾å¤‡ï¼‰
    device = torch.device(Config.GPU_DEVICE if torch.cuda.is_available() else "cpu")
    if not torch.cuda.is_available():
        raise RuntimeError("âŒ æœªæ£€æµ‹åˆ°GPUï¼Œè¯·ç¡®ä¿CUDAå¯ç”¨")
    print(f"\nğŸ”§ åŠ è½½Wav2Vec2æ¨¡å‹ï¼ˆè®¾å¤‡ï¼š{device} | æ‰¹é‡æ¨ç†å¤§å°ï¼š{Config.WINDOW_BATCH_SIZE}ï¼‰")
    processor = Wav2Vec2Processor.from_pretrained(Config.MODEL_PATH)
    model = Wav2Vec2Model.from_pretrained(Config.MODEL_PATH).to(device)
    
    # Step 5ï¼šå¯åŠ¨GPUæ¶ˆè´¹è€…çº¿ç¨‹ï¼ˆå®ˆæŠ¤çº¿ç¨‹ï¼Œä¸»çº¿ç¨‹é€€å‡ºæ—¶è‡ªåŠ¨å…³é—­ï¼‰
    gpu_thread = threading.Thread(
        target=gpu_consumer,
        args=(queue, audio_feature_dict, lock, processor, model, device),
        daemon=True
    )
    gpu_thread.start()
    print(f"âœ… GPUæ¶ˆè´¹è€…çº¿ç¨‹å¯åŠ¨ï¼ˆçº¿ç¨‹IDï¼š{gpu_thread.ident}ï¼‰")
    
    # Step 6ï¼šå¯åŠ¨CPUå¤šçº¿ç¨‹ç”Ÿäº§è€…ï¼ˆ128çº¿ç¨‹å¹¶è¡ŒåŠ è½½+åˆ†çª—ï¼‰
    print(f"\nğŸš€ å¯åŠ¨ {Config.CPU_THREADS} ä¸ªCPUåˆ†çª—çº¿ç¨‹...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=Config.CPU_THREADS) as executor:
        # æ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€è¡ŒéŸ³é¢‘ä¿¡æ¯
        futures = [executor.submit(
            audio_producer,
            audio_df.iloc[i],  # å•ä¸ªéŸ³é¢‘çš„ä¿¡æ¯
            window_count,
            total_window_samples,
            queue,
            audio_feature_dict,
            lock
        ) for i in range(len(audio_df))]
        
        # è¿›åº¦æ¡è·Ÿè¸ªæ‰€æœ‰ç”Ÿäº§è€…ä»»åŠ¡å®Œæˆæƒ…å†µ
        for _ in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc="CPUåˆ†çª—è¿›åº¦"):
            pass
    
    # Step 7ï¼šç­‰å¾…é˜Ÿåˆ—ä¸­æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œä¸”GPUçº¿ç¨‹é€€å‡º
    print(f"\nâ³ ç­‰å¾…GPUå¤„ç†å‰©ä½™çª—å£æ‰¹æ¬¡...")
    queue.join()  # ç­‰å¾…é˜Ÿåˆ—ä¸­æ‰€æœ‰ä»»åŠ¡æ ‡è®°ä¸ºdone
    gpu_thread.join(timeout=Config.TIMEOUT)  # ç­‰å¾…GPUçº¿ç¨‹é€€å‡º
    
    # Step 8ï¼šèšåˆçª—å£ç‰¹å¾â†’éŸ³é¢‘çº§ç‰¹å¾ï¼ˆç¬¬äºŒæ¬¡æ± åŒ–ï¼‰
    print(f"\nğŸ“Š èšåˆçª—å£ç‰¹å¾ï¼Œç”Ÿæˆæœ€ç»ˆéŸ³é¢‘çº§ç‰¹å¾ï¼ˆå…± {len(audio_feature_dict)} ä¸ªéŸ³é¢‘ï¼‰")
    all_features = []
    all_labels = []
    missing_audio_ids = []
    
    for audio_id in tqdm(all_audio_ids, desc="èšåˆç‰¹å¾"):
        if audio_id not in audio_feature_dict:
            missing_audio_ids.append(audio_id)
            continue
        
        audio_info = audio_feature_dict[audio_id]
        # æ£€æŸ¥çª—å£ç‰¹å¾æ˜¯å¦å®Œæ•´ï¼ˆé¿å…æ¼æ‰¹ï¼‰
        if not audio_info["is_complete"] or len(audio_info["window_feats"]) == 0:
            missing_audio_ids.append(audio_id)
            continue
        
        # æ‹¼æ¥æ‰€æœ‰çª—å£ç‰¹å¾â†’ç¬¬äºŒæ¬¡æ± åŒ–ï¼ˆçª—å£ç»´åº¦â†’éŸ³é¢‘çº§ç‰¹å¾ï¼‰
        window_feats = np.concatenate(audio_info["window_feats"], axis=0)  # [window_count, 768]
        audio_feat = np.mean(window_feats, axis=0)  # [768]ï¼ˆç¬¬äºŒæ¬¡æ± åŒ–ï¼‰
        
        # ä¿å­˜ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆæ ‡ç­¾è½¬IDï¼‰
        all_features.append(audio_feat)
        all_labels.append(label2id[audio_info["label"]])
    
    # Step 9ï¼šå¤„ç†ç¼ºå¤±éŸ³é¢‘ï¼ˆå¯é€‰ï¼‰
    if missing_audio_ids:
        print(f"âš ï¸ å…± {len(missing_audio_ids)} ä¸ªéŸ³é¢‘ç‰¹å¾æå–å¤±è´¥ï¼ˆæŸåæˆ–è¶…æ—¶ï¼‰ï¼Œå·²è·³è¿‡")
    
    # Step 10ï¼šè½¬ä¸ºnumpyæ•°ç»„å¹¶åˆ†å±‚åˆ’åˆ†æ•°æ®é›†
    all_features = np.array(all_features, dtype=Config.DTYPE)  # [N, 768]
    all_labels = np.array(all_labels, dtype=np.int64)        # [N]
    print(f"\nâœ… ç‰¹å¾èšåˆå®Œæˆï¼š")
    print(f"   - æœ€ç»ˆæœ‰æ•ˆéŸ³é¢‘æ•°ï¼š{len(all_features)} | ç‰¹å¾ç»´åº¦ï¼š{all_features.shape}")
    print(f"   - æ ‡ç­¾ç»´åº¦ï¼š{all_labels.shape} | ç±»åˆ«æ˜ å°„ï¼š{label2id}")
    
    # åˆ†å±‚åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ï¼ˆ7:0.15:0.15ï¼‰
    train_feat, temp_feat, train_label, temp_label = train_test_split(
        all_features, all_labels, test_size=0.3, stratify=all_labels, random_state=42
    )
    val_feat, test_feat, val_label, test_label = train_test_split(
        temp_feat, temp_label, test_size=0.5, stratify=temp_label, random_state=42
    )
    
    # Step 11ï¼šä¿å­˜ç‰¹å¾å’Œæ ‡ç­¾
    save_paths = {
        "train_feat": os.path.join(Config.SAVE_FEAT_DIR, "train_feat.npy"),
        "train_label": os.path.join(Config.SAVE_FEAT_DIR, "train_label.npy"),
        "val_feat": os.path.join(Config.SAVE_FEAT_DIR, "val_feat.npy"),
        "val_label": os.path.join(Config.SAVE_FEAT_DIR, "val_label.npy"),
        "test_feat": os.path.join(Config.SAVE_FEAT_DIR, "test_feat.npy"),
        "test_label": os.path.join(Config.SAVE_FEAT_DIR, "test_label.npy"),
        "label2id": os.path.join(Config.SAVE_FEAT_DIR, "label2id.npy")
    }
    # ä¿å­˜æ–‡ä»¶
    np.save(save_paths["train_feat"], train_feat)
    np.save(save_paths["train_label"], train_label)
    np.save(save_paths["val_feat"], val_feat)
    np.save(save_paths["val_label"], val_label)
    np.save(save_paths["test_feat"], test_feat)
    np.save(save_paths["test_label"], test_label)
    np.save(save_paths["label2id"], label2id, allow_pickle=True)
    
    # æ‰“å°åˆ’åˆ†ç»“æœ
    print(f"\nğŸ“ˆ æ•°æ®é›†åˆ†å±‚åˆ’åˆ†ç»“æœï¼š")
    print(f"   - è®­ç»ƒé›†ï¼š{len(train_feat)} æ ·æœ¬ï¼ˆ{len(train_feat)/len(all_features)*100:.1f}%ï¼‰")
    print(f"   - éªŒè¯é›†ï¼š{len(val_feat)} æ ·æœ¬ï¼ˆ{len(val_feat)/len(all_features)*100:.1f}%ï¼‰")
    print(f"   - æµ‹è¯•é›†ï¼š{len(test_feat)} æ ·æœ¬ï¼ˆ{len(test_feat)/len(all_features)*100:.1f}%ï¼‰")
    print(f"\nğŸ’¾ ç‰¹å¾ä¿å­˜è·¯å¾„ï¼š{Config.SAVE_FEAT_DIR}")
    print(f"ğŸ‰ ç¦»çº¿ç‰¹å¾æå–å…¨æµç¨‹å®Œæˆï¼")


if __name__ == "__main__":
    main()