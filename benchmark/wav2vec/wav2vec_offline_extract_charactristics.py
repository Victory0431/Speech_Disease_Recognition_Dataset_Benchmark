import os
import warnings
import numpy as np
import pandas as pd
import librosa
import torch
import concurrent.futures
from tqdm import tqdm
from transformers import Wav2Vec2Processor, Wav2Vec2Model
from sklearn.model_selection import train_test_split


# ===================== 1. é…ç½®å‚æ•°ï¼ˆä¸ä¹‹å‰ä¿æŒä¸€è‡´ï¼Œä»…éœ€ç¡®è®¤è·¯å¾„ï¼‰ =====================
class Config:
    # æ•°æ®ä¸æ¨¡å‹è·¯å¾„
    DATA_ROOT = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/fresh_datasets/EDAC"  # EDACæ•°æ®é›†æ ¹ç›®å½•
    MODEL_PATH = "/mnt/data/test1/repo/wav2vec_2/model"  # æœ¬åœ°Wav2Vec2æ¨¡å‹è·¯å¾„
    SAVE_FEAT_DIR = "/mnt/data/test1/Speech_Disease_Recognition_Dataset_Benchmark/wav2vec_characteristics/wav2vec2_offline_features"  # ç‰¹å¾ä¿å­˜ç›®å½•ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
    
    # éŸ³é¢‘é¢„å¤„ç†å‚æ•°ï¼ˆä¸ä¹‹å‰å®Œå…¨ä¸€è‡´ï¼‰
    SAMPLE_RATE = 16000  # Wav2Vec2è¦æ±‚çš„é‡‡æ ·ç‡
    WINDOW_SIZE = 512  # å•ä¸ªçª—å£é‡‡æ ·ç‚¹ï¼ˆ64msï¼Œå¯¹åº”åŸ8kHzçš„512é‡‡æ ·ç‚¹æ—¶é•¿ï¼‰
    MAX_AUDIO_DURATION = 60  # æœ€å¤§éŸ³é¢‘æ—¶é•¿ï¼ˆ3åˆ†é’Ÿ=180ç§’ï¼‰
    MAX_AUDIO_SAMPLES = SAMPLE_RATE * MAX_AUDIO_DURATION  # 3åˆ†é’Ÿå¯¹åº”çš„é‡‡æ ·ç‚¹ï¼ˆ2,880,000ï¼‰
    
    # å¤šçº¿ç¨‹å‚æ•°ï¼ˆè®¡ç®—éŸ³é¢‘é•¿åº¦ç”¨ï¼ŒåŠ å¿«æ•ˆç‡ï¼‰
    MAX_WORKERS = 128  # çº¿ç¨‹æ•°ï¼ˆæ ¹æ®CPUæ ¸å¿ƒè°ƒæ•´ï¼Œ128é€‚åˆå¤šæ ¸å¿ƒæœåŠ¡å™¨ï¼‰


# åˆ›å»ºç‰¹å¾ä¿å­˜ç›®å½•ï¼ˆé¿å…è·¯å¾„ä¸å­˜åœ¨æŠ¥é”™ï¼‰
os.makedirs(Config.SAVE_FEAT_DIR, exist_ok=True)


# ===================== 2. éŸ³é¢‘é¢„å¤„ç†æ ¸å¿ƒå‡½æ•°ï¼ˆåˆ†çª—é€»è¾‘ä¸ä¹‹å‰å®Œå…¨ä¸€è‡´ï¼‰ =====================
def setup_warnings():
    """è¿‡æ»¤æ— å…³è­¦å‘Šï¼ˆé¿å…librosaåŠ è½½éŸ³é¢‘æ—¶çš„å†—ä½™è­¦å‘Šï¼‰"""
    warnings.filterwarnings("ignore", message="PySoundFile failed. Trying audioread instead.")
    warnings.filterwarnings("ignore", category=FutureWarning, message="librosa.core.audio.__audioread_load")


def load_single_audio(file_path):
    """åŠ è½½å•æ¡éŸ³é¢‘ï¼š16kHzé‡‡æ ·ç‡+3åˆ†é’Ÿæˆªæ–­"""
    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            # å¼ºåˆ¶åŠ è½½ä¸º16kHzï¼Œè‹¥åŸå§‹é‡‡æ ·ç‡ä¸åŒè‡ªåŠ¨é‡é‡‡æ ·
            audio, sr = librosa.load(file_path, sr=Config.SAMPLE_RATE)
        
        # æˆªæ–­è¶…è¿‡3åˆ†é’Ÿçš„éƒ¨åˆ†
        if len(audio) > Config.MAX_AUDIO_SAMPLES:
            audio = audio[:Config.MAX_AUDIO_SAMPLES]
        return audio
    except Exception as e:
        print(f"âš ï¸ è·³è¿‡æŸåéŸ³é¢‘ï¼š{file_path} | é”™è¯¯ï¼š{str(e)[:50]}")
        return None


def calculate_global_window_params(all_audio_paths):
    """å¤šçº¿ç¨‹è®¡ç®—å…¨å±€çª—å£å‚æ•°ï¼šåŸºäºæ‰€æœ‰éŸ³é¢‘é•¿åº¦çš„95åˆ†ä½æ•°å®šçª—å£æ•°"""
    print(f"\nğŸ“Š å¤šçº¿ç¨‹è®¡ç®—éŸ³é¢‘é•¿åº¦åˆ†å¸ƒï¼ˆ{len(all_audio_paths)}ä¸ªæ–‡ä»¶ï¼Œ3åˆ†é’Ÿæˆªæ–­ï¼‰")
    
    # å•æ–‡ä»¶é•¿åº¦è®¡ç®—å‡½æ•°ï¼ˆä¾›å¤šçº¿ç¨‹è°ƒç”¨ï¼‰
    def process_audio_length(file_path):
        audio = load_single_audio(file_path)
        return len(audio) if audio is not None else 0
    
    # 128çº¿ç¨‹å¹¶è¡Œè®¡ç®—æ‰€æœ‰éŸ³é¢‘é•¿åº¦
    audio_lengths = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=Config.MAX_WORKERS) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = [executor.submit(process_audio_length, path) for path in all_audio_paths]
        # è¿›åº¦æ¡è·Ÿè¸ª
        for future in tqdm(
            concurrent.futures.as_completed(futures),
            total=len(all_audio_paths),
            desc="è®¡ç®—éŸ³é¢‘é•¿åº¦"
        ):
            length = future.result()
            if length > 0:  # è¿‡æ»¤åŠ è½½å¤±è´¥çš„éŸ³é¢‘ï¼ˆé•¿åº¦ä¸º0ï¼‰
                audio_lengths.append(length)
    
    # å¤„ç†æç«¯æƒ…å†µï¼ˆæ— æœ‰æ•ˆéŸ³é¢‘ï¼‰
    if not audio_lengths:
        raise ValueError(f"âŒ åœ¨ {Config.DATA_ROOT} æœªæ‰¾åˆ°æœ‰æ•ˆWAVæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†è·¯å¾„")
    
    # è®¡ç®—95åˆ†ä½æ•°ï¼Œç¡®å®šå…¨å±€çª—å£æ•°ï¼ˆå‘ä¸Šå–æ•´ç¡®ä¿è¦†ç›–95%éŸ³é¢‘ï¼‰
    percentile_95 = np.percentile(audio_lengths, 95)
    window_count = int(np.ceil(percentile_95 / Config.WINDOW_SIZE))  # çª—å£æ•°=95åˆ†ä½æ•°é•¿åº¦//çª—å£å¤§å°ï¼ˆå‘ä¸Šå–æ•´ï¼‰
    total_window_samples = window_count * Config.WINDOW_SIZE  # å•éŸ³é¢‘æ€»é‡‡æ ·ç‚¹ï¼ˆçª—å£æ•°Ã—çª—å£å¤§å°ï¼‰
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ–¹ä¾¿ç¡®è®¤å‚æ•°åˆç†æ€§ï¼‰
    print(f"\nâœ… éŸ³é¢‘é•¿åº¦ç»Ÿè®¡ç»“æœï¼š")
    print(f"   - 95åˆ†ä½æ•°é•¿åº¦ï¼š{percentile_95:.0f} é‡‡æ ·ç‚¹ï¼ˆ{percentile_95/Config.SAMPLE_RATE:.2f}ç§’ï¼‰")
    print(f"   - å…¨å±€çª—å£æ•°ï¼š{window_count} ä¸ª")
    print(f"   - å•éŸ³é¢‘æ€»é‡‡æ ·ç‚¹ï¼š{total_window_samples}ï¼ˆ{total_window_samples/Config.SAMPLE_RATE:.2f}ç§’ï¼‰")
    print(f"   - å•ä¸ªçª—å£æ—¶é•¿ï¼š{Config.WINDOW_SIZE/Config.SAMPLE_RATE*1000:.1f}æ¯«ç§’")
    
    return window_count, total_window_samples


def split_audio_to_windows(audio, window_count, total_window_samples):
    """å°†å•æ¡éŸ³é¢‘åˆ‡åˆ†ä¸ºå›ºå®šçª—å£ï¼šæˆªæ–­è¶…é•¿éŸ³é¢‘+è¡¥é›¶çŸ­éŸ³é¢‘"""
    # å¤„ç†åŠ è½½å¤±è´¥çš„ç©ºéŸ³é¢‘ï¼ˆè¿”å›å…¨é›¶çª—å£ï¼‰
    if audio is None or len(audio) == 0:
        return np.zeros((window_count, Config.WINDOW_SIZE), dtype=np.float32)
    
    # 1. æˆªæ–­è¶…è¿‡æ€»çª—å£é‡‡æ ·ç‚¹çš„éŸ³é¢‘
    if len(audio) > total_window_samples:
        audio = audio[:total_window_samples]
    # 2. è¡¥é›¶ä¸è¶³æ€»çª—å£é‡‡æ ·ç‚¹çš„éŸ³é¢‘ï¼ˆè¡¥åˆ°total_window_samplesé•¿åº¦ï¼‰
    else:
        audio = np.pad(audio, (0, total_window_samples - len(audio)), mode="constant")
    
    # 3. åˆ‡åˆ†ä¸ºå›ºå®šæ•°é‡çš„çª—å£ï¼ˆshape: [window_count, WINDOW_SIZE]ï¼‰
    windows = np.array([
        audio[i * Config.WINDOW_SIZE : (i + 1) * Config.WINDOW_SIZE]
        for i in range(window_count)
    ], dtype=np.float32)
    
    return windows


# ===================== 3. æ”¶é›†æ•°æ®é›†ä¿¡æ¯ï¼ˆéŸ³é¢‘è·¯å¾„+æ ‡ç­¾ï¼‰ =====================
def collect_audio_info(data_root):
    """éå†EDACæ•°æ®é›†ï¼Œæ”¶é›†æ‰€æœ‰WAVæ–‡ä»¶è·¯å¾„å’Œå¯¹åº”æ ‡ç­¾ï¼ˆç±»åˆ«=æ–‡ä»¶å¤¹åï¼‰"""
    audio_info = []
    # è·å–æ‰€æœ‰ç±»åˆ«æ–‡ä»¶å¤¹ï¼ˆå¦‚Non-Depressionã€Depressionï¼‰
    class_folders = [f for f in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, f))]
    
    if not class_folders:
        raise ValueError(f"âŒ åœ¨ {data_root} æœªæ‰¾åˆ°ç±»åˆ«æ–‡ä»¶å¤¹ï¼Œè¯·ç¡®è®¤æ•°æ®é›†è·¯å¾„æ­£ç¡®")
    
    print(f"\nğŸ“ å‘ç° {len(class_folders)} ä¸ªç±»åˆ«ï¼š{class_folders}")
    
    # éå†æ¯ä¸ªç±»åˆ«æ–‡ä»¶å¤¹ï¼Œæ”¶é›†WAVæ–‡ä»¶
    for class_name in class_folders:
        class_dir = os.path.join(data_root, class_name)
        wav_files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if f.endswith(".wav")]
        
        if not wav_files:
            print(f"âš ï¸ ç±»åˆ« {class_name} ä¸‹æ— WAVæ–‡ä»¶ï¼Œè·³è¿‡")
            continue
        
        # è®°å½•æ¯ä¸ªWAVæ–‡ä»¶çš„è·¯å¾„å’Œæ ‡ç­¾
        for file_path in wav_files:
            audio_info.append({
                "path": file_path,
                "label": class_name
            })
    
    # è½¬ä¸ºDataFrameï¼ˆæ–¹ä¾¿åç»­å¤„ç†å’Œå¯¹é½ï¼‰
    df = pd.DataFrame(audio_info)
    if len(df) == 0:
        raise ValueError("âŒ æœªæ”¶é›†åˆ°ä»»ä½•æœ‰æ•ˆWAVæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æˆ–è·¯å¾„")
    
    print(f"âœ… å…±æ”¶é›† {len(df)} ä¸ªæœ‰æ•ˆéŸ³é¢‘æ–‡ä»¶")
    print(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒï¼š\n{df['label'].value_counts()}")
    return df


# ===================== 4. Wav2Vec2ç‰¹å¾æå–ï¼ˆåŒæ± åŒ–é€»è¾‘ï¼‰ =====================
def extract_wav2vec2_features(df, window_count, total_window_samples):
    """
    ç¦»çº¿æå–æ‰€æœ‰éŸ³é¢‘çš„ç‰¹å¾ï¼š
    æ­¥éª¤ï¼šå•çª—å£â†’Wav2Vec2æç‰¹å¾â†’æ—¶åºç»´åº¦æ± åŒ–â†’çª—å£ç»´åº¦æ± åŒ–â†’è¾“å‡º768ç»´ç‰¹å¾
    """
    # 1. åŠ è½½Wav2Vec2æ¨¡å‹å’Œå¤„ç†å™¨ï¼ˆä¸»è¿›ç¨‹åŠ è½½ï¼Œé¿å…å¤šè¿›ç¨‹åºåˆ—åŒ–é—®é¢˜ï¼‰
    device = torch.device("cuda:2" if torch.cuda.is_available() else "cpu")
    print(f"\nğŸ”§ åŠ è½½Wav2Vec2æ¨¡å‹ï¼ˆè®¾å¤‡ï¼š{device}ï¼‰")
    
    processor = Wav2Vec2Processor.from_pretrained(Config.MODEL_PATH)
    model = Wav2Vec2Model.from_pretrained(Config.MODEL_PATH).to(device)
    model.eval()  # ç‰¹å¾æå–æ¨¡å¼ï¼Œå…³é—­è®­ç»ƒç›¸å…³å±‚ï¼ˆå¦‚Dropoutï¼‰
    
    # 2. æ„å»ºç±»åˆ«â†’IDæ˜ å°„ï¼ˆæ–¹ä¾¿åç»­æ ‡ç­¾å­˜å‚¨ï¼‰
    unique_labels = df["label"].unique()
    label2id = {cls: idx for idx, cls in enumerate(unique_labels)}
    print(f"ğŸ·ï¸ ç±»åˆ«â†’IDæ˜ å°„ï¼š{label2id}")
    
    # 3. é€éŸ³é¢‘æå–ç‰¹å¾ï¼ˆä¸»è¿›ç¨‹æ‰§è¡Œï¼Œç¨³å®šæ— å¤šè¿›ç¨‹é—®é¢˜ï¼‰
    all_features = []  # å­˜å‚¨æ‰€æœ‰éŸ³é¢‘çš„768ç»´ç‰¹å¾
    all_labels = []    # å­˜å‚¨æ‰€æœ‰éŸ³é¢‘çš„æ ‡ç­¾ID
    print(f"\nğŸš€ å¼€å§‹æå–ç‰¹å¾ï¼ˆå…± {len(df)} ä¸ªéŸ³é¢‘ï¼‰")
    
    with torch.no_grad():  # å…³é—­æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœæ˜¾å­˜+åŠ é€Ÿ
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="æå–ç‰¹å¾"):
            file_path = row["path"]
            label = label2id[row["label"]]
            
            # æ­¥éª¤1ï¼šåŠ è½½éŸ³é¢‘å¹¶åˆ†çª—ï¼ˆshape: [window_count, WINDOW_SIZE]ï¼‰
            audio = load_single_audio(file_path)
            windows = split_audio_to_windows(audio, window_count, total_window_samples)
            
            # æ­¥éª¤2ï¼šå•çª—å£ç‰¹å¾æå–+æ—¶åºç»´åº¦æ± åŒ–ï¼ˆç¬¬ä¸€æ¬¡æ± åŒ–ï¼‰
            window_features = []
            for window in windows:
                # éŸ³é¢‘é¢„å¤„ç†ï¼šè½¬ä¸ºWav2Vec2å¯æ¥å—çš„å¼ é‡
                inputs = processor(
                    window,
                    sampling_rate=Config.SAMPLE_RATE,
                    return_tensors="pt",  # è¿”å›PyTorchå¼ é‡
                    padding=False        # æ— éœ€paddingï¼ˆçª—å£å·²å›ºå®š1024é‡‡æ ·ç‚¹ï¼‰
                )["input_values"].to(device)  # shape: [1, WINDOW_SIZE]
                
                # Wav2Vec2æå–ç‰¹å¾ï¼ˆè¾“å‡ºshape: [1, seq_len, 768]ï¼Œseq_lenä¸ºæ—¶åºç‰¹å¾å¸§æ•°é‡ï¼‰
                outputs = model(input_values=inputs)
                # æ—¶åºç»´åº¦æ± åŒ–ï¼šå°†çª—å£å†…çš„æ—¶åºç‰¹å¾å¸§â†’çª—å£çº§ç‰¹å¾ï¼ˆ[1, seq_len, 768] â†’ [1, 768]ï¼‰
                pooled_window_feat = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()
                window_features.append(pooled_window_feat)
            
            # æ­¥éª¤3ï¼šçª—å£ç»´åº¦æ± åŒ–ï¼ˆç¬¬äºŒæ¬¡æ± åŒ–ï¼‰ï¼šæ‰€æœ‰çª—å£â†’æ•´éŸ³é¢‘ç‰¹å¾ï¼ˆ[window_count, 768] â†’ [768]ï¼‰
            audio_feat = np.mean(np.concatenate(window_features, axis=0), axis=0)
            
            # ä¿å­˜å½“å‰éŸ³é¢‘çš„ç‰¹å¾å’Œæ ‡ç­¾
            all_features.append(audio_feat)
            all_labels.append(label)
    
    # è½¬ä¸ºnumpyæ•°ç»„ï¼ˆæ–¹ä¾¿åç»­ä¿å­˜å’ŒåŠ è½½ï¼‰
    all_features = np.array(all_features, dtype=np.float32)  # shape: [N, 768]ï¼ŒNä¸ºæœ‰æ•ˆéŸ³é¢‘æ•°
    all_labels = np.array(all_labels, dtype=np.int64)        # shape: [N]
    
    print(f"\nâœ… ç‰¹å¾æå–å®Œæˆï¼")
    print(f"   - ç‰¹å¾ç»´åº¦ï¼š{all_features.shape}ï¼ˆ{len(all_features)}ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ª768ç»´ï¼‰")
    print(f"   - æ ‡ç­¾ç»´åº¦ï¼š{all_labels.shape}")
    return all_features, all_labels, label2id


# ===================== 5. åˆ†å±‚åˆ’åˆ†æ•°æ®é›†+ä¿å­˜ç‰¹å¾ =====================
def split_and_save_features(all_features, all_labels, label2id):
    """
    åˆ†å±‚åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†ï¼ˆ7:0.15:0.15ï¼‰ï¼Œä¿æŒç±»åˆ«æ¯”ä¾‹ï¼Œä¿å­˜ä¸ºNPYæ–‡ä»¶
    """
    # æ­¥éª¤1ï¼šå…ˆåˆ†è®­ç»ƒé›†ï¼ˆ70%ï¼‰å’Œæš‚å­˜é›†ï¼ˆ30%ï¼‰
    train_feat, temp_feat, train_label, temp_label = train_test_split(
        all_features, all_labels,
        test_size=0.3,          # æš‚å­˜é›†å 30%ï¼ˆåç»­åˆ†éªŒè¯+æµ‹è¯•ï¼‰
        stratify=all_labels,    # åˆ†å±‚åˆ’åˆ†ï¼Œä¿æŒç±»åˆ«æ¯”ä¾‹
        random_state=42         # å›ºå®šéšæœºç§å­ï¼Œç»“æœå¯å¤ç°
    )
    
    # æ­¥éª¤2ï¼šæš‚å­˜é›†åˆ†éªŒè¯é›†ï¼ˆ15%æ€»æ•°æ®ï¼‰å’Œæµ‹è¯•é›†ï¼ˆ15%æ€»æ•°æ®ï¼‰
    val_feat, test_feat, val_label, test_label = train_test_split(
        temp_feat, temp_label,
        test_size=0.5,          # æš‚å­˜é›†å¯¹åŠåˆ†ï¼Œå„å æ€»æ•°æ®çš„15%
        stratify=temp_label,    # åˆ†å±‚åˆ’åˆ†
        random_state=42
    )
    
    # æ­¥éª¤3ï¼šä¿å­˜æ‰€æœ‰æ–‡ä»¶ï¼ˆNPYæ ¼å¼ï¼Œæ”¯æŒå¿«é€ŸåŠ è½½ï¼‰
    save_paths = {
        "train_feat": os.path.join(Config.SAVE_FEAT_DIR, "train_feat.npy"),
        "train_label": os.path.join(Config.SAVE_FEAT_DIR, "train_label.npy"),
        "val_feat": os.path.join(Config.SAVE_FEAT_DIR, "val_feat.npy"),
        "val_label": os.path.join(Config.SAVE_FEAT_DIR, "val_label.npy"),
        "test_feat": os.path.join(Config.SAVE_FEAT_DIR, "test_feat.npy"),
        "test_label": os.path.join(Config.SAVE_FEAT_DIR, "test_label.npy"),
        "label2id": os.path.join(Config.SAVE_FEAT_DIR, "label2id.npy")
    }
    
    # ä¿å­˜ç‰¹å¾å’Œæ ‡ç­¾
    np.save(save_paths["train_feat"], train_feat)
    np.save(save_paths["train_label"], train_label)
    np.save(save_paths["val_feat"], val_feat)
    np.save(save_paths["val_label"], val_label)
    np.save(save_paths["test_feat"], test_feat)
    np.save(save_paths["test_label"], test_label)
    # ä¿å­˜ç±»åˆ«æ˜ å°„ï¼ˆallow_pickle=Trueæ”¯æŒå­—å…¸æ ¼å¼ï¼‰
    np.save(save_paths["label2id"], label2id, allow_pickle=True)
    
    # æ‰“å°åˆ’åˆ†ç»“æœ
    print(f"\nğŸ“ˆ æ•°æ®é›†åˆ†å±‚åˆ’åˆ†ç»“æœï¼š")
    print(f"   - è®­ç»ƒé›†ï¼š{len(train_feat)} æ ·æœ¬ï¼ˆ{len(train_feat)/len(all_features)*100:.1f}%ï¼‰")
    print(f"   - éªŒè¯é›†ï¼š{len(val_feat)} æ ·æœ¬ï¼ˆ{len(val_feat)/len(all_features)*100:.1f}%ï¼‰")
    print(f"   - æµ‹è¯•é›†ï¼š{len(test_feat)} æ ·æœ¬ï¼ˆ{len(test_feat)/len(all_features)*100:.1f}%ï¼‰")
    print(f"\nğŸ’¾ ç‰¹å¾ä¿å­˜è·¯å¾„ï¼š{Config.SAVE_FEAT_DIR}")
    print(f"   åŒ…å«æ–‡ä»¶ï¼štrain_feat.npyã€train_label.npyã€val_feat.npyã€val_label.npyã€test_feat.npyã€test_label.npyã€label2id.npy")


# ===================== 6. ä¸»å‡½æ•°ï¼ˆä¸²è”æ‰€æœ‰æ­¥éª¤ï¼‰ =====================
def main():
    # åˆå§‹åŒ–ï¼šè¿‡æ»¤è­¦å‘Š
    setup_warnings()
    
    # æ­¥éª¤1ï¼šæ”¶é›†éŸ³é¢‘è·¯å¾„å’Œæ ‡ç­¾
    df = collect_audio_info(Config.DATA_ROOT)
    
    # æ­¥éª¤2ï¼šè®¡ç®—å…¨å±€çª—å£å‚æ•°ï¼ˆ95åˆ†ä½æ•°å®šçª—å£æ•°ï¼‰
    window_count, total_window_samples = calculate_global_window_params(df["path"].tolist())
    
    # æ­¥éª¤3ï¼šæå–æ‰€æœ‰éŸ³é¢‘çš„Wav2Vec2ç‰¹å¾ï¼ˆåŒæ± åŒ–ï¼‰
    all_features, all_labels, label2id = extract_wav2vec2_features(df, window_count, total_window_samples)
    
    # æ­¥éª¤4ï¼šåˆ†å±‚åˆ’åˆ†æ•°æ®é›†å¹¶ä¿å­˜ç‰¹å¾
    split_and_save_features(all_features, all_labels, label2id)
    
    print(f"\nğŸ‰ ç¦»çº¿ç‰¹å¾æå–å…¨æµç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥å¯åŠ è½½ç‰¹å¾è®­ç»ƒMLPåˆ†ç±»å¤´")


if __name__ == "__main__":
    main()