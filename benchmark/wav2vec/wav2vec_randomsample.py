import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import GradScaler, autocast
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from imblearn.over_sampling import RandomOverSampler  # æ–°å¢ï¼šè¿‡é‡‡æ ·åº“


# ===================== 1. é…ç½®å‚æ•°ï¼ˆä»…éœ€ç¡®è®¤ç‰¹å¾è·¯å¾„ï¼‰ =====================
class Config:
    # å…³é”®ï¼šé¢„æå–ç‰¹å¾çš„ä¿å­˜è·¯å¾„
    FEAT_ROOT = "/mnt/data/test1/wav2vec2_parallel_features/a180s_512"
    
    # è®­ç»ƒå‚æ•°
    BATCH_SIZE = 64        
    EPOCHS = 50            
    LEARNING_RATE = 1e-4   
    WEIGHT_DECAY = 1e-5    
    DROPOUT = 0.3          
    DEVICE = torch.device("cuda:4" if torch.cuda.is_available() else "cpu")  
    
    # è¯„ä¼°å‚æ•°
    EVAL_METRIC = "weighted"  
    PLOT_CONFUSION_MATRIX = True  
    SAVE_MODEL_DIR = "/mnt/data/test1/wav2vec2_parallel_features/a180s_512222"  


os.makedirs(Config.SAVE_MODEL_DIR, exist_ok=True)


# ===================== 2. æ•°æ®åŠ è½½ï¼ˆæ”¯æŒè¿‡é‡‡æ ·ï¼‰ =====================
class FeatDataset(Dataset):
    """åŠ è½½é¢„æå–çš„ç‰¹å¾å’Œæ ‡ç­¾ï¼Œé€‚é…PyTorch DataLoaderï¼ˆæ”¯æŒç›´æ¥ä¼ å¼ é‡æˆ–ä»æ–‡ä»¶åŠ è½½ï¼‰"""
    def __init__(self, feat=None, label=None, feat_path=None, label_path=None):
        if feat is not None and label is not None:
            self.feats = feat
            self.labels = label
        else:
            self.feats = torch.tensor(np.load(feat_path), dtype=torch.float32)
            self.labels = torch.tensor(np.load(label_path), dtype=torch.long)
        
        assert len(self.feats) == len(self.labels), "ç‰¹å¾ä¸æ ‡ç­¾æ•°é‡ä¸åŒ¹é…ï¼"

    def __len__(self):
        return len(self.feats)

    def __getitem__(self, idx):
        return {"feat": self.feats[idx], "label": self.labels[idx]}


def load_all_data(feat_root):
    """åŠ è½½è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ï¼Œè¿”å›DataLoaderå’Œç±»åˆ«æ˜ å°„"""
    file_paths = {
        "train_feat": os.path.join(feat_root, "train_feat.npy"),
        "train_label": os.path.join(feat_root, "train_label.npy"),
        "val_feat": os.path.join(feat_root, "val_feat.npy"),
        "val_label": os.path.join(feat_root, "val_label.npy"),
        "test_feat": os.path.join(feat_root, "test_feat.npy"),
        "test_label": os.path.join(feat_root, "test_label.npy"),
        "label2id": os.path.join(feat_root, "label2id.npy")
    }
    
    for name, path in file_paths.items():
        if not os.path.exists(path):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æ–‡ä»¶ï¼š{path}")
    
    label2id = np.load(file_paths["label2id"], allow_pickle=True).item()
    id2label = {idx: cls for cls, idx in label2id.items()}
    num_classes = len(label2id)
    print(f"âœ… åŠ è½½ç±»åˆ«æ˜ å°„ï¼š{label2id} | æ€»ç±»åˆ«æ•°ï¼š{num_classes}")
    
    train_dataset = FeatDataset(feat_path=file_paths["train_feat"], label_path=file_paths["train_label"])
    val_dataset = FeatDataset(feat_path=file_paths["val_feat"], label_path=file_paths["val_label"])
    test_dataset = FeatDataset(feat_path=file_paths["test_feat"], label_path=file_paths["test_label"])
    
    train_loader = DataLoader(
        train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, drop_last=True
    )
    val_loader = DataLoader(
        val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False
    )
    test_loader = DataLoader(
        test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False
    )
    
    print(f"\nğŸ“Š åŸå§‹æ•°æ®é›†ç»Ÿè®¡ï¼š")
    print(f"   - è®­ç»ƒé›†ï¼š{len(train_dataset)} æ ·æœ¬ï¼ˆ{len(train_loader)} æ‰¹ï¼‰")
    print(f"   - éªŒè¯é›†ï¼š{len(val_dataset)} æ ·æœ¬ï¼ˆ{len(val_loader)} æ‰¹ï¼‰")
    print(f"   - æµ‹è¯•é›†ï¼š{len(test_dataset)} æ ·æœ¬ï¼ˆ{len(test_loader)} æ‰¹ï¼‰")
    print(f"   - ç‰¹å¾ç»´åº¦ï¼š{train_dataset.feats.shape[1]}")
    
    return train_loader, val_loader, test_loader, label2id, id2label, num_classes


# ===================== 3. MLPåˆ†ç±»æ¨¡å‹å®šä¹‰ =====================
class MLPClassifier(nn.Module):
    def __init__(self, input_dim=768, num_classes=2, dropout=Config.DROPOUT):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        return self.classifier(x)


# ===================== 4. è®­ç»ƒä¸è¯„ä¼°å‡½æ•° =====================
def train_one_epoch(model, dataloader, criterion, optimizer, scaler):
    model.train()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    
    for batch in tqdm(dataloader, desc="è®­ç»ƒä¸­"):
        feats = batch["feat"].to(Config.DEVICE)
        labels = batch["label"].to(Config.DEVICE)
        
        optimizer.zero_grad()
        with autocast():
            logits = model(feats)
            loss = criterion(logits, labels)
        
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        total_loss += loss.item() * feats.size(0)
        preds = torch.argmax(logits, dim=1).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())
    
    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    recall = recall_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    f1 = f1_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    
    return avg_loss, accuracy, precision, recall, f1


def evaluate_model(model, dataloader, criterion, id2label=None, is_test=False):
    model.eval()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="è¯„ä¼°ä¸­"):
            feats = batch["feat"].to(Config.DEVICE)
            labels = batch["label"].to(Config.DEVICE)
            
            with autocast():
                logits = model(feats)
                loss = criterion(logits, labels)
            
            total_loss += loss.item() * feats.size(0)
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())
    
    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    recall = recall_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    f1 = f1_score(all_labels, all_preds, average=Config.EVAL_METRIC, zero_division=0)
    
    if is_test and id2label is not None:
        print(f"\n========== æµ‹è¯•é›†è¯¦ç»†è¯„ä¼°ç»“æœ ==========")
        print(f"1. æ€»ä½“æŒ‡æ ‡ï¼ˆ{Config.EVAL_METRIC}å¹³å‡ï¼‰ï¼š")
        print(f"   - æŸå¤±ï¼š{avg_loss:.4f}")
        print(f"   - å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰ï¼š{accuracy:.4f}")
        print(f"   - ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰ï¼š{precision:.4f}")
        print(f"   - å¬å›ç‡ï¼ˆRecallï¼‰ï¼š{recall:.4f}")
        print(f"   - F1åˆ†æ•°ï¼š{f1:.4f}\n")
        
        class_precision = precision_score(all_labels, all_preds, average=None, zero_division=0)
        class_recall = recall_score(all_labels, all_preds, average=None, zero_division=0)
        class_f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)
        
        for idx in range(len(id2label)):
            cls_name = id2label[idx]
            print(f"   - {cls_name}ï¼š")
            print(f"     ç²¾ç¡®ç‡ï¼š{class_precision[idx]:.4f} | å¬å›ç‡ï¼š{class_recall[idx]:.4f} | F1ï¼š{class_f1[idx]:.4f}")
        
        if Config.PLOT_CONFUSION_MATRIX:
            cm = confusion_matrix(all_labels, all_preds)
            plt.figure(figsize=(8, 6))
            sns.heatmap(
                cm, 
                annot=True, 
                fmt="d", 
                cmap="Blues",
                xticklabels=[id2label[idx] for idx in range(len(id2label))],
                yticklabels=[id2label[idx] for idx in range(len(id2label))]
            )
            plt.xlabel("predict")
            plt.ylabel("real")
            plt.title("test_matrix")
            plt.savefig(os.path.join(Config.SAVE_MODEL_DIR, "confusion_matrix_a180s_51222.png"), dpi=300, bbox_inches="tight")
            plt.close()
            print(f"\n3. æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³ï¼š{os.path.join(Config.SAVE_MODEL_DIR, 'confusion_matrix.png')}")
    
    return avg_loss, accuracy, precision, recall, f1


# ===================== 5. ä¸»å‡½æ•°ï¼ˆæ–°å¢è¿‡é‡‡æ ·é€»è¾‘ï¼‰ =====================
def main():
    # Step 1ï¼šåŠ è½½åŸå§‹æ•°æ®
    train_loader, val_loader, test_loader, label2id, id2label, num_classes = load_all_data(Config.FEAT_ROOT)

    # Step 2ï¼šæå–è®­ç»ƒé›†ç‰¹å¾å’Œæ ‡ç­¾ï¼Œè¿›è¡Œè¿‡é‡‡æ ·
    train_feats_np = train_loader.dataset.feats.numpy()
    train_labels_np = train_loader.dataset.labels.numpy()
    
    print(f"\nğŸ” è¿‡é‡‡æ ·å‰è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒï¼š")
    unique, counts = np.unique(train_labels_np, return_counts=True)
    for cls, cnt in zip(unique, counts):
        print(f"   - ç±»åˆ« {id2label[cls]}ï¼š{cnt} ä¸ªæ ·æœ¬")

    # åº”ç”¨éšæœºè¿‡é‡‡æ ·ï¼ˆä¹Ÿå¯æ›¿æ¢ä¸ºSMOTEç­‰å…¶ä»–è¿‡é‡‡æ ·æ–¹æ³•ï¼‰
    ros = RandomOverSampler(random_state=42)
    train_feats_resampled, train_labels_resampled = ros.fit_resample(train_feats_np, train_labels_np)

    print(f"\nğŸ” è¿‡é‡‡æ ·åè®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒï¼š")
    unique_resampled, counts_resampled = np.unique(train_labels_resampled, return_counts=True)
    for cls, cnt in zip(unique_resampled, counts_resampled):
        print(f"   - ç±»åˆ« {id2label[cls]}ï¼š{cnt} ä¸ªæ ·æœ¬")

    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    train_feats_resampled_tensor = torch.tensor(train_feats_resampled, dtype=torch.float32)
    train_labels_resampled_tensor = torch.tensor(train_labels_resampled, dtype=torch.long)

    # åˆ›å»ºè¿‡é‡‡æ ·åçš„è®­ç»ƒæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset_resampled = FeatDataset(
        feat=train_feats_resampled_tensor,
        label=train_labels_resampled_tensor
    )
    train_loader_resampled = DataLoader(
        train_dataset_resampled, 
        batch_size=Config.BATCH_SIZE, 
        shuffle=True, 
        drop_last=True
    )

    # Step 3ï¼šåˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±ã€ä¼˜åŒ–å™¨
    model = MLPClassifier(
        input_dim=768, 
        num_classes=num_classes,
        dropout=Config.DROPOUT
    ).to(Config.DEVICE)
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(
        model.parameters(),
        lr=Config.LEARNING_RATE,
        weight_decay=Config.WEIGHT_DECAY
    )
    scaler = GradScaler()

    best_val_f1 = 0.0
    print(f"\nğŸ“Œ å¼€å§‹MLPåˆ†ç±»è®­ç»ƒï¼ˆ{Config.EPOCHS}è½®ï¼Œè®¾å¤‡ï¼š{Config.DEVICE}ï¼‰")

    # Step 4ï¼šè®­ç»ƒå¾ªç¯ï¼ˆä½¿ç”¨è¿‡é‡‡æ ·åçš„è®­ç»ƒåŠ è½½å™¨ï¼‰
    for epoch in range(1, Config.EPOCHS + 1):
        print(f"\n===== Epoch {epoch}/{Config.EPOCHS} =====")
        
        train_loss, train_acc, train_prec, train_rec, train_f1 = train_one_epoch(
            model, train_loader_resampled, criterion, optimizer, scaler
        )
        
        val_loss, val_acc, val_prec, val_rec, val_f1 = evaluate_model(
            model, val_loader, criterion, is_test=False
        )
        
        print(f"ğŸ“Š è®­ç»ƒé›†ï¼š")
        print(f"   æŸå¤±ï¼š{train_loss:.4f} | å‡†ç¡®ç‡ï¼š{train_acc:.4f} | F1ï¼š{train_f1:.4f}")
        print(f"ğŸ“Š éªŒè¯é›†ï¼š")
        print(f"   æŸå¤±ï¼š{val_loss:.4f} | å‡†ç¡®ç‡ï¼š{val_acc:.4f} | F1ï¼š{val_f1:.4f}")
        
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            save_path = os.path.join(Config.SAVE_MODEL_DIR, "best_model.pth")
            torch.save(model.state_dict(), save_path)
            print(f"âœ… ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆéªŒè¯é›†F1ï¼š{best_val_f1:.4f}ï¼‰è‡³ {save_path}")

    # Step 5ï¼šæµ‹è¯•é›†è¯„ä¼°
    print(f"\n========== å¼€å§‹æµ‹è¯•é›†è¯„ä¼°ï¼ˆåŠ è½½æœ€ä¼˜æ¨¡å‹ï¼‰ ==========")
    best_model = MLPClassifier(input_dim=768, num_classes=num_classes).to(Config.DEVICE)
    best_model.load_state_dict(torch.load(os.path.join(Config.SAVE_MODEL_DIR, "best_model.pth")))
    
    evaluate_model(best_model, test_loader, criterion, id2label=id2label, is_test=True)
    
    print(f"\nğŸ‰ MLPåˆ†ç±»ä»»åŠ¡å®Œæˆï¼æœ€ä¼˜æ¨¡å‹ä¿å­˜è·¯å¾„ï¼š{Config.SAVE_MODEL_DIR}")


if __name__ == "__main__":
    main()